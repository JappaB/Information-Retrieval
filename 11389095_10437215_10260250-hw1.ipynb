{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "#### Hypothesis Testing â€“ The problem of multiple comparisons [5 points]\n",
    "\n",
    "##### Question A\n",
    "On the first experiment ($m = 1$) the chance that the results are significant given that the experiment lacks the power to reject is equal to $\\alpha$. On the second experiment it is equal to $\\alpha * P(\\text{We didn't reject experiment } m-1)$. So\n",
    "\n",
    "$$P(m^{th} \\text{ experiment is significant | } m \\text{ experiments lacking power to reject } H_0) = (1 - \\alpha)^{m-1} * \\alpha$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\alpha = 0.05$\n",
    "\n",
    "$m = 6$\n",
    "\n",
    "$P(6^{th} \\text{ experiment is significant | } 6 \\text{ experiments lacking power to reject } H_0) = (1-0.05)^{6-1}*0.05 \\approx 0.038$\n",
    "\n",
    "#### Question B\n",
    "\n",
    "There is an increased chance of finding at least one false significant finding the more tests you perform. If m independent comparisons are performed, the family wise error rate (FWER) can be used. The formula for the FWER is the following:\n",
    "\n",
    "$$\\alpha_{fw} = 1-(1-\\alpha)^{m}$$\n",
    "\n",
    "so\n",
    "\n",
    "$$P(\\text{ at least one significant result | } m \\text{ experiments lacking power to reject } H_0) = \\alpha_{fw}$$\n",
    "\n",
    "To arrive at this result, observe that this is $1 - P(m \\text{ insignificant results | } m \\text{ experiments lacking power to reject } H_0)$\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\alpha = 0.05$\n",
    "\n",
    "$m = 6$\n",
    "\n",
    "$\\alpha_{fw} = 1-(1-0.05)^6\\approx0.26$\n",
    "\n",
    "#### Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning â…” of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?\n",
    "\n",
    "Ranking P\n",
    "\n",
    "| D1 N |<br>\n",
    "| D2 N |<br>\n",
    "| D3 R |<br>\n",
    "\n",
    "Ranking E\n",
    "\n",
    "| D2 N |<br>\n",
    "| D3 R |<br>\n",
    "| D1 N |<br>\n",
    "\n",
    "Because D3 is the only relevant document, we'd expect it's clicks to be attributed to E more often than to P.\n",
    "\n",
    "The set of possible interleaved rankings using Team-Draft Interleaving is as follows:\n",
    "\n",
    "| D1 N (P) | &emsp; | D1 N (P) | &emsp; | D2 N (E) | &emsp; | D2 N (E) | <br>\n",
    "| D2 N (E) | &emsp; | D2 N (E) | &emsp; | D1 N (P) | &emsp; | D1 N (P) | <br>\n",
    "| D3 R (P) | &emsp; | D3 R (E) | &emsp; | D3 R (P) | &emsp; | D3 R (E) | <br>\n",
    "\n",
    "Where D3 is in the same position in all rankings, with equal assignments to P and E. Thus, in this case, the expected number of clicks is the same for both ranking functions even though ranking E should win. This is an example of insensitivy to the quality of rankings.\n",
    "\n",
    "Example is based on: Hofmann, K., Whiteson, S., & De Rijke, M. (2011, October). A probabilistic method for inferring preferences from clicks. In Proceedings of the 20th ACM international conference on Information and knowledge management (pp. 249-258). ACM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Rankings of Relevance for E and P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Added the numbers to facilitate easy sorting.\n",
    "REL_SCORES = {'N' : 0, 'R' : 1, 'HR' : 2}\n",
    "\n",
    "# Generate all possible rankings for both Production and Experimental systems\n",
    "P = list(itertools.product(REL_SCORES.keys(), repeat = 5))\n",
    "E = list(itertools.product(REL_SCORES.keys(), repeat = 5))\n",
    " \n",
    "rankings = list(itertools.product(P, E))\n",
    "\n",
    "# Make sure the right amount of combinations are made\n",
    "assert len(rankings) == 3**10\n",
    "\n",
    "# Show a sample of the generated rankings\n",
    "for p, e in rankings[0::10000]:\n",
    "    print(p)\n",
    "    print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quote from slide 15: \"usually user only looks at very few top results : e.g. precision@3\"\n",
    "def precision_at_k(ranking, k):\n",
    "    subset = ranking[:k]\n",
    "    n_relevant = 0\n",
    "    \n",
    "    for result in subset:\n",
    "        if REL_SCORES[result] > 0:\n",
    "            n_relevant += 1\n",
    "    \n",
    "    return n_relevant / k\n",
    "\n",
    "def DCG_at_k(ranking, k, optimal=False):\n",
    "    subset = ranking[:k]\n",
    "    \n",
    "    if optimal:\n",
    "        subset = sorted(subset, reverse=True, key=lambda x: REL_SCORES[x])\n",
    "        \n",
    "    discounted_score = 0\n",
    "    \n",
    "    for i, result in enumerate(subset):\n",
    "        # prepare variables for DCG formula\n",
    "        rank = i + 1\n",
    "        rel = REL_SCORES[result]\n",
    "        \n",
    "        # Calculate score\n",
    "        score = (2**rel - 1) / np.log2(1 + rank)\n",
    "        discounted_score += score\n",
    "    \n",
    "    # NB: sum is part of the formula\n",
    "    return discounted_score\n",
    "        \n",
    "def nDCG_at_k(ranking, k):\n",
    "    true = DCG_at_k(ranking, k)\n",
    "    best = DCG_at_k(ranking, k, optimal=True)\n",
    "    \n",
    "    # This is to prevent dividing by 0. If the best possible DCG is 0, the\n",
    "    # true DCG is also zero (because there are no relevant document is the top k)\n",
    "    if best == 0.0: \n",
    "        return 0.0\n",
    "    else:\n",
    "        return true / best\n",
    "\n",
    "def prob_of_relevance(grade, max_grade):\n",
    "    return (2**grade - 1) / 2**max_grade\n",
    "\n",
    "def ERR(ranking):\n",
    "    \"\"\"Algorithm to compute ERR in linear time, implemented based on \n",
    "    \n",
    "    Chapelle, O., Metlzer, D., Zhang, Y., & Grinspan, P. (2009, November). \n",
    "    Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference\n",
    "    on Information and knowledge management (pp. 621-630). ACM.\"\"\"\n",
    "    \n",
    "    grades = [REL_SCORES[x] for x in ranking]\n",
    "    max_grade = max(REL_SCORES.values())\n",
    "    \n",
    "    p = 1\n",
    "    err = 0\n",
    "    \n",
    "    for r in range(len(grades)):\n",
    "        R = prob_of_relevance(grades[r], max_grade)\n",
    "        err += p * (R/(r+1))\n",
    "        p *= (1 - R)\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell does te calculations for all three metrics and saves the results to a list    \n",
    "\n",
    "depth = 3\n",
    "\n",
    "precision_p = []\n",
    "precision_e = []\n",
    "dcg_p = []\n",
    "dcg_e = []\n",
    "err_p = []\n",
    "err_e = []\n",
    "\n",
    "for i, (p, e) in enumerate(list(rankings)):\n",
    "    precision_p.append(precision_at_k(p, depth))\n",
    "    precision_e.append(precision_at_k(e, depth))\n",
    "\n",
    "    dcg_p.append(nDCG_at_k(p, depth))\n",
    "    dcg_e.append(nDCG_at_k(e, depth))\n",
    "    \n",
    "    err_p.append(ERR(p))\n",
    "    err_e.append(ERR(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(range(len(dcg_p)), dcg_p)\n",
    "\n",
    "# plt.subplot(212)\n",
    "# plt.plot(range(len(dcg_e)), dcg_e, linewidth=0.1)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(211)\n",
    "plt.plot(range(len(precision_p)), precision_p)\n",
    "\n",
    "# plt.subplot(212)\n",
    "# plt.plot(range(len(precision_e)), precision_e, linewidth=0.1)\n",
    "\n",
    "fig = plt.figure(3)\n",
    "plt.subplot(211)\n",
    "plt.plot(range(len(err_p)), err_p)\n",
    "\n",
    "# plt.subplot(212)\n",
    "# plt.plot(range(len(err_e)), err_e, linewidth=0.1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#TODO: asxislabels, graphtitles\n",
    "#TODO: Analsyse algemener maken. De volgorde van R / N / HR is anders op elke machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The lists in the `rankings` variable represents a simulation of all possible outcomes of a comparisson of model P and model E on a query. Because of the way the combinations are generated, subsequent instances of the E model vary rapidly and thus produce chaotic graphs. So for the graphical analysis, we will focus on model P.\n",
    "\n",
    "The first thing that stands out is that there is no obvious pattern in the progression of the metrics as you iterate trough the instances. We would've expected a more symmectric shape of the plot. But at further inspection, there is indeed a pattern. This is especially viable when you compare the three metrics with eachoter. All metrics are mininum around the 30.000th pair, which where the rankings are mostly non-relevant. At the beginning and end of the simulations the rankings are made up of mostly 'R' and 'HR' documents so they score high. In the EGG chart you get a more detailed view, since the grade of the rank is taken into account and grade('R') < grade('HR')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the ð›¥measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the assignment is says to only consider pairs for which E > P. But we found it easier to keep\n",
    "# all pairs, because we have to compare the results with the online rankings. If we discard some pairs,\n",
    "# the arrays don't line up anymore.\n",
    "\n",
    "def delta_measure(scores_a, scores_b):\n",
    "    return [b - a for a, b in zip(scores_a, scores_b)]\n",
    "\n",
    "def percentage_of_wins_for_E(delta_values):\n",
    "    return np.sum(np.array(delta_values) > 0) / len(delta_values)\n",
    "\n",
    "# Calculate delta measure for the three chosen metrics\n",
    "delta_precision = delta_measure(precision_p, precision_e)\n",
    "delta_nDCG = delta_measure(dcg_p, dcg_e)\n",
    "delta_ERR = delta_measure(err_p, err_e)\n",
    "\n",
    "print(\"Delta precision\")\n",
    "print()\n",
    "print(delta_precision[0::1000])\n",
    "print()\n",
    "print(\"Percentage of wins for E:\", percentage_of_wins_for_E(delta_precision))\n",
    "print()\n",
    "print()\n",
    "print(\"Delta nDCG\")\n",
    "print()\n",
    "print(delta_nDCG[0::1000])\n",
    "print()\n",
    "print(\"Percentage of wins for E:\", percentage_of_wins_for_E(delta_nDCG))\n",
    "print()\n",
    "print()\n",
    "print(\"Delta ERR\")\n",
    "print()\n",
    "print(delta_ERR[0::1000])\n",
    "print()\n",
    "print(\"Percentage of wins for E:\", percentage_of_wins_for_E(delta_ERR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Implement Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Step 4: Implement 2 interleaving algorithms: \n",
    "## (1) Team-Draft Interleaving OR Balanced Interleaving, AND \n",
    "## (2) Probabilistic Interleaving.\n",
    "\n",
    "# The interleaving algorithms should: \n",
    "## (a) given two rankings of relevance interleave them into a single ranking, and \n",
    "## (b) given the users clicks on the interleaved ranking assign credit to the algorithms \n",
    "##     that produced the rankings.\n",
    "\n",
    "# (Note 4: \n",
    "## Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, \n",
    "## in our case the rankings consist of relevance labels. Hence in this case \n",
    "## (a) you will assume that E and P return different documents, \n",
    "## (b) the interleaved ranking will also be a ranking of labels.)\n",
    "\n",
    "\n",
    "### BALANCED INTERLEAVING:\n",
    "# Find two random rankings A and B and present them:\n",
    "#A, B = ['a','b','c','d','g','h'], ['b','e','a','f','g','h']\n",
    "A, B = random.choice(rankings)\n",
    "A, B = [str(a) + \"P\" + str(i) for i, a in enumerate(A)], [str(b) + \"E\" + str(i) for i, b in enumerate(B)]\n",
    "\n",
    "# initialize pointers p_a and p_b\n",
    "p_a, p_b, I = 0, 0, []\n",
    "\n",
    "# Flip a coin to decide which pointer to pick highest value from ranking.\n",
    "turn = 'A' if random.randint(0, 1) == 0 else 'B'\n",
    "\n",
    "# Greedily collect rankings from both lists and build the interleaved list.\n",
    "print(turn, \"is first:\")\n",
    "while p_a < len(A) or p_b < len(B):\n",
    "    if p_a < p_b or p_a == p_b and turn == 'A':\n",
    "        if A[p_a] not in I:\n",
    "            I.append(A[p_a])\n",
    "        p_a += 1\n",
    "    else:\n",
    "        if B[p_b] not in I:\n",
    "            I.append(B[p_b])\n",
    "        p_b += 1\n",
    "\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEAM-DRAFT INTERLEAVING:\n",
    "\n",
    "\n",
    "# Fully based on example in paper: TODO\n",
    "#A, B = ['a','b','c','d','g','h'], ['b','e','a','f','g','h']\n",
    "A, B = random.choice(rankings)\n",
    "#print(A, B)\n",
    "\n",
    "A, B = [str(a) + \"P\" + str(i) for i, a in enumerate(A)], [str(b) + \"E\" + str(i) for i, b in enumerate(B)]\n",
    "\n",
    "# create teams\n",
    "team_a, team_b, I = 0, 0, []\n",
    "\n",
    "while any(set(A)-set(I)) or any(set(B)-set(I)):\n",
    "    turn = 'A' if random.randint(0, 1) == 0 else 'B'\n",
    "    if team_a < team_b or team_a == team_b and turn == 'A':\n",
    "        for a in A:\n",
    "            if a not in I:\n",
    "                I.append(a)\n",
    "                break\n",
    "        team_a += 1\n",
    "    else:\n",
    "        for b in B:\n",
    "            if b not in I:\n",
    "                I.append(b)\n",
    "                break\n",
    "        team_b += 1\n",
    "\n",
    "def team_draft_interleaving(A,B):\n",
    "    # create teams\n",
    "    team_a, team_b, I = 0, 0, []\n",
    "    \n",
    "    # While there are still documents not present in the interleaved list\n",
    "    while any(set(A)-set(I)) or any(set(B)-set(I)):\n",
    "        \n",
    "        #Flip a coin\n",
    "        turn = 'A' if random.randint(0, 1) == 0 else 'B'\n",
    "        \n",
    "        #If A wins the coin flip: put the highest ranked document of A\n",
    "        # that is not yet in the interleaved list, in the interleaved\n",
    "        if team_a < team_b or team_a == team_b and turn == 'A':\n",
    "            for a in A:\n",
    "                if a not in I:\n",
    "                    I.append(a)\n",
    "                    break\n",
    "            team_a += 1\n",
    "        # If B wins the flip: put the highest ranked document of A\n",
    "        # that is not yet in the interleaved list, in the interleaved\n",
    "        else:\n",
    "            for b in B:\n",
    "                if b not in I:\n",
    "                    I.append(b)\n",
    "                    break\n",
    "            team_b += 1\n",
    "    return(I)\n",
    "\n",
    "\n",
    "\n",
    "interleaved_lists = []\n",
    "\n",
    "for A,B in rankings:\n",
    "    A, B = [str(a) + \"P\" + str(i) for i, a in enumerate(A)], [str(b) + \"E\" + str(i) for i, b in enumerate(B)]\n",
    "    interleaved_list = team_draft_interleaving(A,B)\n",
    "    interleaved_lists.append(interleaved_list)\n",
    "\n",
    "print(interleaved_lists[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Implement User Clicks Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' In this cell the Random click model is implemented. We use the PyClick module to parse the Yandex file.\n",
    "Our TA assured us it is fine, if we use it just for the parsing of the Yandex file.\n",
    "Furthermore we assume all queries should be used, even if none of the results are clicked'''\n",
    "\n",
    "def execute_clicks(click_probs):\n",
    "    \"\"\"Returns a list of booleans indicating on which positions was clicked, based on the\n",
    "    given click_probs (click probabilities)\"\"\"\n",
    "    \n",
    "    # Flip one coin for (and with) every probability in click_probs\n",
    "    return np.random.binomial(1, p=click_probs)\n",
    "\n",
    "def MLE_random_click(sessions):\n",
    "    '''As slide 62 of lecture 3 shows, the MLE of a bernouilli distribution, rho, is just\n",
    "    the ratio of clicks with respect to the total number of URls shown. \n",
    "    '''\n",
    "    total_clicks = 0\n",
    "    total_shown = len(sessions)*10\n",
    "    \n",
    "    for sesh in sessions:\n",
    "        total_clicks += sum(sesh.get_clicks())\n",
    "    \n",
    "    rho = total_clicks/total_shown\n",
    "    return rho\n",
    "            \n",
    "def random_click_probabilities(ranking, rho):\n",
    "    '''probabilities of a click from the random clicking model'''\n",
    "    return [rho] * len(ranking)\n",
    "\n",
    "def simulate_random_click_model(ranking, rho, N):\n",
    "    '''Generate N click patterns based on the random click model'''\n",
    "    return [execute_clicks(random_click_probabilities(ranking, rho)) for _ in range(N)]\n",
    "\n",
    "def sdbn_click_probabilities(ranking, alphas, sigmas):\n",
    "    '''Probabilities of a click from the Simple Bayesian Dynamic Network'''\n",
    "    \n",
    "    # Assume zero probabilities\n",
    "    click_probs = [0.0] * len(ranking)\n",
    "    \n",
    "    for i, relevance in enumerate(ranking):\n",
    "        \n",
    "        # Click when attracted\n",
    "        alpha = alphas[relevance[:-2]]\n",
    "        click_probs[i] = alpha\n",
    "        \n",
    "        # If satisfied, we do not evaluate the rest of the documents\n",
    "        satisfied = float(np.random.choice([0,1], 1, p=[(1-sigmas[i]),(sigmas[i])]))\n",
    "        if satisfied:\n",
    "            break\n",
    "    \n",
    "    return click_probs\n",
    "\n",
    "def simulate_sdbn_click_model(ranking, alphas, sigmas, N):\n",
    "    '''Generate N click patterns based on the SDBN click model'''\n",
    "    return [execute_clicks(sdbn_click_probabilities(ranking, alphas, sigmas)) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclick\n",
    "from pyclick.utils import YandexRelPredChallengeParser\n",
    "\n",
    "parser = YandexRelPredChallengeParser.YandexRelPredChallengeParser()\n",
    "search_sessions = parser.parse('YandexRelPredChallenge.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: 1 query always returns the same docIDs (ie. ranking is deterministic)\n",
    "# TODO: dit kunnen we in principe gewoon testen\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def learn_sdbn_satisfaction(search_sessions):\n",
    "    last_click_ranks = dict()\n",
    "    clicks_per_query = dict()\n",
    "    last_total_ratio = dict()\n",
    "\n",
    "    # Find last click rank per query:\n",
    "    for session in search_sessions:\n",
    "        \n",
    "        # If get_last_click_rank returns 10, there hasn't been click action for the query.\n",
    "        if session.get_last_click_rank() == 10:\n",
    "            continue\n",
    "\n",
    "        # Find last click:\n",
    "        if session.query in last_click_ranks:\n",
    "            last_click_ranks[session.query].append(session.get_last_click_rank())\n",
    "        else:\n",
    "            last_click_ranks[session.query] = [session.get_last_click_rank()]\n",
    "            \n",
    "        # Find all click ranks per query:\n",
    "        if session.query not in clicks_per_query:\n",
    "            clicks_per_query[session.query] = []\n",
    "\n",
    "        for idx, val in enumerate(session.get_clicks()):\n",
    "            if val == 1:\n",
    "                clicks_per_query[session.query].append(idx)    \n",
    "\n",
    "    # Calculate sigmas for each unique query and each of its ranks:\n",
    "    for query in last_click_ranks.keys():\n",
    "        last_total_ratio[query] = [0] * 10\n",
    "    \n",
    "        count_last = Counter(last_click_ranks[query])\n",
    "        count_total = Counter(clicks_per_query[query])\n",
    "    \n",
    "        for rank in list(count_last):\n",
    "            sigma = count_last[rank] / count_total[rank]\n",
    "            \n",
    "            last_total_ratio[query][rank] = sigma\n",
    "            \n",
    "    # Compose final list of average sigmas, i.e. 1 sigma for each rank:\n",
    "    avg_sigmas = [0.0] * 10\n",
    "\n",
    "    for query in list(last_total_ratio):\n",
    "        avg_sigmas = [sum(sigma) for sigma in zip(last_total_ratio[query], avg_sigmas)]\n",
    "    \n",
    "    click_model = [sigma / len(last_total_ratio) for sigma in avg_sigmas]\n",
    "    norm_click_model = [float(sigma) / sum(click_model) for sigma in click_model]\n",
    "    \n",
    "    return last_click_ranks, clicks_per_query, last_total_ratio, click_model, norm_click_model\n",
    "\n",
    "\n",
    "%time last, total, sigmas, cm, ncm = learn_sdbn_satisfaction(search_sessions)\n",
    "# np.sum(sigmas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(last[\"209\"]))\n",
    "print()\n",
    "print(sorted(total[\"209\"]))\n",
    "print()\n",
    "print(sigmas[\"209\"])\n",
    "\n",
    "ncm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Simulate Interleaving Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Random click model experiment'''\n",
    "\n",
    "def click_ranker_mapping(clicks, interleaved_list):\n",
    "    '''Input a list of clicks (1 or 0) and a interleaved list with information on which ranker\n",
    "    attributed which file.\n",
    "    \n",
    "    Output the winner of the interleaved list if any. Else it outputs a tie.'''\n",
    "    \n",
    "    # counters for how many times P and E are clicked\n",
    "    P_click = 0\n",
    "    E_click = 0\n",
    "    \n",
    "    for i, value in enumerate(clicks):\n",
    "        #if clicked\n",
    "        if value == 1.0:\n",
    "            # Check which algorithm contributed it to the list\n",
    "            if interleaved_list[i][-2] == 'E':\n",
    "                E_click += 1\n",
    "            else:\n",
    "                P_click += 1\n",
    "                    \n",
    "    if P_click > E_click:\n",
    "        return 'P'\n",
    "    if E_click > P_click:\n",
    "        return 'E'\n",
    "    \n",
    "    return 'Tie'\n",
    "\n",
    "def experiment(clicks, interleaved_list):\n",
    "    '''Does experiments for one interleaved list with a list of click patterns.\n",
    "    Outputs a proportion of wins for E over all experiments, ties included.'''\n",
    "    \n",
    "    clicks = np.array(clicks)\n",
    "    \n",
    "    # Boolean array where each contribution of E is marked True\n",
    "    E_docs = np.array([x[-2] == 'E' for x in interleaved_list])\n",
    "    \n",
    "    # Overlay of clicks and the boolean mask above. Evaluates\n",
    "    # to True if a rank has been clicked and was contributed by E.\n",
    "    # Finally sum those Truths (1s) to get total clicks on E per experiment\n",
    "    E_clicks = np.sum(np.logical_and(E_docs, clicks), axis=1)\n",
    "    \n",
    "    # Clicks on P are the difference of total clicks and E-clicks\n",
    "    P_clicks = np.sum(clicks, axis=1) - E_clicks\n",
    "    \n",
    "    return  np.sum(E_clicks > P_clicks) / len(clicks)\n",
    "\n",
    "def experiment_loop(clicks, interleaved_list):\n",
    "    '''Does N experiments for one interleaved list\n",
    "    Outputs a proportion of wins for E over all experiments, ties included.'''\n",
    "    \n",
    "    P_wins = 0\n",
    "    E_wins = 0\n",
    "    \n",
    "    for click_pattern in clicks:\n",
    "        winner = click_ranker_mapping(click_pattern, interleaved_list)\n",
    "        \n",
    "        if winner == 'P':\n",
    "            P_wins += 1\n",
    "        \n",
    "        if winner == 'E':\n",
    "            E_wins += 1\n",
    "    \n",
    "    return (E_wins/len(clicks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 / 59049 \t\t at 17.862511797488374/sec\n",
      "201 / 59049 \t\t at 17.40684748053797/sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-1621f37df783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# SDBN simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0msdbn_clicks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_sdbn_click_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterleaved_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'navigational'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mE_win_proportions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sdbn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdbn_clicks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterleaved_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-8292929eb2c6>\u001b[0m in \u001b[0;36msimulate_sdbn_click_model\u001b[0;34m(ranking, alphas, sigmas, N)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimulate_sdbn_click_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;34m'''Generate N click patterns based on the SDBN click model'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecute_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdbn_click_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-8292929eb2c6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimulate_sdbn_click_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;34m'''Generate N click patterns based on the SDBN click model'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecute_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdbn_click_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-8292929eb2c6>\u001b[0m in \u001b[0;36msdbn_click_probabilities\u001b[0;34m(ranking, alphas, sigmas)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# If satisfied, we do not evaluate the rest of the documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0msatisfied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msatisfied\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''In this cell the experiment for the random click model is run'''\n",
    "\n",
    "E_win_proportions = {\n",
    "    'random' : [],\n",
    "    'sdbn' : []\n",
    "}\n",
    "\n",
    "# Estimate alpha_uq from relevance labels\n",
    "\n",
    "# Katja Hofmann, Anne Schuth, Shimon Whiteson, and Maarten de Rijke. Reusing historical interaction\n",
    "# data for faster online learning to rank for IR. In WSDM, 2013. ACM Press. 69, 70, 77\n",
    "\n",
    "alpha = {\n",
    "    'perfect' : {'N' : 0.0, 'R' : 0.5, 'HR' : 1.0},\n",
    "    'navigational' : {'N' : 0.05, 'R' : 0.5, 'HR' : 0.95},\n",
    "    'informal' : {'N' : 0.4, 'R' : 0.7 , 'HR' : 0.9}\n",
    "}\n",
    "\n",
    "# Calculate the Rho for the random click model\n",
    "rho = MLE_random_click(search_sessions)\n",
    "\n",
    "import time\n",
    "\n",
    "N = 100\n",
    "start = time.time()\n",
    "for i in range(len(interleaved_lists)):\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        print(\"{} / {} \\t\\t at {}/sec\".format(i+1, len(interleaved_lists), (i+1)/(time.time() - start)))\n",
    "    \n",
    "    # Random Click Model simulation\n",
    "    random_clicks = simulate_random_click_model(interleaved_lists[i], rho, N)\n",
    "#     e1 = experiment(random_clicks, interleaved_lists[i])\n",
    "#     e2 = experiment_loop(random_clicks, interleaved_lists[i])\n",
    "#     assert e1 == e2, \"{} / {}\".format(e1, e2) \n",
    "    E_win_proportions['random'].append(experiment(random_clicks, interleaved_lists[i]))\n",
    "    \n",
    "    # SDBN simulation\n",
    "    sdbn_clicks = simulate_sdbn_click_model(interleaved_lists[i], alpha['navigational'], ncm, N)\n",
    "    E_win_proportions['sdbn'].append(experiment(sdbn_clicks, interleaved_lists[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In this cell the visualizations for the random click model experiment are generated\n",
    "\n",
    "\n",
    "#Beautiful\n",
    "\n",
    "'''\n",
    "\n",
    "# Tournament: when does P beat E and does it depend on the metric?\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "\n",
    "def mean_scores(metric, sample):\n",
    "    result_p = []\n",
    "    result_e = []\n",
    "\n",
    "    for p, e in sample:\n",
    "\n",
    "        result_p.append(metric(p))\n",
    "        result_e.append(metric(e))\n",
    "    \n",
    "    return result_p, result_e\n",
    "\n",
    "def test_greater_than(sample_a, sample_b, alpha):\n",
    "    \"\"\"Tests whether the difference [mean(sample_a) - mean(sample_b)] is \n",
    "    significantly greater than zero\"\"\"\n",
    "    \n",
    "    t_test = stats.ttest_ind(sample_a, sample_b)\n",
    "    return t_test.statistic > 0 and t_test.pvalue/2 < alpha\n",
    "\n",
    "def test_greater_than_single_sample(sample, alpha):\n",
    "    \n",
    "    t_test = stats.ttest_1samp(sample, 0.5)\n",
    "    return t_test.statistic > 0 and t_test.pvalue/2 < alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible test:\n",
    "# - Is there a meaningful difference between SDBN and Random\n",
    "# - Is there a meaningful difference between \n",
    "\n",
    "wins_under_precision = [delta > 0 for delta in delta_precision]\n",
    "wins_under_nDCG = [delta > 0 for delta in delta_nDCG]\n",
    "wins_under_ERR = [delta > 0 for delta in delta_ERR]\n",
    "wins_under_SDBN = [p > 0.5 for p in E_win_proportions['sdbn']]\n",
    "wins_under_RCM = [p > 0.5 for p in E_win_proportions['random']]\n",
    "\n",
    "print(\"Interleaving:\")\n",
    "print(\"Mean proportion of wins for E under sdbn\", np.mean(E_win_proportions['sdbn']), test_greater_than_single_sample(E_win_proportions['sdbn'], 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
