{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute_0.1_validation_queries.txt\n",
      "absolute_0.9_validation_queries.txt\n",
      "jelinek_0.1_validation_queries.run\n",
      "BM25_testqueries.txt\n",
      "BM25_testqueries.run\n",
      "dirichlet_500_validation_queries.run\n",
      "jelinek_0.5_validation_queries.run\n",
      "absolute_0.5_validation_queries.txt\n",
      "absolute_0.1_validation_queries.run\n",
      "TF-IDF.txt\n",
      "dirichlet_1500_validation_queries.run\n",
      "dirichlet_1000_validation_queries.run\n",
      "absolute_0.9_validation_queries.run\n",
      "TF-IDF.run\n",
      "absolute_0.5_validation_queries.run\n",
      "jelinek_0.9_validation_queries.run\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import subprocess\n",
    "from collections import defaultdict \n",
    "\n",
    "# Run evaluation bash script...\n",
    "# subprocess.call([\"./assignment_evals.sh\"])\n",
    "\n",
    "\n",
    "# ... And retrieve again\n",
    "eval_data = defaultdict(list)\n",
    "\n",
    "for file in os.listdir(\"run\"):\n",
    "    print(file)\n",
    "    if file == \"TF-IDF.txt\":\n",
    "        with open(\"run/\" + file) as file:\n",
    "            content = [line.strip().split() for line in file.readlines()]\n",
    "#             print(content)\n",
    "            for metric, query, score in content:\n",
    "                eval_data[metric].append((query,score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'P_10': [('100', '0.1000'),\n",
       "              ('101', '0.3000'),\n",
       "              ('102', '0.2000'),\n",
       "              ('104', '0.5000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.2000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2000'),\n",
       "              ('112', '0.3000'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.1000'),\n",
       "              ('116', '0.7000'),\n",
       "              ('117', '0.4000'),\n",
       "              ('118', '0.4000'),\n",
       "              ('119', '0.2000'),\n",
       "              ('121', '0.2000'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.3000'),\n",
       "              ('125', '0.1000'),\n",
       "              ('126', '0.4000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4000'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.6000'),\n",
       "              ('131', '0.1000'),\n",
       "              ('132', '0.8000'),\n",
       "              ('133', '0.7000'),\n",
       "              ('134', '0.6000'),\n",
       "              ('136', '0.1000'),\n",
       "              ('137', '0.2000'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.3000'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.1000'),\n",
       "              ('142', '0.2000'),\n",
       "              ('145', '0.2000'),\n",
       "              ('146', '0.3000'),\n",
       "              ('147', '0.1000'),\n",
       "              ('148', '0.1000'),\n",
       "              ('149', '0.1000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.5000'),\n",
       "              ('154', '0.6000'),\n",
       "              ('156', '0.1000'),\n",
       "              ('157', '0.3000'),\n",
       "              ('159', '0.1000'),\n",
       "              ('160', '0.2000'),\n",
       "              ('161', '1.0000'),\n",
       "              ('162', '0.2000'),\n",
       "              ('163', '0.9000'),\n",
       "              ('164', '0.6000'),\n",
       "              ('166', '0.4000'),\n",
       "              ('168', '0.1000'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.2000'),\n",
       "              ('174', '0.7000'),\n",
       "              ('175', '0.2000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.1000'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '1.0000'),\n",
       "              ('184', '0.3000'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.1000'),\n",
       "              ('187', '0.1000'),\n",
       "              ('188', '0.6000'),\n",
       "              ('189', '0.5000'),\n",
       "              ('190', '0.1000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.9000'),\n",
       "              ('194', '0.1000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.1000'),\n",
       "              ('197', '0.3000'),\n",
       "              ('198', '0.4000'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.6000'),\n",
       "              ('51', '0.7000'),\n",
       "              ('52', '0.9000'),\n",
       "              ('54', '0.3000'),\n",
       "              ('55', '0.2000'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.5000'),\n",
       "              ('59', '0.1000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.2000'),\n",
       "              ('62', '0.2000'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.1000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.1000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.7000'),\n",
       "              ('71', '0.5000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.6000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.4000'),\n",
       "              ('82', '0.3000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.7000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.2000'),\n",
       "              ('98', '0.3000'),\n",
       "              ('99', '0.0000'),\n",
       "              ('all', '0.2400')],\n",
       "             'P_100': [('100', '0.0600'),\n",
       "              ('101', '0.1100'),\n",
       "              ('102', '0.0800'),\n",
       "              ('104', '0.3200'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.1300'),\n",
       "              ('107', '0.1600'),\n",
       "              ('108', '0.0400'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2000'),\n",
       "              ('112', '0.1100'),\n",
       "              ('113', '0.0100'),\n",
       "              ('115', '0.1400'),\n",
       "              ('116', '0.1500'),\n",
       "              ('117', '0.2100'),\n",
       "              ('118', '0.1900'),\n",
       "              ('119', '0.1400'),\n",
       "              ('121', '0.0400'),\n",
       "              ('122', '0.0600'),\n",
       "              ('124', '0.1000'),\n",
       "              ('125', '0.2400'),\n",
       "              ('126', '0.0700'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.3500'),\n",
       "              ('129', '0.0800'),\n",
       "              ('130', '0.4600'),\n",
       "              ('131', '0.0400'),\n",
       "              ('132', '0.6900'),\n",
       "              ('133', '0.1300'),\n",
       "              ('134', '0.0600'),\n",
       "              ('136', '0.0500'),\n",
       "              ('137', '0.2200'),\n",
       "              ('138', '0.0500'),\n",
       "              ('139', '0.0800'),\n",
       "              ('140', '0.0800'),\n",
       "              ('141', '0.0400'),\n",
       "              ('142', '0.0200'),\n",
       "              ('145', '0.0900'),\n",
       "              ('146', '0.3800'),\n",
       "              ('147', '0.1600'),\n",
       "              ('148', '0.1200'),\n",
       "              ('149', '0.0600'),\n",
       "              ('150', '0.0100'),\n",
       "              ('152', '0.0700'),\n",
       "              ('153', '0.1200'),\n",
       "              ('154', '0.5500'),\n",
       "              ('156', '0.1800'),\n",
       "              ('157', '0.1500'),\n",
       "              ('159', '0.0200'),\n",
       "              ('160', '0.0600'),\n",
       "              ('161', '0.5900'),\n",
       "              ('162', '0.2100'),\n",
       "              ('163', '0.6200'),\n",
       "              ('164', '0.2600'),\n",
       "              ('166', '0.1000'),\n",
       "              ('168', '0.1000'),\n",
       "              ('169', '0.1100'),\n",
       "              ('171', '0.0200'),\n",
       "              ('172', '0.0400'),\n",
       "              ('174', '0.4200'),\n",
       "              ('175', '0.2600'),\n",
       "              ('176', '0.0100'),\n",
       "              ('177', '0.1300'),\n",
       "              ('178', '0.0400'),\n",
       "              ('179', '0.0200'),\n",
       "              ('181', '0.0300'),\n",
       "              ('183', '0.5000'),\n",
       "              ('184', '0.1500'),\n",
       "              ('185', '0.1600'),\n",
       "              ('186', '0.0500'),\n",
       "              ('187', '0.1900'),\n",
       "              ('188', '0.2100'),\n",
       "              ('189', '0.2500'),\n",
       "              ('190', '0.0300'),\n",
       "              ('191', '0.0100'),\n",
       "              ('193', '0.4700'),\n",
       "              ('194', '0.0500'),\n",
       "              ('195', '0.0700'),\n",
       "              ('196', '0.0400'),\n",
       "              ('197', '0.0500'),\n",
       "              ('198', '0.1400'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.2700'),\n",
       "              ('51', '0.2000'),\n",
       "              ('52', '0.8300'),\n",
       "              ('54', '0.2200'),\n",
       "              ('55', '0.5400'),\n",
       "              ('56', '0.7700'),\n",
       "              ('58', '0.3900'),\n",
       "              ('59', '0.2400'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.1400'),\n",
       "              ('62', '0.1500'),\n",
       "              ('63', '0.0200'),\n",
       "              ('64', '0.0600'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0400'),\n",
       "              ('68', '0.0200'),\n",
       "              ('70', '0.3800'),\n",
       "              ('71', '0.2900'),\n",
       "              ('72', '0.0400'),\n",
       "              ('73', '0.0100'),\n",
       "              ('75', '0.0600'),\n",
       "              ('76', '0.0100'),\n",
       "              ('77', '0.5100'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0100'),\n",
       "              ('81', '0.1400'),\n",
       "              ('82', '0.2600'),\n",
       "              ('83', '0.1000'),\n",
       "              ('84', '0.0200'),\n",
       "              ('85', '0.6200'),\n",
       "              ('87', '0.0100'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0400'),\n",
       "              ('97', '0.0800'),\n",
       "              ('98', '0.1100'),\n",
       "              ('99', '0.1400'),\n",
       "              ('all', '0.1578')],\n",
       "             'P_1000': [('100', '0.0140'),\n",
       "              ('101', '0.0250'),\n",
       "              ('102', '0.0130'),\n",
       "              ('104', '0.0500'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0360'),\n",
       "              ('107', '0.0220'),\n",
       "              ('108', '0.0280'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1960'),\n",
       "              ('112', '0.0120'),\n",
       "              ('113', '0.0240'),\n",
       "              ('115', '0.0740'),\n",
       "              ('116', '0.0170'),\n",
       "              ('117', '0.0290'),\n",
       "              ('118', '0.0830'),\n",
       "              ('119', '0.0770'),\n",
       "              ('121', '0.0350'),\n",
       "              ('122', '0.0180'),\n",
       "              ('124', '0.0320'),\n",
       "              ('125', '0.0810'),\n",
       "              ('126', '0.0510'),\n",
       "              ('127', '0.0090'),\n",
       "              ('128', '0.0470'),\n",
       "              ('129', '0.0820'),\n",
       "              ('130', '0.1590'),\n",
       "              ('131', '0.0060'),\n",
       "              ('132', '0.1310'),\n",
       "              ('133', '0.0130'),\n",
       "              ('134', '0.0060'),\n",
       "              ('136', '0.0100'),\n",
       "              ('137', '0.0370'),\n",
       "              ('138', '0.0270'),\n",
       "              ('139', '0.0340'),\n",
       "              ('140', '0.0200'),\n",
       "              ('141', '0.0140'),\n",
       "              ('142', '0.0520'),\n",
       "              ('145', '0.0330'),\n",
       "              ('146', '0.1960'),\n",
       "              ('147', '0.0220'),\n",
       "              ('148', '0.0430'),\n",
       "              ('149', '0.0200'),\n",
       "              ('150', '0.0290'),\n",
       "              ('152', '0.0770'),\n",
       "              ('153', '0.0280'),\n",
       "              ('154', '0.1810'),\n",
       "              ('156', '0.1490'),\n",
       "              ('157', '0.0200'),\n",
       "              ('159', '0.0030'),\n",
       "              ('160', '0.0130'),\n",
       "              ('161', '0.1210'),\n",
       "              ('162', '0.0430'),\n",
       "              ('163', '0.0750'),\n",
       "              ('164', '0.0320'),\n",
       "              ('166', '0.0150'),\n",
       "              ('168', '0.0550'),\n",
       "              ('169', '0.0240'),\n",
       "              ('171', '0.0060'),\n",
       "              ('172', '0.0060'),\n",
       "              ('174', '0.0820'),\n",
       "              ('175', '0.0520'),\n",
       "              ('176', '0.0250'),\n",
       "              ('177', '0.0580'),\n",
       "              ('178', '0.0210'),\n",
       "              ('179', '0.0200'),\n",
       "              ('181', '0.0070'),\n",
       "              ('183', '0.0610'),\n",
       "              ('184', '0.0380'),\n",
       "              ('185', '0.0740'),\n",
       "              ('186', '0.0170'),\n",
       "              ('187', '0.0870'),\n",
       "              ('188', '0.0580'),\n",
       "              ('189', '0.2270'),\n",
       "              ('190', '0.0220'),\n",
       "              ('191', '0.0070'),\n",
       "              ('193', '0.0630'),\n",
       "              ('194', '0.0250'),\n",
       "              ('195', '0.0650'),\n",
       "              ('196', '0.0210'),\n",
       "              ('197', '0.0140'),\n",
       "              ('198', '0.0560'),\n",
       "              ('199', '0.0030'),\n",
       "              ('200', '0.0370'),\n",
       "              ('51', '0.0320'),\n",
       "              ('52', '0.2070'),\n",
       "              ('54', '0.0820'),\n",
       "              ('55', '0.2210'),\n",
       "              ('56', '0.2270'),\n",
       "              ('58', '0.0850'),\n",
       "              ('59', '0.0990'),\n",
       "              ('60', '0.0040'),\n",
       "              ('61', '0.0530'),\n",
       "              ('62', '0.0350'),\n",
       "              ('63', '0.0040'),\n",
       "              ('64', '0.1250'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0290'),\n",
       "              ('68', '0.0180'),\n",
       "              ('70', '0.0400'),\n",
       "              ('71', '0.0770'),\n",
       "              ('72', '0.0130'),\n",
       "              ('73', '0.0020'),\n",
       "              ('75', '0.0100'),\n",
       "              ('76', '0.0160'),\n",
       "              ('77', '0.0660'),\n",
       "              ('79', '0.0070'),\n",
       "              ('80', '0.0090'),\n",
       "              ('81', '0.0370'),\n",
       "              ('82', '0.0800'),\n",
       "              ('83', '0.0580'),\n",
       "              ('84', '0.0120'),\n",
       "              ('85', '0.2940'),\n",
       "              ('87', '0.0030'),\n",
       "              ('88', '0.0200'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0090'),\n",
       "              ('97', '0.0170'),\n",
       "              ('98', '0.0150'),\n",
       "              ('99', '0.1520'),\n",
       "              ('all', '0.0504')],\n",
       "             'P_15': [('100', '0.0667'),\n",
       "              ('101', '0.3333'),\n",
       "              ('102', '0.2000'),\n",
       "              ('104', '0.4000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.2000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1333'),\n",
       "              ('112', '0.2667'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.1333'),\n",
       "              ('116', '0.5333'),\n",
       "              ('117', '0.3333'),\n",
       "              ('118', '0.2667'),\n",
       "              ('119', '0.1333'),\n",
       "              ('121', '0.1333'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.2000'),\n",
       "              ('125', '0.2667'),\n",
       "              ('126', '0.2667'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4000'),\n",
       "              ('129', '0.0667'),\n",
       "              ('130', '0.5333'),\n",
       "              ('131', '0.0667'),\n",
       "              ('132', '0.7333'),\n",
       "              ('133', '0.6000'),\n",
       "              ('134', '0.4000'),\n",
       "              ('136', '0.0667'),\n",
       "              ('137', '0.1333'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.2000'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.0667'),\n",
       "              ('142', '0.1333'),\n",
       "              ('145', '0.2667'),\n",
       "              ('146', '0.4000'),\n",
       "              ('147', '0.0667'),\n",
       "              ('148', '0.1333'),\n",
       "              ('149', '0.0667'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.3333'),\n",
       "              ('154', '0.5333'),\n",
       "              ('156', '0.2667'),\n",
       "              ('157', '0.2667'),\n",
       "              ('159', '0.1333'),\n",
       "              ('160', '0.1333'),\n",
       "              ('161', '0.9333'),\n",
       "              ('162', '0.2667'),\n",
       "              ('163', '0.9333'),\n",
       "              ('164', '0.4667'),\n",
       "              ('166', '0.2667'),\n",
       "              ('168', '0.2000'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.1333'),\n",
       "              ('174', '0.8000'),\n",
       "              ('175', '0.4000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.0667'),\n",
       "              ('179', '0.0667'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '0.9333'),\n",
       "              ('184', '0.2000'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.1333'),\n",
       "              ('187', '0.0667'),\n",
       "              ('188', '0.4667'),\n",
       "              ('189', '0.4667'),\n",
       "              ('190', '0.1333'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.8667'),\n",
       "              ('194', '0.0667'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0667'),\n",
       "              ('197', '0.2000'),\n",
       "              ('198', '0.3333'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.6000'),\n",
       "              ('51', '0.6000'),\n",
       "              ('52', '0.8667'),\n",
       "              ('54', '0.2667'),\n",
       "              ('55', '0.2667'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.4000'),\n",
       "              ('59', '0.0667'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.2000'),\n",
       "              ('62', '0.2000'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.1333'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0667'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.4667'),\n",
       "              ('71', '0.4000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0667'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.5333'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.2667'),\n",
       "              ('82', '0.3333'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0667'),\n",
       "              ('85', '0.7333'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.1333'),\n",
       "              ('98', '0.2667'),\n",
       "              ('99', '0.0000'),\n",
       "              ('all', '0.2189')],\n",
       "             'P_20': [('100', '0.0500'),\n",
       "              ('101', '0.2500'),\n",
       "              ('102', '0.1500'),\n",
       "              ('104', '0.3500'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0500'),\n",
       "              ('107', '0.1500'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1500'),\n",
       "              ('112', '0.3000'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.1000'),\n",
       "              ('116', '0.4000'),\n",
       "              ('117', '0.4000'),\n",
       "              ('118', '0.3000'),\n",
       "              ('119', '0.2000'),\n",
       "              ('121', '0.1500'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.1500'),\n",
       "              ('125', '0.2500'),\n",
       "              ('126', '0.2500'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4500'),\n",
       "              ('129', '0.0500'),\n",
       "              ('130', '0.5000'),\n",
       "              ('131', '0.0500'),\n",
       "              ('132', '0.7500'),\n",
       "              ('133', '0.4500'),\n",
       "              ('134', '0.3000'),\n",
       "              ('136', '0.0500'),\n",
       "              ('137', '0.1500'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.2500'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.0500'),\n",
       "              ('142', '0.1000'),\n",
       "              ('145', '0.2500'),\n",
       "              ('146', '0.3500'),\n",
       "              ('147', '0.1000'),\n",
       "              ('148', '0.1500'),\n",
       "              ('149', '0.0500'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.2500'),\n",
       "              ('154', '0.5500'),\n",
       "              ('156', '0.2000'),\n",
       "              ('157', '0.2500'),\n",
       "              ('159', '0.1000'),\n",
       "              ('160', '0.2000'),\n",
       "              ('161', '0.8500'),\n",
       "              ('162', '0.2500'),\n",
       "              ('163', '0.9500'),\n",
       "              ('164', '0.5500'),\n",
       "              ('166', '0.2000'),\n",
       "              ('168', '0.1500'),\n",
       "              ('169', '0.1000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.1000'),\n",
       "              ('174', '0.7500'),\n",
       "              ('175', '0.3500'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.0500'),\n",
       "              ('179', '0.0500'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '0.9500'),\n",
       "              ('184', '0.1500'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.1500'),\n",
       "              ('187', '0.1500'),\n",
       "              ('188', '0.3500'),\n",
       "              ('189', '0.5000'),\n",
       "              ('190', '0.1000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.7500'),\n",
       "              ('194', '0.0500'),\n",
       "              ('195', '0.0500'),\n",
       "              ('196', '0.0500'),\n",
       "              ('197', '0.2000'),\n",
       "              ('198', '0.3500'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.5500'),\n",
       "              ('51', '0.6000'),\n",
       "              ('52', '0.9000'),\n",
       "              ('54', '0.2000'),\n",
       "              ('55', '0.4000'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.4000'),\n",
       "              ('59', '0.0500'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.2000'),\n",
       "              ('62', '0.2500'),\n",
       "              ('63', '0.0500'),\n",
       "              ('64', '0.1500'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0500'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.5500'),\n",
       "              ('71', '0.4500'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0500'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.5500'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.2000'),\n",
       "              ('82', '0.3500'),\n",
       "              ('83', '0.0500'),\n",
       "              ('84', '0.0500'),\n",
       "              ('85', '0.7000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.1000'),\n",
       "              ('97', '0.1000'),\n",
       "              ('98', '0.2000'),\n",
       "              ('99', '0.1000'),\n",
       "              ('all', '0.2138')],\n",
       "             'P_200': [('100', '0.0400'),\n",
       "              ('101', '0.1000'),\n",
       "              ('102', '0.0600'),\n",
       "              ('104', '0.2300'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0900'),\n",
       "              ('107', '0.1000'),\n",
       "              ('108', '0.0400'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2350'),\n",
       "              ('112', '0.0600'),\n",
       "              ('113', '0.0250'),\n",
       "              ('115', '0.1350'),\n",
       "              ('116', '0.0850'),\n",
       "              ('117', '0.1250'),\n",
       "              ('118', '0.1400'),\n",
       "              ('119', '0.1550'),\n",
       "              ('121', '0.0550'),\n",
       "              ('122', '0.0650'),\n",
       "              ('124', '0.0700'),\n",
       "              ('125', '0.1900'),\n",
       "              ('126', '0.0550'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.1900'),\n",
       "              ('129', '0.1300'),\n",
       "              ('130', '0.3800'),\n",
       "              ('131', '0.0200'),\n",
       "              ('132', '0.5300'),\n",
       "              ('133', '0.0650'),\n",
       "              ('134', '0.0300'),\n",
       "              ('136', '0.0400'),\n",
       "              ('137', '0.1450'),\n",
       "              ('138', '0.0550'),\n",
       "              ('139', '0.0900'),\n",
       "              ('140', '0.0450'),\n",
       "              ('141', '0.0250'),\n",
       "              ('142', '0.0300'),\n",
       "              ('145', '0.0700'),\n",
       "              ('146', '0.3200'),\n",
       "              ('147', '0.0900'),\n",
       "              ('148', '0.1000'),\n",
       "              ('149', '0.0550'),\n",
       "              ('150', '0.0100'),\n",
       "              ('152', '0.0700'),\n",
       "              ('153', '0.0750'),\n",
       "              ('154', '0.4550'),\n",
       "              ('156', '0.1750'),\n",
       "              ('157', '0.0900'),\n",
       "              ('159', '0.0150'),\n",
       "              ('160', '0.0450'),\n",
       "              ('161', '0.4500'),\n",
       "              ('162', '0.1550'),\n",
       "              ('163', '0.3400'),\n",
       "              ('164', '0.1400'),\n",
       "              ('166', '0.0500'),\n",
       "              ('168', '0.1350'),\n",
       "              ('169', '0.0800'),\n",
       "              ('171', '0.0100'),\n",
       "              ('172', '0.0250'),\n",
       "              ('174', '0.3100'),\n",
       "              ('175', '0.1800'),\n",
       "              ('176', '0.0250'),\n",
       "              ('177', '0.1600'),\n",
       "              ('178', '0.0550'),\n",
       "              ('179', '0.0300'),\n",
       "              ('181', '0.0200'),\n",
       "              ('183', '0.2750'),\n",
       "              ('184', '0.1100'),\n",
       "              ('185', '0.1700'),\n",
       "              ('186', '0.0400'),\n",
       "              ('187', '0.1550'),\n",
       "              ('188', '0.1650'),\n",
       "              ('189', '0.2100'),\n",
       "              ('190', '0.0550'),\n",
       "              ('191', '0.0050'),\n",
       "              ('193', '0.2700'),\n",
       "              ('194', '0.0650'),\n",
       "              ('195', '0.1000'),\n",
       "              ('196', '0.0500'),\n",
       "              ('197', '0.0450'),\n",
       "              ('198', '0.1450'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.1550'),\n",
       "              ('51', '0.1250'),\n",
       "              ('52', '0.6350'),\n",
       "              ('54', '0.2000'),\n",
       "              ('55', '0.5000'),\n",
       "              ('56', '0.6050'),\n",
       "              ('58', '0.2750'),\n",
       "              ('59', '0.1900'),\n",
       "              ('60', '0.0050'),\n",
       "              ('61', '0.1300'),\n",
       "              ('62', '0.1100'),\n",
       "              ('63', '0.0200'),\n",
       "              ('64', '0.0550'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0250'),\n",
       "              ('68', '0.0200'),\n",
       "              ('70', '0.2000'),\n",
       "              ('71', '0.1600'),\n",
       "              ('72', '0.0200'),\n",
       "              ('73', '0.0050'),\n",
       "              ('75', '0.0500'),\n",
       "              ('76', '0.0100'),\n",
       "              ('77', '0.3300'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0100'),\n",
       "              ('81', '0.1100'),\n",
       "              ('82', '0.2250'),\n",
       "              ('83', '0.0950'),\n",
       "              ('84', '0.0100'),\n",
       "              ('85', '0.5400'),\n",
       "              ('87', '0.0100'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0250'),\n",
       "              ('97', '0.0650'),\n",
       "              ('98', '0.0650'),\n",
       "              ('99', '0.1850'),\n",
       "              ('all', '0.1216')],\n",
       "             'P_30': [('100', '0.0667'),\n",
       "              ('101', '0.2000'),\n",
       "              ('102', '0.1000'),\n",
       "              ('104', '0.3000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0667'),\n",
       "              ('107', '0.2000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1667'),\n",
       "              ('112', '0.2667'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.1333'),\n",
       "              ('116', '0.3333'),\n",
       "              ('117', '0.4000'),\n",
       "              ('118', '0.2333'),\n",
       "              ('119', '0.1667'),\n",
       "              ('121', '0.1000'),\n",
       "              ('122', '0.0333'),\n",
       "              ('124', '0.1667'),\n",
       "              ('125', '0.2333'),\n",
       "              ('126', '0.1667'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4333'),\n",
       "              ('129', '0.0333'),\n",
       "              ('130', '0.6000'),\n",
       "              ('131', '0.0333'),\n",
       "              ('132', '0.8000'),\n",
       "              ('133', '0.3333'),\n",
       "              ('134', '0.2000'),\n",
       "              ('136', '0.1000'),\n",
       "              ('137', '0.1667'),\n",
       "              ('138', '0.0333'),\n",
       "              ('139', '0.1667'),\n",
       "              ('140', '0.1000'),\n",
       "              ('141', '0.0333'),\n",
       "              ('142', '0.0667'),\n",
       "              ('145', '0.1667'),\n",
       "              ('146', '0.4333'),\n",
       "              ('147', '0.1333'),\n",
       "              ('148', '0.2000'),\n",
       "              ('149', '0.0333'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0333'),\n",
       "              ('153', '0.2667'),\n",
       "              ('154', '0.5333'),\n",
       "              ('156', '0.2000'),\n",
       "              ('157', '0.2000'),\n",
       "              ('159', '0.0667'),\n",
       "              ('160', '0.1333'),\n",
       "              ('161', '0.7333'),\n",
       "              ('162', '0.2667'),\n",
       "              ('163', '0.9333'),\n",
       "              ('164', '0.5000'),\n",
       "              ('166', '0.1667'),\n",
       "              ('168', '0.1000'),\n",
       "              ('169', '0.1000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.1333'),\n",
       "              ('174', '0.7000'),\n",
       "              ('175', '0.3333'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.1000'),\n",
       "              ('178', '0.0333'),\n",
       "              ('179', '0.0333'),\n",
       "              ('181', '0.0667'),\n",
       "              ('183', '0.8333'),\n",
       "              ('184', '0.1667'),\n",
       "              ('185', '0.1000'),\n",
       "              ('186', '0.1000'),\n",
       "              ('187', '0.2333'),\n",
       "              ('188', '0.3000'),\n",
       "              ('189', '0.4000'),\n",
       "              ('190', '0.0667'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.7333'),\n",
       "              ('194', '0.0333'),\n",
       "              ('195', '0.0333'),\n",
       "              ('196', '0.0333'),\n",
       "              ('197', '0.1333'),\n",
       "              ('198', '0.2333'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.5000'),\n",
       "              ('51', '0.4333'),\n",
       "              ('52', '0.8333'),\n",
       "              ('54', '0.2333'),\n",
       "              ('55', '0.4333'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.4333'),\n",
       "              ('59', '0.1000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.1333'),\n",
       "              ('62', '0.1667'),\n",
       "              ('63', '0.0667'),\n",
       "              ('64', '0.1333'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0333'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.5000'),\n",
       "              ('71', '0.4667'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.1000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.5000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.1667'),\n",
       "              ('82', '0.3000'),\n",
       "              ('83', '0.0667'),\n",
       "              ('84', '0.0333'),\n",
       "              ('85', '0.6000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.1333'),\n",
       "              ('97', '0.1000'),\n",
       "              ('98', '0.1667'),\n",
       "              ('99', '0.1333'),\n",
       "              ('all', '0.2017')],\n",
       "             'P_5': [('100', '0.2000'),\n",
       "              ('101', '0.4000'),\n",
       "              ('102', '0.2000'),\n",
       "              ('104', '0.4000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.2000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2000'),\n",
       "              ('112', '0.2000'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.2000'),\n",
       "              ('116', '0.8000'),\n",
       "              ('117', '0.2000'),\n",
       "              ('118', '0.2000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.4000'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.6000'),\n",
       "              ('125', '0.2000'),\n",
       "              ('126', '0.6000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4000'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.8000'),\n",
       "              ('131', '0.0000'),\n",
       "              ('132', '0.8000'),\n",
       "              ('133', '0.8000'),\n",
       "              ('134', '0.8000'),\n",
       "              ('136', '0.2000'),\n",
       "              ('137', '0.4000'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.6000'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.0000'),\n",
       "              ('142', '0.2000'),\n",
       "              ('145', '0.4000'),\n",
       "              ('146', '0.4000'),\n",
       "              ('147', '0.2000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.2000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.8000'),\n",
       "              ('154', '0.4000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.4000'),\n",
       "              ('159', '0.0000'),\n",
       "              ('160', '0.4000'),\n",
       "              ('161', '1.0000'),\n",
       "              ('162', '0.2000'),\n",
       "              ('163', '0.8000'),\n",
       "              ('164', '0.8000'),\n",
       "              ('166', '0.4000'),\n",
       "              ('168', '0.0000'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.2000'),\n",
       "              ('174', '0.6000'),\n",
       "              ('175', '0.0000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.2000'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '1.0000'),\n",
       "              ('184', '0.6000'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.2000'),\n",
       "              ('188', '0.6000'),\n",
       "              ('189', '0.8000'),\n",
       "              ('190', '0.2000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '1.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.2000'),\n",
       "              ('197', '0.4000'),\n",
       "              ('198', '0.2000'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.6000'),\n",
       "              ('51', '0.8000'),\n",
       "              ('52', '0.8000'),\n",
       "              ('54', '0.0000'),\n",
       "              ('55', '0.2000'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.4000'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.4000'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.2000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '1.0000'),\n",
       "              ('71', '0.4000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.8000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.6000'),\n",
       "              ('82', '0.6000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.8000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.4000'),\n",
       "              ('98', '0.4000'),\n",
       "              ('99', '0.0000'),\n",
       "              ('all', '0.2667')],\n",
       "             'P_500': [('100', '0.0240'),\n",
       "              ('101', '0.0480'),\n",
       "              ('102', '0.0240'),\n",
       "              ('104', '0.0980'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0520'),\n",
       "              ('107', '0.0420'),\n",
       "              ('108', '0.0300'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2120'),\n",
       "              ('112', '0.0240'),\n",
       "              ('113', '0.0340'),\n",
       "              ('115', '0.1100'),\n",
       "              ('116', '0.0340'),\n",
       "              ('117', '0.0580'),\n",
       "              ('118', '0.1120'),\n",
       "              ('119', '0.1080'),\n",
       "              ('121', '0.0480'),\n",
       "              ('122', '0.0320'),\n",
       "              ('124', '0.0360'),\n",
       "              ('125', '0.1360'),\n",
       "              ('126', '0.0480'),\n",
       "              ('127', '0.0040'),\n",
       "              ('128', '0.0940'),\n",
       "              ('129', '0.1160'),\n",
       "              ('130', '0.2600'),\n",
       "              ('131', '0.0120'),\n",
       "              ('132', '0.2480'),\n",
       "              ('133', '0.0260'),\n",
       "              ('134', '0.0120'),\n",
       "              ('136', '0.0200'),\n",
       "              ('137', '0.0680'),\n",
       "              ('138', '0.0340'),\n",
       "              ('139', '0.0560'),\n",
       "              ('140', '0.0320'),\n",
       "              ('141', '0.0220'),\n",
       "              ('142', '0.0480'),\n",
       "              ('145', '0.0520'),\n",
       "              ('146', '0.2700'),\n",
       "              ('147', '0.0400'),\n",
       "              ('148', '0.0600'),\n",
       "              ('149', '0.0340'),\n",
       "              ('150', '0.0280'),\n",
       "              ('152', '0.0840'),\n",
       "              ('153', '0.0440'),\n",
       "              ('154', '0.2980'),\n",
       "              ('156', '0.1900'),\n",
       "              ('157', '0.0400'),\n",
       "              ('159', '0.0060'),\n",
       "              ('160', '0.0240'),\n",
       "              ('161', '0.2280'),\n",
       "              ('162', '0.0780'),\n",
       "              ('163', '0.1500'),\n",
       "              ('164', '0.0640'),\n",
       "              ('166', '0.0220'),\n",
       "              ('168', '0.0840'),\n",
       "              ('169', '0.0420'),\n",
       "              ('171', '0.0120'),\n",
       "              ('172', '0.0120'),\n",
       "              ('174', '0.1540'),\n",
       "              ('175', '0.0860'),\n",
       "              ('176', '0.0360'),\n",
       "              ('177', '0.0960'),\n",
       "              ('178', '0.0380'),\n",
       "              ('179', '0.0320'),\n",
       "              ('181', '0.0140'),\n",
       "              ('183', '0.1220'),\n",
       "              ('184', '0.0640'),\n",
       "              ('185', '0.1160'),\n",
       "              ('186', '0.0300'),\n",
       "              ('187', '0.1280'),\n",
       "              ('188', '0.1020'),\n",
       "              ('189', '0.2360'),\n",
       "              ('190', '0.0360'),\n",
       "              ('191', '0.0040'),\n",
       "              ('193', '0.1220'),\n",
       "              ('194', '0.0400'),\n",
       "              ('195', '0.0820'),\n",
       "              ('196', '0.0360'),\n",
       "              ('197', '0.0240'),\n",
       "              ('198', '0.0860'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.0720'),\n",
       "              ('51', '0.0580'),\n",
       "              ('52', '0.3600'),\n",
       "              ('54', '0.1320'),\n",
       "              ('55', '0.3840'),\n",
       "              ('56', '0.3800'),\n",
       "              ('58', '0.1520'),\n",
       "              ('59', '0.1260'),\n",
       "              ('60', '0.0040'),\n",
       "              ('61', '0.0820'),\n",
       "              ('62', '0.0560'),\n",
       "              ('63', '0.0080'),\n",
       "              ('64', '0.0860'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0320'),\n",
       "              ('68', '0.0240'),\n",
       "              ('70', '0.0800'),\n",
       "              ('71', '0.0900'),\n",
       "              ('72', '0.0160'),\n",
       "              ('73', '0.0020'),\n",
       "              ('75', '0.0200'),\n",
       "              ('76', '0.0100'),\n",
       "              ('77', '0.1320'),\n",
       "              ('79', '0.0060'),\n",
       "              ('80', '0.0100'),\n",
       "              ('81', '0.0740'),\n",
       "              ('82', '0.1380'),\n",
       "              ('83', '0.0800'),\n",
       "              ('84', '0.0180'),\n",
       "              ('85', '0.4140'),\n",
       "              ('87', '0.0060'),\n",
       "              ('88', '0.0100'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0120'),\n",
       "              ('97', '0.0300'),\n",
       "              ('98', '0.0300'),\n",
       "              ('99', '0.2120'),\n",
       "              ('all', '0.0771')],\n",
       "             'Rprec': [('100', '0.0465'),\n",
       "              ('101', '0.2222'),\n",
       "              ('102', '0.1579'),\n",
       "              ('104', '0.2745'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0851'),\n",
       "              ('107', '0.2000'),\n",
       "              ('108', '0.0319'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2093'),\n",
       "              ('112', '0.2500'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.1646'),\n",
       "              ('116', '0.4706'),\n",
       "              ('117', '0.4138'),\n",
       "              ('118', '0.1414'),\n",
       "              ('119', '0.1423'),\n",
       "              ('121', '0.0625'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.1379'),\n",
       "              ('125', '0.2421'),\n",
       "              ('126', '0.0575'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4107'),\n",
       "              ('129', '0.1257'),\n",
       "              ('130', '0.3596'),\n",
       "              ('131', '0.0000'),\n",
       "              ('132', '0.6423'),\n",
       "              ('133', '0.5385'),\n",
       "              ('134', '0.7500'),\n",
       "              ('136', '0.1000'),\n",
       "              ('137', '0.1852'),\n",
       "              ('138', '0.0278'),\n",
       "              ('139', '0.1064'),\n",
       "              ('140', '0.0800'),\n",
       "              ('141', '0.0455'),\n",
       "              ('142', '0.0357'),\n",
       "              ('145', '0.0971'),\n",
       "              ('146', '0.2938'),\n",
       "              ('147', '0.1356'),\n",
       "              ('148', '0.0909'),\n",
       "              ('149', '0.0333'),\n",
       "              ('150', '0.0085'),\n",
       "              ('152', '0.0900'),\n",
       "              ('153', '0.2162'),\n",
       "              ('154', '0.3267'),\n",
       "              ('156', '0.1758'),\n",
       "              ('157', '0.2500'),\n",
       "              ('159', '0.0000'),\n",
       "              ('160', '0.1538'),\n",
       "              ('161', '0.5714'),\n",
       "              ('162', '0.2273'),\n",
       "              ('163', '0.7692'),\n",
       "              ('164', '0.5294'),\n",
       "              ('166', '0.2105'),\n",
       "              ('168', '0.1111'),\n",
       "              ('169', '0.1250'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.2500'),\n",
       "              ('174', '0.4940'),\n",
       "              ('175', '0.3235'),\n",
       "              ('176', '0.0143'),\n",
       "              ('177', '0.1235'),\n",
       "              ('178', '0.0333'),\n",
       "              ('179', '0.0244'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '0.6557'),\n",
       "              ('184', '0.1818'),\n",
       "              ('185', '0.1619'),\n",
       "              ('186', '0.0698'),\n",
       "              ('187', '0.1701'),\n",
       "              ('188', '0.2090'),\n",
       "              ('189', '0.2279'),\n",
       "              ('190', '0.0645'),\n",
       "              ('191', '0.0079'),\n",
       "              ('193', '0.3917'),\n",
       "              ('194', '0.0526'),\n",
       "              ('195', '0.0921'),\n",
       "              ('196', '0.0357'),\n",
       "              ('197', '0.0794'),\n",
       "              ('198', '0.1711'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.4615'),\n",
       "              ('51', '0.4375'),\n",
       "              ('52', '0.5252'),\n",
       "              ('54', '0.2174'),\n",
       "              ('55', '0.4733'),\n",
       "              ('56', '0.5211'),\n",
       "              ('58', '0.3939'),\n",
       "              ('59', '0.1243'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.1194'),\n",
       "              ('62', '0.1013'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.0737'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0339'),\n",
       "              ('68', '0.0213'),\n",
       "              ('70', '0.5000'),\n",
       "              ('71', '0.1201'),\n",
       "              ('72', '0.0444'),\n",
       "              ('73', '0.0094'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.4957'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0060'),\n",
       "              ('81', '0.1509'),\n",
       "              ('82', '0.2683'),\n",
       "              ('83', '0.1012'),\n",
       "              ('84', '0.0455'),\n",
       "              ('85', '0.3925'),\n",
       "              ('87', '0.0192'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.1000'),\n",
       "              ('98', '0.2222'),\n",
       "              ('99', '0.1850'),\n",
       "              ('all', '0.1761')],\n",
       "             'bpref': [('100', '0.3072'),\n",
       "              ('101', '0.1811'),\n",
       "              ('102', '0.3878'),\n",
       "              ('104', '0.2810'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.2965'),\n",
       "              ('107', '0.3622'),\n",
       "              ('108', '0.1902'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.3858'),\n",
       "              ('112', '0.2153'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.3666'),\n",
       "              ('116', '0.4602'),\n",
       "              ('117', '0.4816'),\n",
       "              ('118', '0.2746'),\n",
       "              ('119', '0.2383'),\n",
       "              ('121', '0.0868'),\n",
       "              ('122', '0.0775'),\n",
       "              ('124', '0.2158'),\n",
       "              ('125', '0.4566'),\n",
       "              ('126', '0.2125'),\n",
       "              ('127', '0.0507'),\n",
       "              ('128', '0.7328'),\n",
       "              ('129', '0.3464'),\n",
       "              ('130', '0.5815'),\n",
       "              ('131', '0.0278'),\n",
       "              ('132', '0.7750'),\n",
       "              ('133', '0.5858'),\n",
       "              ('134', '0.6719'),\n",
       "              ('136', '0.0800'),\n",
       "              ('137', '0.5110'),\n",
       "              ('138', '0.0355'),\n",
       "              ('139', '0.0964'),\n",
       "              ('140', '0.0288'),\n",
       "              ('141', '0.0372'),\n",
       "              ('142', '0.1514'),\n",
       "              ('145', '0.2663'),\n",
       "              ('146', '0.5160'),\n",
       "              ('147', '0.1652'),\n",
       "              ('148', '0.1866'),\n",
       "              ('149', '0.0344'),\n",
       "              ('150', '0.1213'),\n",
       "              ('152', '0.2192'),\n",
       "              ('153', '0.1812'),\n",
       "              ('154', '0.3167'),\n",
       "              ('156', '0.3991'),\n",
       "              ('157', '0.2200'),\n",
       "              ('159', '0.3200'),\n",
       "              ('160', '0.1317'),\n",
       "              ('161', '0.7050'),\n",
       "              ('162', '0.4051'),\n",
       "              ('163', '0.7784'),\n",
       "              ('164', '0.4619'),\n",
       "              ('166', '0.1911'),\n",
       "              ('168', '0.0640'),\n",
       "              ('169', '0.0674'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.1875'),\n",
       "              ('174', '0.7175'),\n",
       "              ('175', '0.2247'),\n",
       "              ('176', '0.1481'),\n",
       "              ('177', '0.3861'),\n",
       "              ('178', '0.0311'),\n",
       "              ('179', '0.1095'),\n",
       "              ('181', '0.0500'),\n",
       "              ('183', '0.7425'),\n",
       "              ('184', '0.1312'),\n",
       "              ('185', '0.4587'),\n",
       "              ('186', '0.0768'),\n",
       "              ('187', '0.4063'),\n",
       "              ('188', '0.2644'),\n",
       "              ('189', '0.2335'),\n",
       "              ('190', '0.0666'),\n",
       "              ('191', '0.0485'),\n",
       "              ('193', '0.4201'),\n",
       "              ('194', '0.0815'),\n",
       "              ('195', '0.2567'),\n",
       "              ('196', '0.1084'),\n",
       "              ('197', '0.1547'),\n",
       "              ('198', '0.6150'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.4609'),\n",
       "              ('51', '0.4785'),\n",
       "              ('52', '0.7168'),\n",
       "              ('54', '0.2274'),\n",
       "              ('55', '0.7699'),\n",
       "              ('56', '0.7592'),\n",
       "              ('58', '0.5472'),\n",
       "              ('59', '0.1773'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.2308'),\n",
       "              ('62', '0.1174'),\n",
       "              ('63', '0.4400'),\n",
       "              ('64', '0.1967'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0579'),\n",
       "              ('68', '0.1734'),\n",
       "              ('70', '0.6594'),\n",
       "              ('71', '0.2165'),\n",
       "              ('72', '0.1383'),\n",
       "              ('73', '0.0137'),\n",
       "              ('75', '0.3077'),\n",
       "              ('76', '0.1435'),\n",
       "              ('77', '0.4285'),\n",
       "              ('79', '0.0432'),\n",
       "              ('80', '0.0261'),\n",
       "              ('81', '0.1125'),\n",
       "              ('82', '0.5370'),\n",
       "              ('83', '0.2096'),\n",
       "              ('84', '0.0620'),\n",
       "              ('85', '0.4430'),\n",
       "              ('87', '0.0385'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.1806'),\n",
       "              ('97', '0.3714'),\n",
       "              ('98', '0.4043'),\n",
       "              ('99', '0.5012'),\n",
       "              ('all', '0.2671')],\n",
       "             'gm_map': [('all', '0.0395')],\n",
       "             'iprec_at_recall_0.00': [('100', '0.3333'),\n",
       "              ('101', '1.0000'),\n",
       "              ('102', '0.3333'),\n",
       "              ('104', '0.6250'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.1410'),\n",
       "              ('107', '0.2500'),\n",
       "              ('108', '0.0488'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '1.0000'),\n",
       "              ('112', '1.0000'),\n",
       "              ('113', '0.0360'),\n",
       "              ('115', '0.3333'),\n",
       "              ('116', '1.0000'),\n",
       "              ('117', '0.4286'),\n",
       "              ('118', '0.4000'),\n",
       "              ('119', '0.2381'),\n",
       "              ('121', '1.0000'),\n",
       "              ('122', '0.0823'),\n",
       "              ('124', '1.0000'),\n",
       "              ('125', '0.5000'),\n",
       "              ('126', '1.0000'),\n",
       "              ('127', '0.0113'),\n",
       "              ('128', '0.5000'),\n",
       "              ('129', '0.1419'),\n",
       "              ('130', '1.0000'),\n",
       "              ('131', '0.1111'),\n",
       "              ('132', '1.0000'),\n",
       "              ('133', '1.0000'),\n",
       "              ('134', '1.0000'),\n",
       "              ('136', '0.3333'),\n",
       "              ('137', '0.5000'),\n",
       "              ('138', '0.0679'),\n",
       "              ('139', '1.0000'),\n",
       "              ('140', '0.1463'),\n",
       "              ('141', '0.1250'),\n",
       "              ('142', '0.2500'),\n",
       "              ('145', '1.0000'),\n",
       "              ('146', '1.0000'),\n",
       "              ('147', '0.2078'),\n",
       "              ('148', '0.2174'),\n",
       "              ('149', '0.3333'),\n",
       "              ('150', '0.0329'),\n",
       "              ('152', '0.0957'),\n",
       "              ('153', '1.0000'),\n",
       "              ('154', '1.0000'),\n",
       "              ('156', '0.2857'),\n",
       "              ('157', '1.0000'),\n",
       "              ('159', '0.1429'),\n",
       "              ('160', '1.0000'),\n",
       "              ('161', '1.0000'),\n",
       "              ('162', '1.0000'),\n",
       "              ('163', '0.9565'),\n",
       "              ('164', '0.8000'),\n",
       "              ('166', '0.5000'),\n",
       "              ('168', '0.2308'),\n",
       "              ('169', '0.1714'),\n",
       "              ('171', '0.0426'),\n",
       "              ('172', '1.0000'),\n",
       "              ('174', '0.8235'),\n",
       "              ('175', '0.4000'),\n",
       "              ('176', '0.0396'),\n",
       "              ('177', '0.1623'),\n",
       "              ('178', '0.2000'),\n",
       "              ('179', '0.0769'),\n",
       "              ('181', '0.0833'),\n",
       "              ('183', '1.0000'),\n",
       "              ('184', '1.0000'),\n",
       "              ('185', '0.1905'),\n",
       "              ('186', '0.1765'),\n",
       "              ('187', '1.0000'),\n",
       "              ('188', '1.0000'),\n",
       "              ('189', '0.8000'),\n",
       "              ('190', '0.5000'),\n",
       "              ('191', '0.0152'),\n",
       "              ('193', '1.0000'),\n",
       "              ('194', '0.1111'),\n",
       "              ('195', '0.1049'),\n",
       "              ('196', '0.2000'),\n",
       "              ('197', '1.0000'),\n",
       "              ('198', '0.5000'),\n",
       "              ('199', '0.0034'),\n",
       "              ('200', '1.0000'),\n",
       "              ('51', '1.0000'),\n",
       "              ('52', '1.0000'),\n",
       "              ('54', '0.3636'),\n",
       "              ('55', '0.5474'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.6667'),\n",
       "              ('59', '0.2553'),\n",
       "              ('60', '0.0064'),\n",
       "              ('61', '0.5000'),\n",
       "              ('62', '0.2632'),\n",
       "              ('63', '0.0870'),\n",
       "              ('64', '0.2000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.1429'),\n",
       "              ('68', '0.0312'),\n",
       "              ('70', '1.0000'),\n",
       "              ('71', '0.5714'),\n",
       "              ('72', '0.0606'),\n",
       "              ('73', '0.0114'),\n",
       "              ('75', '0.1071'),\n",
       "              ('76', '0.0162'),\n",
       "              ('77', '1.0000'),\n",
       "              ('79', '0.0090'),\n",
       "              ('80', '0.0238'),\n",
       "              ('81', '0.6667'),\n",
       "              ('82', '0.6000'),\n",
       "              ('83', '0.1233'),\n",
       "              ('84', '0.0769'),\n",
       "              ('85', '1.0000'),\n",
       "              ('87', '0.0238'),\n",
       "              ('88', '0.0203'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.1667'),\n",
       "              ('97', '1.0000'),\n",
       "              ('98', '1.0000'),\n",
       "              ('99', '0.2168'),\n",
       "              ('all', '0.4675')],\n",
       "             'iprec_at_recall_0.10': [('100', '0.0680'),\n",
       "              ('101', '0.5000'),\n",
       "              ('102', '0.3333'),\n",
       "              ('104', '0.5455'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.1410'),\n",
       "              ('107', '0.2195'),\n",
       "              ('108', '0.0346'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2455'),\n",
       "              ('112', '0.3333'),\n",
       "              ('113', '0.0360'),\n",
       "              ('115', '0.1646'),\n",
       "              ('116', '1.0000'),\n",
       "              ('117', '0.4286'),\n",
       "              ('118', '0.1923'),\n",
       "              ('119', '0.1920'),\n",
       "              ('121', '0.0637'),\n",
       "              ('122', '0.0823'),\n",
       "              ('124', '0.1935'),\n",
       "              ('125', '0.2687'),\n",
       "              ('126', '0.0601'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.5000'),\n",
       "              ('129', '0.1419'),\n",
       "              ('130', '0.5227'),\n",
       "              ('131', '0.1111'),\n",
       "              ('132', '0.8182'),\n",
       "              ('133', '1.0000'),\n",
       "              ('134', '1.0000'),\n",
       "              ('136', '0.3333'),\n",
       "              ('137', '0.2292'),\n",
       "              ('138', '0.0679'),\n",
       "              ('139', '0.2778'),\n",
       "              ('140', '0.1463'),\n",
       "              ('141', '0.0741'),\n",
       "              ('142', '0.0556'),\n",
       "              ('145', '0.0973'),\n",
       "              ('146', '0.3956'),\n",
       "              ('147', '0.2078'),\n",
       "              ('148', '0.0922'),\n",
       "              ('149', '0.0781'),\n",
       "              ('150', '0.0323'),\n",
       "              ('152', '0.0891'),\n",
       "              ('153', '1.0000'),\n",
       "              ('154', '0.5568'),\n",
       "              ('156', '0.1930'),\n",
       "              ('157', '0.4000'),\n",
       "              ('159', '0.1429'),\n",
       "              ('160', '0.2000'),\n",
       "              ('161', '1.0000'),\n",
       "              ('162', '0.2830'),\n",
       "              ('163', '0.9565'),\n",
       "              ('164', '0.8000'),\n",
       "              ('166', '0.5000'),\n",
       "              ('168', '0.1562'),\n",
       "              ('169', '0.1714'),\n",
       "              ('171', '0.0426'),\n",
       "              ('172', '1.0000'),\n",
       "              ('174', '0.8235'),\n",
       "              ('175', '0.3810'),\n",
       "              ('176', '0.0396'),\n",
       "              ('177', '0.1623'),\n",
       "              ('178', '0.0586'),\n",
       "              ('179', '0.0395'),\n",
       "              ('181', '0.0833'),\n",
       "              ('183', '1.0000'),\n",
       "              ('184', '0.1935'),\n",
       "              ('185', '0.1905'),\n",
       "              ('186', '0.0725'),\n",
       "              ('187', '0.2424'),\n",
       "              ('188', '0.5833'),\n",
       "              ('189', '0.2441'),\n",
       "              ('190', '0.0569'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.9231'),\n",
       "              ('194', '0.0653'),\n",
       "              ('195', '0.1049'),\n",
       "              ('196', '0.0542'),\n",
       "              ('197', '0.0516'),\n",
       "              ('198', '0.2727'),\n",
       "              ('199', '0.0034'),\n",
       "              ('200', '0.6111'),\n",
       "              ('51', '1.0000'),\n",
       "              ('52', '0.8983'),\n",
       "              ('54', '0.2432'),\n",
       "              ('55', '0.5474'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.4444'),\n",
       "              ('59', '0.1429'),\n",
       "              ('60', '0.0064'),\n",
       "              ('61', '0.1615'),\n",
       "              ('62', '0.1111'),\n",
       "              ('63', '0.0870'),\n",
       "              ('64', '0.1268'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0257'),\n",
       "              ('70', '1.0000'),\n",
       "              ('71', '0.1856'),\n",
       "              ('72', '0.0163'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.1071'),\n",
       "              ('76', '0.0162'),\n",
       "              ('77', '0.6087'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.1746'),\n",
       "              ('82', '0.2766'),\n",
       "              ('83', '0.1111'),\n",
       "              ('84', '0.0209'),\n",
       "              ('85', '0.6263'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0203'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.1667'),\n",
       "              ('97', '1.0000'),\n",
       "              ('98', '0.6667'),\n",
       "              ('99', '0.2168'),\n",
       "              ('all', '0.2920')],\n",
       "             'iprec_at_recall_0.20': [('100', '0.0317'),\n",
       "              ('101', '0.2308'),\n",
       "              ('102', '0.1364'),\n",
       "              ('104', '0.3298'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.1410'),\n",
       "              ('107', '0.2195'),\n",
       "              ('108', '0.0326'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2152'),\n",
       "              ('112', '0.3333'),\n",
       "              ('113', '0.0360'),\n",
       "              ('115', '0.1508'),\n",
       "              ('116', '1.0000'),\n",
       "              ('117', '0.4286'),\n",
       "              ('118', '0.1278'),\n",
       "              ('119', '0.1302'),\n",
       "              ('121', '0.0637'),\n",
       "              ('122', '0.0823'),\n",
       "              ('124', '0.0745'),\n",
       "              ('125', '0.2625'),\n",
       "              ('126', '0.0601'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.5000'),\n",
       "              ('129', '0.1419'),\n",
       "              ('130', '0.4742'),\n",
       "              ('131', '0.0500'),\n",
       "              ('132', '0.8169'),\n",
       "              ('133', '0.8571'),\n",
       "              ('134', '0.8571'),\n",
       "              ('136', '0.1200'),\n",
       "              ('137', '0.2292'),\n",
       "              ('138', '0.0679'),\n",
       "              ('139', '0.0994'),\n",
       "              ('140', '0.1463'),\n",
       "              ('141', '0.0296'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0744'),\n",
       "              ('146', '0.3265'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0698'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0778'),\n",
       "              ('153', '0.2667'),\n",
       "              ('154', '0.4892'),\n",
       "              ('156', '0.1930'),\n",
       "              ('157', '0.2727'),\n",
       "              ('159', '0.1429'),\n",
       "              ('160', '0.0661'),\n",
       "              ('161', '0.7568'),\n",
       "              ('162', '0.2609'),\n",
       "              ('163', '0.9565'),\n",
       "              ('164', '0.5500'),\n",
       "              ('166', '0.5000'),\n",
       "              ('168', '0.1562'),\n",
       "              ('169', '0.1429'),\n",
       "              ('171', '0.0426'),\n",
       "              ('172', '0.2857'),\n",
       "              ('174', '0.7500'),\n",
       "              ('175', '0.3590'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.1623'),\n",
       "              ('178', '0.0586'),\n",
       "              ('179', '0.0367'),\n",
       "              ('181', '0.0833'),\n",
       "              ('183', '0.9524'),\n",
       "              ('184', '0.1765'),\n",
       "              ('185', '0.1905'),\n",
       "              ('186', '0.0482'),\n",
       "              ('187', '0.1705'),\n",
       "              ('188', '0.2373'),\n",
       "              ('189', '0.2339'),\n",
       "              ('190', '0.0569'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.7500'),\n",
       "              ('194', '0.0530'),\n",
       "              ('195', '0.0935'),\n",
       "              ('196', '0.0542'),\n",
       "              ('197', '0.0141'),\n",
       "              ('198', '0.1459'),\n",
       "              ('199', '0.0034'),\n",
       "              ('200', '0.6111'),\n",
       "              ('51', '0.7778'),\n",
       "              ('52', '0.8788'),\n",
       "              ('54', '0.2377'),\n",
       "              ('55', '0.5474'),\n",
       "              ('56', '0.8406'),\n",
       "              ('58', '0.4384'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0064'),\n",
       "              ('61', '0.1615'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0870'),\n",
       "              ('64', '0.1268'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0253'),\n",
       "              ('70', '0.5714'),\n",
       "              ('71', '0.0778'),\n",
       "              ('72', '0.0159'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.1071'),\n",
       "              ('76', '0.0162'),\n",
       "              ('77', '0.6042'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.1746'),\n",
       "              ('82', '0.2766'),\n",
       "              ('83', '0.0665'),\n",
       "              ('84', '0.0209'),\n",
       "              ('85', '0.5618'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0203'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.1667'),\n",
       "              ('97', '0.1143'),\n",
       "              ('98', '0.3077'),\n",
       "              ('99', '0.2168'),\n",
       "              ('all', '0.2233')],\n",
       "             'iprec_at_recall_0.30': [('100', '0.0247'),\n",
       "              ('101', '0.1316'),\n",
       "              ('102', '0.1364'),\n",
       "              ('104', '0.3298'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.1119'),\n",
       "              ('107', '0.2195'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2140'),\n",
       "              ('112', '0.3333'),\n",
       "              ('113', '0.0360'),\n",
       "              ('115', '0.1508'),\n",
       "              ('116', '0.7273'),\n",
       "              ('117', '0.4286'),\n",
       "              ('118', '0.1033'),\n",
       "              ('119', '0.0864'),\n",
       "              ('121', '0.0637'),\n",
       "              ('122', '0.0823'),\n",
       "              ('124', '0.0384'),\n",
       "              ('125', '0.2130'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4500'),\n",
       "              ('129', '0.1337'),\n",
       "              ('130', '0.3819'),\n",
       "              ('131', '0.0500'),\n",
       "              ('132', '0.8169'),\n",
       "              ('133', '0.8571'),\n",
       "              ('134', '0.8571'),\n",
       "              ('136', '0.1200'),\n",
       "              ('137', '0.2292'),\n",
       "              ('138', '0.0679'),\n",
       "              ('139', '0.0994'),\n",
       "              ('140', '0.1067'),\n",
       "              ('141', '0.0268'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0355'),\n",
       "              ('146', '0.2964'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0617'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.1429'),\n",
       "              ('154', '0.4500'),\n",
       "              ('156', '0.1930'),\n",
       "              ('157', '0.2727'),\n",
       "              ('159', '0.1333'),\n",
       "              ('160', '0.0661'),\n",
       "              ('161', '0.6164'),\n",
       "              ('162', '0.2143'),\n",
       "              ('163', '0.9556'),\n",
       "              ('164', '0.5500'),\n",
       "              ('166', '0.1538'),\n",
       "              ('168', '0.1562'),\n",
       "              ('169', '0.1429'),\n",
       "              ('171', '0.0171'),\n",
       "              ('172', '0.1538'),\n",
       "              ('174', '0.7000'),\n",
       "              ('175', '0.3333'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.1623'),\n",
       "              ('178', '0.0586'),\n",
       "              ('179', '0.0360'),\n",
       "              ('181', '0.0833'),\n",
       "              ('183', '0.9524'),\n",
       "              ('184', '0.1647'),\n",
       "              ('185', '0.1719'),\n",
       "              ('186', '0.0346'),\n",
       "              ('187', '0.1518'),\n",
       "              ('188', '0.2264'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0569'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.5902'),\n",
       "              ('194', '0.0323'),\n",
       "              ('195', '0.0769'),\n",
       "              ('196', '0.0542'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.1459'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.5455'),\n",
       "              ('51', '0.6316'),\n",
       "              ('52', '0.8317'),\n",
       "              ('54', '0.2377'),\n",
       "              ('55', '0.5325'),\n",
       "              ('56', '0.7679'),\n",
       "              ('58', '0.4384'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0064'),\n",
       "              ('61', '0.1615'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0870'),\n",
       "              ('64', '0.1268'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0228'),\n",
       "              ('70', '0.5714'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0690'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.5104'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.1277'),\n",
       "              ('82', '0.2420'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0209'),\n",
       "              ('85', '0.4818'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0203'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.1667'),\n",
       "              ('97', '0.1143'),\n",
       "              ('98', '0.1875'),\n",
       "              ('99', '0.2168'),\n",
       "              ('all', '0.1949')],\n",
       "             'iprec_at_recall_0.40': [('100', '0.0000'),\n",
       "              ('101', '0.1188'),\n",
       "              ('102', '0.0882'),\n",
       "              ('104', '0.3298'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0936'),\n",
       "              ('107', '0.2182'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2060'),\n",
       "              ('112', '0.3333'),\n",
       "              ('113', '0.0360'),\n",
       "              ('115', '0.1410'),\n",
       "              ('116', '0.7273'),\n",
       "              ('117', '0.4286'),\n",
       "              ('118', '0.0956'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.0510'),\n",
       "              ('122', '0.0823'),\n",
       "              ('124', '0.0384'),\n",
       "              ('125', '0.1959'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4211'),\n",
       "              ('129', '0.1069'),\n",
       "              ('130', '0.3494'),\n",
       "              ('131', '0.0405'),\n",
       "              ('132', '0.8169'),\n",
       "              ('133', '0.8571'),\n",
       "              ('134', '0.8571'),\n",
       "              ('136', '0.1053'),\n",
       "              ('137', '0.2292'),\n",
       "              ('138', '0.0351'),\n",
       "              ('139', '0.0844'),\n",
       "              ('140', '0.0457'),\n",
       "              ('141', '0.0268'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0000'),\n",
       "              ('146', '0.2758'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0490'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.0893'),\n",
       "              ('154', '0.2043'),\n",
       "              ('156', '0.1852'),\n",
       "              ('157', '0.2424'),\n",
       "              ('159', '0.1333'),\n",
       "              ('160', '0.0428'),\n",
       "              ('161', '0.6129'),\n",
       "              ('162', '0.0907'),\n",
       "              ('163', '0.9556'),\n",
       "              ('164', '0.5455'),\n",
       "              ('166', '0.1304'),\n",
       "              ('168', '0.1500'),\n",
       "              ('169', '0.0955'),\n",
       "              ('171', '0.0171'),\n",
       "              ('172', '0.1538'),\n",
       "              ('174', '0.5000'),\n",
       "              ('175', '0.2478'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.1472'),\n",
       "              ('178', '0.0586'),\n",
       "              ('179', '0.0334'),\n",
       "              ('181', '0.0370'),\n",
       "              ('183', '0.8333'),\n",
       "              ('184', '0.1449'),\n",
       "              ('185', '0.1488'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.1290'),\n",
       "              ('188', '0.2014'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0569'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.3934'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0683'),\n",
       "              ('196', '0.0526'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.1260'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.5161'),\n",
       "              ('51', '0.5652'),\n",
       "              ('52', '0.7019'),\n",
       "              ('54', '0.2284'),\n",
       "              ('55', '0.5156'),\n",
       "              ('56', '0.6667'),\n",
       "              ('58', '0.3810'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0064'),\n",
       "              ('61', '0.1346'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0870'),\n",
       "              ('64', '0.1268'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.5278'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0690'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.5104'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.1264'),\n",
       "              ('82', '0.2227'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0209'),\n",
       "              ('85', '0.3773'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0357'),\n",
       "              ('97', '0.1143'),\n",
       "              ('98', '0.1279'),\n",
       "              ('99', '0.2159'),\n",
       "              ('all', '0.1716')],\n",
       "             'iprec_at_recall_0.50': [('100', '0.0000'),\n",
       "              ('101', '0.1186'),\n",
       "              ('102', '0.0813'),\n",
       "              ('104', '0.3298'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0654'),\n",
       "              ('107', '0.1974'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1962'),\n",
       "              ('112', '0.3333'),\n",
       "              ('113', '0.0360'),\n",
       "              ('115', '0.1129'),\n",
       "              ('116', '0.4286'),\n",
       "              ('117', '0.3571'),\n",
       "              ('118', '0.0000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.0510'),\n",
       "              ('122', '0.0823'),\n",
       "              ('124', '0.0378'),\n",
       "              ('125', '0.1788'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.3882'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.2907'),\n",
       "              ('131', '0.0405'),\n",
       "              ('132', '0.7188'),\n",
       "              ('133', '0.7778'),\n",
       "              ('134', '0.8571'),\n",
       "              ('136', '0.0735'),\n",
       "              ('137', '0.1667'),\n",
       "              ('138', '0.0351'),\n",
       "              ('139', '0.0650'),\n",
       "              ('140', '0.0414'),\n",
       "              ('141', '0.0251'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0000'),\n",
       "              ('146', '0.2356'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0478'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.0693'),\n",
       "              ('154', '0.0000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.2326'),\n",
       "              ('159', '0.0166'),\n",
       "              ('160', '0.0132'),\n",
       "              ('161', '0.5929'),\n",
       "              ('162', '0.0000'),\n",
       "              ('163', '0.9556'),\n",
       "              ('164', '0.5455'),\n",
       "              ('166', '0.1010'),\n",
       "              ('168', '0.1111'),\n",
       "              ('169', '0.0930'),\n",
       "              ('171', '0.0171'),\n",
       "              ('172', '0.1538'),\n",
       "              ('174', '0.4615'),\n",
       "              ('175', '0.2179'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.1284'),\n",
       "              ('178', '0.0554'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0165'),\n",
       "              ('183', '0.7083'),\n",
       "              ('184', '0.1410'),\n",
       "              ('185', '0.1377'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.1184'),\n",
       "              ('188', '0.1692'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0494'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.1760'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0505'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.0950'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.3636'),\n",
       "              ('51', '0.3721'),\n",
       "              ('52', '0.5517'),\n",
       "              ('54', '0.1888'),\n",
       "              ('55', '0.4602'),\n",
       "              ('56', '0.5577'),\n",
       "              ('58', '0.2931'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.1100'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0286'),\n",
       "              ('64', '0.0000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.5128'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0690'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.5038'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.1007'),\n",
       "              ('82', '0.2000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0159'),\n",
       "              ('85', '0.0000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0250'),\n",
       "              ('97', '0.1053'),\n",
       "              ('98', '0.1279'),\n",
       "              ('99', '0.2159'),\n",
       "              ('all', '0.1383')],\n",
       "             'iprec_at_recall_0.60': [('100', '0.0000'),\n",
       "              ('101', '0.1064'),\n",
       "              ('102', '0.0719'),\n",
       "              ('104', '0.3298'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0448'),\n",
       "              ('107', '0.1525'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.0000'),\n",
       "              ('112', '0.3333'),\n",
       "              ('113', '0.0314'),\n",
       "              ('115', '0.1129'),\n",
       "              ('116', '0.2973'),\n",
       "              ('117', '0.3167'),\n",
       "              ('118', '0.0000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.0510'),\n",
       "              ('122', '0.0823'),\n",
       "              ('124', '0.0000'),\n",
       "              ('125', '0.1740'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.3723'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.2299'),\n",
       "              ('131', '0.0404'),\n",
       "              ('132', '0.6667'),\n",
       "              ('133', '0.6000'),\n",
       "              ('134', '0.8571'),\n",
       "              ('136', '0.0611'),\n",
       "              ('137', '0.1031'),\n",
       "              ('138', '0.0314'),\n",
       "              ('139', '0.0555'),\n",
       "              ('140', '0.0389'),\n",
       "              ('141', '0.0183'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0000'),\n",
       "              ('146', '0.1990'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0348'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.0389'),\n",
       "              ('154', '0.0000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.1648'),\n",
       "              ('159', '0.0166'),\n",
       "              ('160', '0.0000'),\n",
       "              ('161', '0.5674'),\n",
       "              ('162', '0.0000'),\n",
       "              ('163', '0.9286'),\n",
       "              ('164', '0.5250'),\n",
       "              ('166', '0.0189'),\n",
       "              ('168', '0.1044'),\n",
       "              ('169', '0.0482'),\n",
       "              ('171', '0.0171'),\n",
       "              ('172', '0.0407'),\n",
       "              ('174', '0.4000'),\n",
       "              ('175', '0.1340'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0930'),\n",
       "              ('178', '0.0400'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0165'),\n",
       "              ('183', '0.6842'),\n",
       "              ('184', '0.1034'),\n",
       "              ('185', '0.0988'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.0000'),\n",
       "              ('188', '0.1246'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0377'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0424'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.0860'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.3210'),\n",
       "              ('51', '0.2532'),\n",
       "              ('52', '0.4744'),\n",
       "              ('54', '0.1526'),\n",
       "              ('55', '0.4482'),\n",
       "              ('56', '0.4560'),\n",
       "              ('58', '0.2402'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.0844'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0286'),\n",
       "              ('64', '0.0000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.5094'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0690'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.0000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.0846'),\n",
       "              ('82', '0.1162'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.0000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0129'),\n",
       "              ('97', '0.1053'),\n",
       "              ('98', '0.1279'),\n",
       "              ('99', '0.2152'),\n",
       "              ('all', '0.1120')],\n",
       "             'iprec_at_recall_0.70': [('100', '0.0000'),\n",
       "              ('101', '0.1064'),\n",
       "              ('102', '0.0000'),\n",
       "              ('104', '0.2826'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0382'),\n",
       "              ('107', '0.0482'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.0000'),\n",
       "              ('112', '0.2500'),\n",
       "              ('113', '0.0314'),\n",
       "              ('115', '0.1063'),\n",
       "              ('116', '0.2449'),\n",
       "              ('117', '0.2561'),\n",
       "              ('118', '0.0000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.0354'),\n",
       "              ('122', '0.0642'),\n",
       "              ('124', '0.0000'),\n",
       "              ('125', '0.1450'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.1136'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.0000'),\n",
       "              ('131', '0.0195'),\n",
       "              ('132', '0.6125'),\n",
       "              ('133', '0.4167'),\n",
       "              ('134', '0.8571'),\n",
       "              ('136', '0.0611'),\n",
       "              ('137', '0.0000'),\n",
       "              ('138', '0.0291'),\n",
       "              ('139', '0.0439'),\n",
       "              ('140', '0.0257'),\n",
       "              ('141', '0.0000'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0000'),\n",
       "              ('146', '0.0000'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.0363'),\n",
       "              ('154', '0.0000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.1648'),\n",
       "              ('159', '0.0000'),\n",
       "              ('160', '0.0000'),\n",
       "              ('161', '0.4455'),\n",
       "              ('162', '0.0000'),\n",
       "              ('163', '0.9167'),\n",
       "              ('164', '0.3478'),\n",
       "              ('166', '0.0154'),\n",
       "              ('168', '0.0784'),\n",
       "              ('169', '0.0420'),\n",
       "              ('171', '0.0171'),\n",
       "              ('172', '0.0156'),\n",
       "              ('174', '0.3429'),\n",
       "              ('175', '0.0676'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0663'),\n",
       "              ('178', '0.0302'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0153'),\n",
       "              ('183', '0.5972'),\n",
       "              ('184', '0.0829'),\n",
       "              ('185', '0.0744'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.0000'),\n",
       "              ('188', '0.1130'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0296'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0263'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.0720'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.2593'),\n",
       "              ('51', '0.1933'),\n",
       "              ('52', '0.2542'),\n",
       "              ('54', '0.1404'),\n",
       "              ('55', '0.4175'),\n",
       "              ('56', '0.3199'),\n",
       "              ('58', '0.1918'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.0789'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0215'),\n",
       "              ('64', '0.0000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.5000'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0629'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.0000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.0741'),\n",
       "              ('82', '0.0000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.0000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0098'),\n",
       "              ('97', '0.0407'),\n",
       "              ('98', '0.0903'),\n",
       "              ('99', '0.1997'),\n",
       "              ('all', '0.0853')],\n",
       "             'iprec_at_recall_0.80': [('100', '0.0000'),\n",
       "              ('101', '0.0928'),\n",
       "              ('102', '0.0000'),\n",
       "              ('104', '0.2789'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.0000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.0000'),\n",
       "              ('112', '0.2500'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.0896'),\n",
       "              ('116', '0.2121'),\n",
       "              ('117', '0.1805'),\n",
       "              ('118', '0.0000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.0000'),\n",
       "              ('122', '0.0340'),\n",
       "              ('124', '0.0000'),\n",
       "              ('125', '0.1166'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.1136'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.0000'),\n",
       "              ('131', '0.0195'),\n",
       "              ('132', '0.5263'),\n",
       "              ('133', '0.2821'),\n",
       "              ('134', '0.0000'),\n",
       "              ('136', '0.0611'),\n",
       "              ('137', '0.0000'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.0000'),\n",
       "              ('140', '0.0217'),\n",
       "              ('141', '0.0000'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0000'),\n",
       "              ('146', '0.0000'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.0000'),\n",
       "              ('154', '0.0000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.1197'),\n",
       "              ('159', '0.0000'),\n",
       "              ('160', '0.0000'),\n",
       "              ('161', '0.3603'),\n",
       "              ('162', '0.0000'),\n",
       "              ('163', '0.5833'),\n",
       "              ('164', '0.1806'),\n",
       "              ('166', '0.0000'),\n",
       "              ('168', '0.0621'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0171'),\n",
       "              ('172', '0.0000'),\n",
       "              ('174', '0.2226'),\n",
       "              ('175', '0.0000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.0000'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '0.5698'),\n",
       "              ('184', '0.0409'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.0000'),\n",
       "              ('188', '0.1015'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0000'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.0000'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.1350'),\n",
       "              ('51', '0.1079'),\n",
       "              ('52', '0.0000'),\n",
       "              ('54', '0.1091'),\n",
       "              ('55', '0.3625'),\n",
       "              ('56', '0.0000'),\n",
       "              ('58', '0.1286'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.0000'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0215'),\n",
       "              ('64', '0.0000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.4925'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.0000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.0000'),\n",
       "              ('82', '0.0000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.0000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.0195'),\n",
       "              ('98', '0.0579'),\n",
       "              ('99', '0.1821'),\n",
       "              ('all', '0.0513')],\n",
       "             'iprec_at_recall_0.90': [('100', '0.0000'),\n",
       "              ('101', '0.0449'),\n",
       "              ('102', '0.0000'),\n",
       "              ('104', '0.2486'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.0000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.0000'),\n",
       "              ('112', '0.1667'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.0800'),\n",
       "              ('116', '0.1441'),\n",
       "              ('117', '0.1262'),\n",
       "              ('118', '0.0000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.0000'),\n",
       "              ('122', '0.0181'),\n",
       "              ('124', '0.0000'),\n",
       "              ('125', '0.0000'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.0000'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.0000'),\n",
       "              ('131', '0.0130'),\n",
       "              ('132', '0.2633'),\n",
       "              ('133', '0.2791'),\n",
       "              ('134', '0.0000'),\n",
       "              ('136', '0.0425'),\n",
       "              ('137', '0.0000'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.0000'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.0000'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0000'),\n",
       "              ('146', '0.0000'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.0000'),\n",
       "              ('154', '0.0000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.0984'),\n",
       "              ('159', '0.0000'),\n",
       "              ('160', '0.0000'),\n",
       "              ('161', '0.1227'),\n",
       "              ('162', '0.0000'),\n",
       "              ('163', '0.2659'),\n",
       "              ('164', '0.0904'),\n",
       "              ('166', '0.0000'),\n",
       "              ('168', '0.0000'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.0000'),\n",
       "              ('174', '0.1659'),\n",
       "              ('175', '0.0000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.0000'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '0.2895'),\n",
       "              ('184', '0.0000'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.0000'),\n",
       "              ('188', '0.0000'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0000'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.0000'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.1136'),\n",
       "              ('51', '0.0782'),\n",
       "              ('52', '0.0000'),\n",
       "              ('54', '0.0000'),\n",
       "              ('55', '0.2415'),\n",
       "              ('56', '0.0000'),\n",
       "              ('58', '0.0000'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.0000'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.0000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.4235'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.0000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.0000'),\n",
       "              ('82', '0.0000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.0000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.0000'),\n",
       "              ('98', '0.0000'),\n",
       "              ('99', '0.0000'),\n",
       "              ('all', '0.0276')],\n",
       "             'iprec_at_recall_1.00': [('100', '0.0000'),\n",
       "              ('101', '0.0000'),\n",
       "              ('102', '0.0000'),\n",
       "              ('104', '0.0000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.0000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.0000'),\n",
       "              ('112', '0.1188'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.0000'),\n",
       "              ('116', '0.1214'),\n",
       "              ('117', '0.0775'),\n",
       "              ('118', '0.0000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.0000'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.0000'),\n",
       "              ('125', '0.0000'),\n",
       "              ('126', '0.0000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.0000'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.0000'),\n",
       "              ('131', '0.0130'),\n",
       "              ('132', '0.0000'),\n",
       "              ('133', '0.2766'),\n",
       "              ('134', '0.0000'),\n",
       "              ('136', '0.0321'),\n",
       "              ('137', '0.0000'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.0000'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.0000'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.0000'),\n",
       "              ('146', '0.0000'),\n",
       "              ('147', '0.0000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.0000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.0000'),\n",
       "              ('154', '0.0000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.0704'),\n",
       "              ('159', '0.0000'),\n",
       "              ('160', '0.0000'),\n",
       "              ('161', '0.0000'),\n",
       "              ('162', '0.0000'),\n",
       "              ('163', '0.0000'),\n",
       "              ('164', '0.0000'),\n",
       "              ('166', '0.0000'),\n",
       "              ('168', '0.0000'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.0000'),\n",
       "              ('174', '0.0000'),\n",
       "              ('175', '0.0000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.0000'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '0.1871'),\n",
       "              ('184', '0.0000'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.0000'),\n",
       "              ('188', '0.0000'),\n",
       "              ('189', '0.0000'),\n",
       "              ('190', '0.0000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '0.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0000'),\n",
       "              ('197', '0.0000'),\n",
       "              ('198', '0.0000'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.0000'),\n",
       "              ('51', '0.0531'),\n",
       "              ('52', '0.0000'),\n",
       "              ('54', '0.0000'),\n",
       "              ('55', '0.0000'),\n",
       "              ('56', '0.0000'),\n",
       "              ('58', '0.0000'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.0000'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.0000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '0.2817'),\n",
       "              ('71', '0.0000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.0000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.0000'),\n",
       "              ('82', '0.0000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.0000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.0000'),\n",
       "              ('98', '0.0000'),\n",
       "              ('99', '0.0000'),\n",
       "              ('all', '0.0103')],\n",
       "             'map': [('100', '0.0224'),\n",
       "              ('101', '0.2055'),\n",
       "              ('102', '0.0869'),\n",
       "              ('104', '0.3027'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0611'),\n",
       "              ('107', '0.1315'),\n",
       "              ('108', '0.0100'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1078'),\n",
       "              ('112', '0.3259'),\n",
       "              ('113', '0.0215'),\n",
       "              ('115', '0.1151'),\n",
       "              ('116', '0.5316'),\n",
       "              ('117', '0.2941'),\n",
       "              ('118', '0.0593'),\n",
       "              ('119', '0.0443'),\n",
       "              ('121', '0.0691'),\n",
       "              ('122', '0.0525'),\n",
       "              ('124', '0.0844'),\n",
       "              ('125', '0.1664'),\n",
       "              ('126', '0.0325'),\n",
       "              ('127', '0.0004'),\n",
       "              ('128', '0.2988'),\n",
       "              ('129', '0.0556'),\n",
       "              ('130', '0.2655'),\n",
       "              ('131', '0.0458'),\n",
       "              ('132', '0.6335'),\n",
       "              ('133', '0.6495'),\n",
       "              ('134', '0.6134'),\n",
       "              ('136', '0.0964'),\n",
       "              ('137', '0.1253'),\n",
       "              ('138', '0.0313'),\n",
       "              ('139', '0.1124'),\n",
       "              ('140', '0.0517'),\n",
       "              ('141', '0.0241'),\n",
       "              ('142', '0.0082'),\n",
       "              ('145', '0.0457'),\n",
       "              ('146', '0.1863'),\n",
       "              ('147', '0.0250'),\n",
       "              ('148', '0.0199'),\n",
       "              ('149', '0.0435'),\n",
       "              ('150', '0.0030'),\n",
       "              ('152', '0.0194'),\n",
       "              ('153', '0.1848'),\n",
       "              ('154', '0.1791'),\n",
       "              ('156', '0.0831'),\n",
       "              ('157', '0.2361'),\n",
       "              ('159', '0.0586'),\n",
       "              ('160', '0.1095'),\n",
       "              ('161', '0.5455'),\n",
       "              ('162', '0.1090'),\n",
       "              ('163', '0.7730'),\n",
       "              ('164', '0.4204'),\n",
       "              ('166', '0.1472'),\n",
       "              ('168', '0.0970'),\n",
       "              ('169', '0.0721'),\n",
       "              ('171', '0.0188'),\n",
       "              ('172', '0.2020'),\n",
       "              ('174', '0.4689'),\n",
       "              ('175', '0.1786'),\n",
       "              ('176', '0.0056'),\n",
       "              ('177', '0.0862'),\n",
       "              ('178', '0.0376'),\n",
       "              ('179', '0.0170'),\n",
       "              ('181', '0.0284'),\n",
       "              ('183', '0.7114'),\n",
       "              ('184', '0.1582'),\n",
       "              ('185', '0.0946'),\n",
       "              ('186', '0.0255'),\n",
       "              ('187', '0.0979'),\n",
       "              ('188', '0.2102'),\n",
       "              ('189', '0.0650'),\n",
       "              ('190', '0.0507'),\n",
       "              ('191', '0.0005'),\n",
       "              ('193', '0.3206'),\n",
       "              ('194', '0.0165'),\n",
       "              ('195', '0.0341'),\n",
       "              ('196', '0.0388'),\n",
       "              ('197', '0.0412'),\n",
       "              ('198', '0.1193'),\n",
       "              ('199', '0.0005'),\n",
       "              ('200', '0.3998'),\n",
       "              ('51', '0.4339'),\n",
       "              ('52', '0.4976'),\n",
       "              ('54', '0.1650'),\n",
       "              ('55', '0.4063'),\n",
       "              ('56', '0.5198'),\n",
       "              ('58', '0.2821'),\n",
       "              ('59', '0.0300'),\n",
       "              ('60', '0.0024'),\n",
       "              ('61', '0.1036'),\n",
       "              ('62', '0.0188'),\n",
       "              ('63', '0.0385'),\n",
       "              ('64', '0.0451'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0024'),\n",
       "              ('68', '0.0094'),\n",
       "              ('70', '0.5689'),\n",
       "              ('71', '0.0608'),\n",
       "              ('72', '0.0070'),\n",
       "              ('73', '0.0001'),\n",
       "              ('75', '0.0557'),\n",
       "              ('76', '0.0031'),\n",
       "              ('77', '0.3235'),\n",
       "              ('79', '0.0004'),\n",
       "              ('80', '0.0006'),\n",
       "              ('81', '0.1197'),\n",
       "              ('82', '0.1537'),\n",
       "              ('83', '0.0203'),\n",
       "              ('84', '0.0121'),\n",
       "              ('85', '0.2477'),\n",
       "              ('87', '0.0010'),\n",
       "              ('88', '0.0042'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0464'),\n",
       "              ('97', '0.1596'),\n",
       "              ('98', '0.2012'),\n",
       "              ('99', '0.1681'),\n",
       "              ('all', '0.1436')],\n",
       "             'num_q': [('all', '120')],\n",
       "             'num_rel': [('100', '43'),\n",
       "              ('101', '27'),\n",
       "              ('102', '19'),\n",
       "              ('104', '51'),\n",
       "              ('105', '33'),\n",
       "              ('106', '47'),\n",
       "              ('107', '30'),\n",
       "              ('108', '94'),\n",
       "              ('109', '8'),\n",
       "              ('110', '387'),\n",
       "              ('112', '12'),\n",
       "              ('113', '32'),\n",
       "              ('115', '79'),\n",
       "              ('116', '17'),\n",
       "              ('117', '29'),\n",
       "              ('118', '198'),\n",
       "              ('119', '239'),\n",
       "              ('121', '48'),\n",
       "              ('122', '20'),\n",
       "              ('124', '58'),\n",
       "              ('125', '95'),\n",
       "              ('126', '174'),\n",
       "              ('127', '165'),\n",
       "              ('128', '56'),\n",
       "              ('129', '167'),\n",
       "              ('130', '228'),\n",
       "              ('131', '6'),\n",
       "              ('132', '137'),\n",
       "              ('133', '13'),\n",
       "              ('134', '8'),\n",
       "              ('136', '10'),\n",
       "              ('137', '54'),\n",
       "              ('138', '36'),\n",
       "              ('139', '47'),\n",
       "              ('140', '25'),\n",
       "              ('141', '22'),\n",
       "              ('142', '336'),\n",
       "              ('145', '103'),\n",
       "              ('146', '320'),\n",
       "              ('147', '118'),\n",
       "              ('148', '220'),\n",
       "              ('149', '30'),\n",
       "              ('150', '236'),\n",
       "              ('152', '311'),\n",
       "              ('153', '37'),\n",
       "              ('154', '450'),\n",
       "              ('156', '330'),\n",
       "              ('157', '20'),\n",
       "              ('159', '5'),\n",
       "              ('160', '26'),\n",
       "              ('161', '133'),\n",
       "              ('162', '88'),\n",
       "              ('163', '78'),\n",
       "              ('164', '34'),\n",
       "              ('166', '19'),\n",
       "              ('168', '63'),\n",
       "              ('169', '32'),\n",
       "              ('171', '7'),\n",
       "              ('172', '8'),\n",
       "              ('174', '83'),\n",
       "              ('175', '68'),\n",
       "              ('176', '140'),\n",
       "              ('177', '81'),\n",
       "              ('178', '30'),\n",
       "              ('179', '41'),\n",
       "              ('181', '10'),\n",
       "              ('183', '61'),\n",
       "              ('184', '44'),\n",
       "              ('185', '105'),\n",
       "              ('186', '43'),\n",
       "              ('187', '147'),\n",
       "              ('188', '67'),\n",
       "              ('189', '882'),\n",
       "              ('190', '31'),\n",
       "              ('191', '127'),\n",
       "              ('193', '120'),\n",
       "              ('194', '76'),\n",
       "              ('195', '152'),\n",
       "              ('196', '28'),\n",
       "              ('197', '63'),\n",
       "              ('198', '76'),\n",
       "              ('199', '15'),\n",
       "              ('200', '39'),\n",
       "              ('51', '32'),\n",
       "              ('52', '278'),\n",
       "              ('54', '92'),\n",
       "              ('55', '243'),\n",
       "              ('56', '284'),\n",
       "              ('58', '99'),\n",
       "              ('59', '507'),\n",
       "              ('60', '9'),\n",
       "              ('61', '67'),\n",
       "              ('62', '227'),\n",
       "              ('63', '5'),\n",
       "              ('64', '285'),\n",
       "              ('65', '0'),\n",
       "              ('66', '2'),\n",
       "              ('67', '442'),\n",
       "              ('68', '47'),\n",
       "              ('70', '40'),\n",
       "              ('71', '308'),\n",
       "              ('72', '45'),\n",
       "              ('73', '106'),\n",
       "              ('75', '13'),\n",
       "              ('76', '69'),\n",
       "              ('77', '115'),\n",
       "              ('79', '128'),\n",
       "              ('80', '168'),\n",
       "              ('81', '53'),\n",
       "              ('82', '123'),\n",
       "              ('83', '247'),\n",
       "              ('84', '22'),\n",
       "              ('85', '614'),\n",
       "              ('87', '52'),\n",
       "              ('88', '66'),\n",
       "              ('91', '5'),\n",
       "              ('96', '12'),\n",
       "              ('97', '20'),\n",
       "              ('98', '18'),\n",
       "              ('99', '173'),\n",
       "              ('all', '13263')],\n",
       "             'num_rel_ret': [('100', '14'),\n",
       "              ('101', '25'),\n",
       "              ('102', '13'),\n",
       "              ('104', '50'),\n",
       "              ('105', '0'),\n",
       "              ('106', '36'),\n",
       "              ('107', '22'),\n",
       "              ('108', '28'),\n",
       "              ('109', '0'),\n",
       "              ('110', '196'),\n",
       "              ('112', '12'),\n",
       "              ('113', '24'),\n",
       "              ('115', '74'),\n",
       "              ('116', '17'),\n",
       "              ('117', '29'),\n",
       "              ('118', '83'),\n",
       "              ('119', '77'),\n",
       "              ('121', '35'),\n",
       "              ('122', '18'),\n",
       "              ('124', '32'),\n",
       "              ('125', '81'),\n",
       "              ('126', '51'),\n",
       "              ('127', '9'),\n",
       "              ('128', '47'),\n",
       "              ('129', '82'),\n",
       "              ('130', '159'),\n",
       "              ('131', '6'),\n",
       "              ('132', '131'),\n",
       "              ('133', '13'),\n",
       "              ('134', '6'),\n",
       "              ('136', '10'),\n",
       "              ('137', '37'),\n",
       "              ('138', '27'),\n",
       "              ('139', '34'),\n",
       "              ('140', '20'),\n",
       "              ('141', '14'),\n",
       "              ('142', '52'),\n",
       "              ('145', '33'),\n",
       "              ('146', '196'),\n",
       "              ('147', '22'),\n",
       "              ('148', '43'),\n",
       "              ('149', '20'),\n",
       "              ('150', '29'),\n",
       "              ('152', '77'),\n",
       "              ('153', '28'),\n",
       "              ('154', '181'),\n",
       "              ('156', '149'),\n",
       "              ('157', '20'),\n",
       "              ('159', '3'),\n",
       "              ('160', '13'),\n",
       "              ('161', '121'),\n",
       "              ('162', '43'),\n",
       "              ('163', '75'),\n",
       "              ('164', '32'),\n",
       "              ('166', '15'),\n",
       "              ('168', '55'),\n",
       "              ('169', '24'),\n",
       "              ('171', '6'),\n",
       "              ('172', '6'),\n",
       "              ('174', '82'),\n",
       "              ('175', '52'),\n",
       "              ('176', '25'),\n",
       "              ('177', '58'),\n",
       "              ('178', '21'),\n",
       "              ('179', '20'),\n",
       "              ('181', '7'),\n",
       "              ('183', '61'),\n",
       "              ('184', '38'),\n",
       "              ('185', '74'),\n",
       "              ('186', '17'),\n",
       "              ('187', '87'),\n",
       "              ('188', '58'),\n",
       "              ('189', '227'),\n",
       "              ('190', '22'),\n",
       "              ('191', '7'),\n",
       "              ('193', '63'),\n",
       "              ('194', '25'),\n",
       "              ('195', '65'),\n",
       "              ('196', '21'),\n",
       "              ('197', '14'),\n",
       "              ('198', '56'),\n",
       "              ('199', '3'),\n",
       "              ('200', '37'),\n",
       "              ('51', '32'),\n",
       "              ('52', '207'),\n",
       "              ('54', '82'),\n",
       "              ('55', '221'),\n",
       "              ('56', '227'),\n",
       "              ('58', '85'),\n",
       "              ('59', '99'),\n",
       "              ('60', '4'),\n",
       "              ('61', '53'),\n",
       "              ('62', '35'),\n",
       "              ('63', '4'),\n",
       "              ('64', '125'),\n",
       "              ('65', '0'),\n",
       "              ('66', '0'),\n",
       "              ('67', '29'),\n",
       "              ('68', '18'),\n",
       "              ('70', '40'),\n",
       "              ('71', '77'),\n",
       "              ('72', '13'),\n",
       "              ('73', '2'),\n",
       "              ('75', '10'),\n",
       "              ('76', '16'),\n",
       "              ('77', '66'),\n",
       "              ('79', '7'),\n",
       "              ('80', '9'),\n",
       "              ('81', '37'),\n",
       "              ('82', '80'),\n",
       "              ('83', '58'),\n",
       "              ('84', '12'),\n",
       "              ('85', '294'),\n",
       "              ('87', '3'),\n",
       "              ('88', '20'),\n",
       "              ('91', '0'),\n",
       "              ('96', '9'),\n",
       "              ('97', '17'),\n",
       "              ('98', '15'),\n",
       "              ('99', '152'),\n",
       "              ('all', '6053')],\n",
       "             'num_ret': [('100', '1000'),\n",
       "              ('101', '1000'),\n",
       "              ('102', '1000'),\n",
       "              ('104', '1000'),\n",
       "              ('105', '1000'),\n",
       "              ('106', '1000'),\n",
       "              ('107', '1000'),\n",
       "              ('108', '1000'),\n",
       "              ('109', '1000'),\n",
       "              ('110', '1000'),\n",
       "              ('112', '1000'),\n",
       "              ('113', '1000'),\n",
       "              ('115', '1000'),\n",
       "              ('116', '1000'),\n",
       "              ('117', '1000'),\n",
       "              ('118', '1000'),\n",
       "              ('119', '1000'),\n",
       "              ('121', '1000'),\n",
       "              ('122', '1000'),\n",
       "              ('124', '1000'),\n",
       "              ('125', '1000'),\n",
       "              ('126', '1000'),\n",
       "              ('127', '1000'),\n",
       "              ('128', '1000'),\n",
       "              ('129', '1000'),\n",
       "              ('130', '1000'),\n",
       "              ('131', '1000'),\n",
       "              ('132', '1000'),\n",
       "              ('133', '1000'),\n",
       "              ('134', '1000'),\n",
       "              ('136', '1000'),\n",
       "              ('137', '1000'),\n",
       "              ('138', '1000'),\n",
       "              ('139', '1000'),\n",
       "              ('140', '1000'),\n",
       "              ('141', '1000'),\n",
       "              ('142', '1000'),\n",
       "              ('145', '1000'),\n",
       "              ('146', '1000'),\n",
       "              ('147', '1000'),\n",
       "              ('148', '1000'),\n",
       "              ('149', '1000'),\n",
       "              ('150', '1000'),\n",
       "              ('152', '1000'),\n",
       "              ('153', '1000'),\n",
       "              ('154', '1000'),\n",
       "              ('156', '1000'),\n",
       "              ('157', '1000'),\n",
       "              ('159', '1000'),\n",
       "              ('160', '1000'),\n",
       "              ('161', '1000'),\n",
       "              ('162', '1000'),\n",
       "              ('163', '1000'),\n",
       "              ('164', '1000'),\n",
       "              ('166', '1000'),\n",
       "              ('168', '1000'),\n",
       "              ('169', '1000'),\n",
       "              ('171', '1000'),\n",
       "              ('172', '1000'),\n",
       "              ('174', '1000'),\n",
       "              ('175', '1000'),\n",
       "              ('176', '1000'),\n",
       "              ('177', '1000'),\n",
       "              ('178', '1000'),\n",
       "              ('179', '1000'),\n",
       "              ('181', '1000'),\n",
       "              ('183', '1000'),\n",
       "              ('184', '1000'),\n",
       "              ('185', '1000'),\n",
       "              ('186', '1000'),\n",
       "              ('187', '1000'),\n",
       "              ('188', '1000'),\n",
       "              ('189', '1000'),\n",
       "              ('190', '1000'),\n",
       "              ('191', '1000'),\n",
       "              ('193', '1000'),\n",
       "              ('194', '1000'),\n",
       "              ('195', '1000'),\n",
       "              ('196', '1000'),\n",
       "              ('197', '1000'),\n",
       "              ('198', '1000'),\n",
       "              ('199', '1000'),\n",
       "              ('200', '1000'),\n",
       "              ('51', '1000'),\n",
       "              ('52', '1000'),\n",
       "              ('54', '1000'),\n",
       "              ('55', '1000'),\n",
       "              ('56', '1000'),\n",
       "              ('58', '1000'),\n",
       "              ('59', '1000'),\n",
       "              ('60', '1000'),\n",
       "              ('61', '1000'),\n",
       "              ('62', '1000'),\n",
       "              ('63', '1000'),\n",
       "              ('64', '1000'),\n",
       "              ('65', '1000'),\n",
       "              ('66', '1000'),\n",
       "              ('67', '1000'),\n",
       "              ('68', '1000'),\n",
       "              ('70', '1000'),\n",
       "              ('71', '1000'),\n",
       "              ('72', '1000'),\n",
       "              ('73', '1000'),\n",
       "              ('75', '1000'),\n",
       "              ('76', '1000'),\n",
       "              ('77', '1000'),\n",
       "              ('79', '1000'),\n",
       "              ('80', '1000'),\n",
       "              ('81', '1000'),\n",
       "              ('82', '1000'),\n",
       "              ('83', '1000'),\n",
       "              ('84', '1000'),\n",
       "              ('85', '1000'),\n",
       "              ('87', '1000'),\n",
       "              ('88', '1000'),\n",
       "              ('91', '1000'),\n",
       "              ('96', '1000'),\n",
       "              ('97', '1000'),\n",
       "              ('98', '1000'),\n",
       "              ('99', '1000'),\n",
       "              ('all', '120000')],\n",
       "             'recip_rank': [('100', '0.3333'),\n",
       "              ('101', '1.0000'),\n",
       "              ('102', '0.2500'),\n",
       "              ('104', '0.5000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0625'),\n",
       "              ('107', '0.2500'),\n",
       "              ('108', '0.0303'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '1.0000'),\n",
       "              ('112', '1.0000'),\n",
       "              ('113', '0.0130'),\n",
       "              ('115', '0.3333'),\n",
       "              ('116', '1.0000'),\n",
       "              ('117', '0.2000'),\n",
       "              ('118', '0.3333'),\n",
       "              ('119', '0.1250'),\n",
       "              ('121', '1.0000'),\n",
       "              ('122', '0.0385'),\n",
       "              ('124', '1.0000'),\n",
       "              ('125', '0.5000'),\n",
       "              ('126', '1.0000'),\n",
       "              ('127', '0.0029'),\n",
       "              ('128', '0.3333'),\n",
       "              ('129', '0.0769'),\n",
       "              ('130', '1.0000'),\n",
       "              ('131', '0.1111'),\n",
       "              ('132', '1.0000'),\n",
       "              ('133', '1.0000'),\n",
       "              ('134', '1.0000'),\n",
       "              ('136', '0.3333'),\n",
       "              ('137', '0.3333'),\n",
       "              ('138', '0.0400'),\n",
       "              ('139', '1.0000'),\n",
       "              ('140', '0.0455'),\n",
       "              ('141', '0.1250'),\n",
       "              ('142', '0.2500'),\n",
       "              ('145', '1.0000'),\n",
       "              ('146', '1.0000'),\n",
       "              ('147', '0.2000'),\n",
       "              ('148', '0.1000'),\n",
       "              ('149', '0.3333'),\n",
       "              ('150', '0.0100'),\n",
       "              ('152', '0.0357'),\n",
       "              ('153', '1.0000'),\n",
       "              ('154', '1.0000'),\n",
       "              ('156', '0.1667'),\n",
       "              ('157', '1.0000'),\n",
       "              ('159', '0.1429'),\n",
       "              ('160', '1.0000'),\n",
       "              ('161', '1.0000'),\n",
       "              ('162', '1.0000'),\n",
       "              ('163', '0.5000'),\n",
       "              ('164', '0.5000'),\n",
       "              ('166', '0.3333'),\n",
       "              ('168', '0.1429'),\n",
       "              ('169', '0.0588'),\n",
       "              ('171', '0.0286'),\n",
       "              ('172', '1.0000'),\n",
       "              ('174', '0.5000'),\n",
       "              ('175', '0.1250'),\n",
       "              ('176', '0.0278'),\n",
       "              ('177', '0.0455'),\n",
       "              ('178', '0.2000'),\n",
       "              ('179', '0.0769'),\n",
       "              ('181', '0.0417'),\n",
       "              ('183', '1.0000'),\n",
       "              ('184', '1.0000'),\n",
       "              ('185', '0.0400'),\n",
       "              ('186', '0.1429'),\n",
       "              ('187', '1.0000'),\n",
       "              ('188', '1.0000'),\n",
       "              ('189', '0.5000'),\n",
       "              ('190', '0.5000'),\n",
       "              ('191', '0.0152'),\n",
       "              ('193', '1.0000'),\n",
       "              ('194', '0.1111'),\n",
       "              ('195', '0.0526'),\n",
       "              ('196', '0.2000'),\n",
       "              ('197', '1.0000'),\n",
       "              ('198', '0.5000'),\n",
       "              ('199', '0.0014'),\n",
       "              ('200', '1.0000'),\n",
       "              ('51', '1.0000'),\n",
       "              ('52', '1.0000'),\n",
       "              ('54', '0.1667'),\n",
       "              ('55', '0.2000'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.5000'),\n",
       "              ('59', '0.1111'),\n",
       "              ('60', '0.0060'),\n",
       "              ('61', '0.3333'),\n",
       "              ('62', '0.1250'),\n",
       "              ('63', '0.0556'),\n",
       "              ('64', '0.2000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.1429'),\n",
       "              ('68', '0.0312'),\n",
       "              ('70', '1.0000'),\n",
       "              ('71', '0.2500'),\n",
       "              ('72', '0.0323'),\n",
       "              ('73', '0.0114'),\n",
       "              ('75', '0.0714'),\n",
       "              ('76', '0.0139'),\n",
       "              ('77', '1.0000'),\n",
       "              ('79', '0.0046'),\n",
       "              ('80', '0.0238'),\n",
       "              ('81', '0.5000'),\n",
       "              ('82', '0.3333'),\n",
       "              ('83', '0.0500'),\n",
       "              ('84', '0.0769'),\n",
       "              ('85', '1.0000'),\n",
       "              ('87', '0.0238'),\n",
       "              ('88', '0.0038'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0588'),\n",
       "              ('97', '1.0000'),\n",
       "              ('98', '1.0000'),\n",
       "              ('99', '0.0556'),\n",
       "              ('all', '0.4136')],\n",
       "             'runid': [('all', 'TF-IDF')]})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "    $ ./trec_eval/trec_eval -q ap_88_89/qrel_test ap_88_89/TF-IDF.run | grep -E \"^map\\s\"\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164598\n",
      "1\n",
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(index.maximum_document())\n",
    "print(index.document_base())\n",
    "\n",
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412, 92160, 3, 192, 0, 363, 34, 1441, 0, 174134, 0, 200, 0, 894, 412, 2652, 0, 810, 107, 49, 4903, 420, 0, 1, 48, 35, 489, 0, 35, 687, 192, 243, 0, 249311, 1877, 0, 1651, 1174, 0, 2701, 117, 412, 0, 810, 391, 245233, 1225, 5838, 16, 0, 233156, 3496, 0, 393, 17, 0, 2435, 4819, 930, 0, 0, 200, 0, 894, 0, 22, 398, 145, 0, 3, 271, 115, 0, 1176, 2777, 292, 0, 725, 192, 0, 0, 50046, 0, 1901, 1130, 0, 192, 0, 408, 0, 243779, 0, 0, 553, 192, 0, 363, 0, 3747, 0, 0, 0, 0, 1176, 0, 1239, 0, 0, 1115, 17, 0, 0, 585, 192, 1963, 0, 0, 412, 54356, 0, 773, 0, 0, 0, 192, 0, 0, 1130, 0, 363, 0, 545, 192, 0, 1174, 1901, 1130, 0, 4, 398, 145, 39, 0, 577, 0, 355, 0, 491, 0, 6025, 0, 0, 193156, 88, 34, 437, 0, 0, 1852, 0, 828, 0, 1588, 0, 0, 0, 2615, 0, 0, 107, 49, 420, 0, 0, 190, 7, 714, 2701, 0, 237, 192, 157, 0, 412, 34, 437, 0, 0, 200, 6025, 26, 0, 0, 0, 0, 363, 0, 22, 398, 145, 0, 200, 638, 126222, 6018, 0, 880, 0, 0, 161, 0, 0, 319, 894, 2701, 0, 0, 0, 301, 1200, 0, 363, 251, 430, 0, 207, 0, 76143, 1773, 0, 243779, 0, 0, 72030, 0, 55, 4903, 420, 0, 2701, 1496, 420, 0, 25480, 0, 420, 0, 0, 200, 0, 392, 2949, 0, 1738, 0, 61, 0, 71, 79, 0, 200, 903, 0, 188, 53, 6, 0, 476, 2, 0, 2028, 97, 334, 0, 0, 200, 178, 0, 0, 107, 49, 0, 214, 0, 0, 0, 114, 3866, 1505, 195, 79893, 574, 0, 198, 2160, 0, 192, 0, 420, 0, 384, 0, 2701, 0, 114, 6025, 1549, 74627, 0, 238, 0, 0, 0, 3729, 0, 192, 0, 79893, 0, 0, 729, 3141, 129, 0, 192, 196764, 39, 0, 0, 714, 63, 0, 55, 420, 3356, 0, 0, 117, 412, 0, 0, 79758, 0, 1901, 1130, 4067, 2133, 0, 0, 875, 72, 0, 0, 336, 2789, 0, 0, 25, 920, 121, 104, 0, 3162, 0, 0, 420, 0, 2178, 0, 0, 386, 192545, 159306, 0, 0, 0, 1914, 0, 200, 0, 1794, 0, 2654, 0, 0, 25480, 420, 0, 2795, 0, 0, 229690, 0, 32559, 0, 0, 392, 253919, 0, 0, 0, 0, 379, 0, 0, 114, 0, 553, 10, 0, 1128, 0, 23610, 248, 151, 0, 418, 0, 651, 0, 36, 0, 0, 645, 0, 0, 513, 0, 0, 25480, 420, 34, 0, 0, 0, 15, 0, 3348, 0, 3496, 0, 35, 687, 0, 1, 48, 0, 0, 2803, 0, 0, 714, 1274, 0, 114, 62, 1006, 70268, 1200, 2357, 0, 497, 0, 497, 125, 0, 913, 4647, 3985, 0, 0, 3370, 245233, 0, 0, 687, 0, 4, 1288, 0, 0, 0, 0, 715, 0, 0, 687, 583, 0, 0, 1627, 0, 0, 11, 357, 1359, 0, 849, 0, 0, 1518, 462, 245233, 0, 0, 0, 0, 0, 0, 171, 70268, 0))\n",
      "521\n"
     ]
    }
   ],
   "source": [
    "# APXXXXXXXXX corresponds to the unique identifier of a document:\n",
    "# e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier.\n",
    "\n",
    "example_document = index.document(1)\n",
    "print(example_document)\n",
    "print(len(example_document[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time')]\n",
      "267318\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])\n",
    "print(len(id2token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                print(\"Max capacity reached... Breaking...\")\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of  in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of  [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of  in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of soft passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use  equal to 50, and Dirichlet smoothing with  optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand how the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Dont forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  164597\n",
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 65.20294213294983 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "print('Total number of documents: ', num_documents)\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "#print('Query term IDs: ', query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "# Used to build query/document relevance matrix in task 4\n",
    "ext2int = defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    \n",
    "    ext2int[ext_doc_id] = int_doc_id\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queries:  150\n",
      "Average document length:  256.4381975370147\n"
     ]
    }
   ],
   "source": [
    "print('Total number of queries: ', len(queries))\n",
    "print('Average document length: ', avg_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['beachfront']\n",
      "Query by ids with stopwords: [36062]\n",
      "Query by ids without stopwords: [36062]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"Beachfront\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)\n",
    "print(inverted_index[query_id_tokens[0]][649])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0. 171350.      0.      0.      0.      0.      0.      0. 119771.\n",
      "      0.      0.  96612.      0.  84131.      0.      0.      0.      0.\n",
      "      0.  73013.  71782.  69256.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.  56945.      0.  55709.\n",
      "      0.      0.  54180.  53986.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "  46470.  46166.      0.  45936.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.  41803.  41350.      0.      0.\n",
      "  40954.      0.      0.  40696.      0.  40579.  40331.      0.  40019.\n",
      "      0.  39433.      0.      0.  38950.      0.      0.      0.      0.\n",
      "      0.      0.  37135.      0.  36935.      0.      0.  36376.      0.\n",
      "      0.]\n",
      "(267319,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"Helper functions\"\"\"\n",
    "\n",
    "def term_frequency(term_id, document_id):\n",
    "    \"\"\"\n",
    "    Retrieves the term frequency a given word in a given document\n",
    "    \n",
    "    NB: depends on global variable 'inverted_index'\n",
    "    \n",
    "    :param term_id: The id of the word in question\n",
    "    :param document_id: The internal document_id of the document in question\n",
    "    \"\"\"\n",
    "    \n",
    "    return inverted_index[term_id].get(document_id, 0)\n",
    "\n",
    "def compute_BM25(doc_length, tf_raw, term_id, k, b):\n",
    "    \"\"\"\n",
    "    Helper function to isolate computations shared by both query and document versions of BM25 method\n",
    "    \"\"\"\n",
    "    \n",
    "    inv_norm = doc_length / avg_doc_length\n",
    "    document_freq = len(inverted_index[term_id])\n",
    "    \n",
    "    tf_normalized = tf_raw * (avg_doc_length / doc_length)\n",
    "\n",
    "    # Note: slides are not particularly clear about whether to still use the normalized TF or not.\n",
    "    # In this case we will continue with this normalized variable.\n",
    "    tf = ((1.0 + k) * tf_normalized) / (k * ((b - 1.0) + b * (inv_norm)) + tf_normalized)\n",
    "    \n",
    "    idf = np.log(num_documents / document_freq)\n",
    "    \n",
    "    return tf * idf\n",
    "\n",
    "def build_document_vector(document_id, vocabulary_size):\n",
    "    '''\n",
    "    Creates a vector of length V with the term frequency for each word in the vocabulary at its\n",
    "    corresponding index\n",
    "    \n",
    "    :param document_id: The internal document id\n",
    "    :param vocabulary_size: The number of words in the vocabulary\n",
    "    '''\n",
    "    \n",
    "    vector = np.zeros(vocabulary_size)\n",
    "    \n",
    "    for token_id in inverted_index:\n",
    "        documents_for_term = inverted_index[token_id]\n",
    "        \n",
    "        if document_id in documents_for_term:\n",
    "            vector[token_id] = documents_for_term[document_id]\n",
    "        \n",
    "    return vector\n",
    "        \n",
    "def build_query_vector(query_terms, vocabulary_size):\n",
    "    '''\n",
    "    Creates a vector of length V with the term frequency for each word in the vocabulary at its\n",
    "    corresponding index\n",
    "    \n",
    "    :param query_terms: A list of query term ids\n",
    "    :param vocabulary_size: The number of words in the vocabulary\n",
    "    '''\n",
    "    \n",
    "    vector = np.zeros(vocabulary_size)\n",
    "    counts = collections.Counter(query_terms)\n",
    "    \n",
    "    for token_id in counts:\n",
    "        vector[token_id] = counts[token_id]\n",
    "        \n",
    "    return vector\n",
    "\n",
    "# Vector representation of the collection frequencies dictionary\n",
    "collection_freq_matrix = np.zeros(len(id2token) + 1)\n",
    "for key in sorted(collection_frequencies.keys()):\n",
    "    if key > 0:\n",
    "        collection_freq_matrix[key] = collection_frequencies[key]\n",
    "\n",
    "print(collection_freq_matrix[:100])\n",
    "print(collection_freq_matrix.shape)\n",
    "# print(collection_freq_matrix.getnnz() - len(collection_freq_matrix.nonzero()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 167,
=======
   "execution_count": 9,
>>>>>>> 4ef1dac6be9b76e1f5fe02750ef18e57f69b50ec
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-167-e829e4053ac2>, line 307)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-167-e829e4053ac2>\"\u001b[0;36m, line \u001b[0;32m307\u001b[0m\n\u001b[0;31m    log_pos_values = np.log(smooth_pos_values)a T\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "def score_similarity(query_terms, document_id, weight_fn = None, query_weight_fn = None):\n",
    "    '''\n",
    "    Scoring function for a document and a query\n",
    "    \n",
    "    :param query_terms: A list of query term ids\n",
    "    :param document_id: The internal document id\n",
    "    :param weight: A function to re-weight the vectors\n",
    "    \n",
    "    returns a score (float)\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(id2token)\n",
    "    \n",
    "    # Create query vector\n",
    "    # Create document vector\n",
    "    doc_vec = build_document_vector(document_id, vocab_size)\n",
    "    query_vec = build_query_vector(query_terms, vocab_size)\n",
    "    \n",
    "    # Combine query and document into a matrix with sparse representation (only save indices of non-zero entries)\n",
    "    paired_matrix = np.array([query_vec, doc_vec])\n",
    "    paired_matrix = sparse.csr_matrix(paired_matrix)\n",
    "    \n",
    "    if weight_fn and query_weight_fn:\n",
    "        # nonzero() returns a tuple of two lists\n",
    "        # 0 -> i dimension of non zero entries in array where i == 0 is the query vector\n",
    "        # 1 -> j dimension of non zero entries in array array where j corresponds to the term_id\n",
    "        document_type, term_ids = paired_matrix.nonzero()\n",
    "        \n",
    "        for document_type, term_id in zip(document_type, term_ids):\n",
    "            if document_type == 0:\n",
    "                # query stuff\n",
    "                paired_matrix[document_type, term_id] = query_weight_fn(query_terms, term_id)\n",
    "            else:\n",
    "                # document stuff\n",
    "                paired_matrix[document_type, term_id] = weight_fn(document_id, term_id)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(paired_matrix)\n",
    "    \n",
    "    # Only first row is relevant (query document similarities)\n",
    "    similarities = similarity_matrix[0, 1:]\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def tfidf_query(query_terms, query_term_id):\n",
    "    counts = collections.Counter(query_terms)\n",
    "    \n",
    "    tf = np.log(1 + counts[query_term_id])\n",
    "    \n",
    "    df = len(inverted_index[query_term_id])\n",
    "    idf = np.log(num_documents / df)\n",
    "    \n",
    "    score = tf * idf\n",
    "    \n",
    "    return score\n",
    "\n",
    "def tfidf(int_document_id, query_term_id):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_term_id: the query term id (assuming you have split the query to tokens)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check: RAW COUNT OR NORMALIZED TF???\n",
    "    tf = np.log(1 + term_frequency(query_term_id, int_document_id))\n",
    "    \n",
    "    # inverted_index[query_term_id] consists of all the documents this term appears in\n",
    "    df = len(inverted_index[query_term_id])\n",
    "    idf = np.log(num_documents / df)\n",
    "    \n",
    "    score = tf * idf \n",
    "    \n",
    "    return score\n",
    "\n",
    "# Here we will normalize the raw TF and a different kind of damping: (1 + k)/k\n",
    "def BM25(int_document_id, query_term_id, k=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and all terms of a single query\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_term_id: Token id of the term to be re-weighted\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    doc_length = len(index.document(int_document_id)[1])\n",
    "    tf_raw = term_frequency(query_term_id, int_document_id)\n",
    "    \n",
    "    return compute_BM25(doc_length, tf_raw, query_term_id, k, b)\n",
    "\n",
    "def BM25_query(query_terms, query_term_id, k=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and all terms of a single query\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_term_id: Token id of the term to be re-weighted\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    doc_length = len(query_terms) \n",
    "    counts = collections.Counter(query_terms)\n",
    "    tf_raw = counts[query_term_id]\n",
    "    \n",
    "    return compute_BM25(doc_length, tf_raw, query_term_id, k, b)\n",
    "\n",
    "def absolute_discounting(int_document_id, query_term_id, delta=0.5):\n",
    "    \n",
    "    # TODO: figure our if tf should be raw or not. Max to ensure tf in range [0,1]\n",
    "    tf_raw = max(term_frequency(query_term_id, int_document_id) - delta, 0)\n",
    "    tot_nr_terms_in_doc = len(index.document(int_document_id)[1])\n",
    "    unique_terms_in_doc = unique_terms_per_document[int_document_id]\n",
    "    \n",
    "    corpus_size = total_terms\n",
    "    word_frequency = collection_frequencies[query_term_id]\n",
    "    p_w_c = word_frequency / corpus_size\n",
    "    \n",
    "    score = tf_raw / tot_nr_terms_in_doc + ((delta * tot_nr_terms_in_doc) / tot_nr_terms_in_doc) * p_w_c\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def dirichlet_prior_smoothing(int_document_id, query_term_id, mu=1000):\n",
    "    ''' input: one document id, one query term id, mu\n",
    "        output: for one query word , the dirichlet smoothed prior\n",
    "    '''\n",
    "\n",
    "    # Calculate the components of the model: tf, |d|, p(w|c)\n",
    "    tf = term_frequency(query_term_id, int_document_id)\n",
    "    document_length = document_lengths[int_document_id]\n",
    "    # TODO: functie van tf en p_wc maken (alhoewel de dict lookup voor TF best clean is)\n",
    "    # TODO: Checken of p_wc klopt bij docenten\n",
    "    p_wc = collection_frequencies[query_term_id]/total_terms\n",
    "#     print('tf',tf)\n",
    "#     print('doc length',document_lengths[int_document_id])\n",
    "#     print('p(w|C),', collection_frequencies[query_term_id],'/',sum(collection_frequencies.values()))\n",
    "    dir_smoothed_prior = ((document_length/(document_length+mu))*(tf/document_length))+ ((mu/(document_length + mu))*p_wc)\n",
    "    return dir_smoothed_prior\n",
    "\n",
    "def jelinek_mercer(int_document_id, query_term_id, lambd=0.5):\n",
    "    \"\"\"\n",
    "    Calcuate probabilty of a term by linearly interpolating with background language model.\n",
    "    \n",
    "    NB: Background probablity is calculated by weighting all words equally.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param lambd: the lambda paramter for the function\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Checken of het klopt (was vet moe toen ik dit implementeerde)\n",
    "    \n",
    "    doc_length = len(index.document(int_document_id)[1])\n",
    "    \n",
    "    term_freq_doc = term_frequency(query_term_id, int_doc_id)\n",
    "    term_freq_corpus = sum(inverted_index[query_term_id].values())\n",
    "    \n",
    "    signal = (term_freq_doc / doc_length)\n",
    "    background = (term_freq_corpus / total_terms)\n",
    "    \n",
    "    return lambd * signal  + (1 - lambd) * background\n",
    "\n",
    "\n",
    "\n",
    "# TODO implement tools to help you with the analysis of the results.\n",
    "def PLM_score_old(query_id_tokens, int_document_id, kernel_func, mu, sigma = 50):\n",
    "    '''input: one document id, one query id (so the id of one whole query)(ranging from 1-150) ,\n",
    "        a kernel function, mu, sigma (=50)\n",
    "    output: a language model for the document with a best position scoring strategy\n",
    "\n",
    "    assumption: following the paper 'Positional Language Models for Information Retrieval - Yuanhua Lv, ChengXiang Zhai'\n",
    "    \n",
    "    Given a query, suppose all terms in a document have the\n",
    "    same propagation function with the same , and the curve\n",
    "    of the kernel density function is symmetric. Then we have\n",
    "    k(i, j) = k(j, i). '''\n",
    "    \n",
    "    # document variables\n",
    "    document = index.document(int_document_id)[1]\n",
    "    doc_length = len(document)    \n",
    "    \n",
    "    # Query id tokens without stopwords\n",
    "    print('PLM', query_id_tokens)\n",
    "    query_id_no_stopwrds = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "    query_length = len(query_id_no_stopwrds)\n",
    "        \n",
    "    # Create a matrix to store all c(w,i)' values that matter\n",
    "    c_wi_prime_matrix = np.zeros((query_length,doc_length))\n",
    "    \n",
    "    # Todo: matrix maken van kernel\n",
    "    one_sided_kernel = [kernel_func(0, j) for j in range(doc_length)]\n",
    "    kernel = np.array(list(reversed(one_sided_kernel))[:-1] + one_sided_kernel)\n",
    "    \n",
    "    # Loop over all positions in the document\n",
    "    for i, word_id_i in enumerate(document):\n",
    "        # If a word occurs in the query we should propagate its frequency to other positions\n",
    "        # TODO: check how to handle multiple occurences of word in query\n",
    "        for q, wrd in enumerate(query_id_no_stopwrds):\n",
    "            if word_id_i == wrd:\n",
    "                # Loop over all positions in the document as to have a distance from position i to j\n",
    "                # for positions where i is a query word\n",
    "                for j in max_usefull_range(one_sided_kernel, i, doc_length):\n",
    "                    c_wi_prime_matrix[q][j] += kernel[i - j]\n",
    "    \n",
    "    # Go through all positions in the document again now all the \n",
    "    # frequencies have been propagated\n",
    "    Zi_vec = np.sum(c_wi_prime_matrix,axis=0)\n",
    "\n",
    "    # Dirichlet smoothed probability of a query word on a position in the document\n",
    "    p_wc_vec = collection_freq_matrix[np.array(query_id_tokens)] / total_terms\n",
    "    p_wc_vec = np.expand_dims(p_wc_vec, axis=1)\n",
    "    print(p_wc_vec.shape)\n",
    "    print(c_wi_prime_matrix.shape)\n",
    "    \n",
    "    smooth_pos_values = (c_wi_prime_matrix + mu * p_wc_vec) / (Zi_vec + mu)\n",
    "    \n",
    "    # Calculate p(w|Q)\n",
    "    q_language = ml_query_word_language_model(query_id_tokens)\n",
    "    \n",
    "    # log( P(w|D,i) ) for all w, i pairs\n",
    "    log_pos_values = np.log(smooth_pos_values)\n",
    "    \n",
    "    # Simplified KL-divergence formula (proportional to original for the same query)\n",
    "    kl_divergence_scores = np.sum(q_language * log_pos_values.T, axis=0)\n",
    "            \n",
    "    maxscore = np.amax(kl_divergence_scores)\n",
    "    \n",
    "    return maxscore\n",
    "\n",
    "def max_usefull_range(kernel_vector, i, N):\n",
    "    \n",
    "    try:\n",
    "        max_distance = kernel_vector.index(0)\n",
    "    except:\n",
    "        max_distance = len(kernel_vector)\n",
    "        \n",
    "    minimum = max(0, i - max_distance)\n",
    "    maximum = min(N, i + max_distance)\n",
    "    \n",
    "    return range(minimum, maximum)\n",
    "\n",
    "def PLM_score(int_document_id, query_term_ids, kernel_func, mu, sigma = 50):\n",
    "    '''input: one document id, one query id (so the id of one whole query)(ranging from 1-150) ,\n",
    "        a kernel function, mu, sigma (=50)\n",
    "    output: a language model for the document with a best position scoring strategy\n",
    "\n",
    "    assumption: following the paper 'Positional Language Models for Information Retrieval - Yuanhua Lv, ChengXiang Zhai'\n",
    "    \n",
    "    Given a query, suppose all terms in a document have the\n",
    "    same propagation function with the same , and the curve\n",
    "    of the kernel density function is symmetric. Then we have\n",
    "    k(i, j) = k(j, i). '''\n",
    "    \n",
    "    if int_document_id % 10000 == 0:\n",
    "        print(int_document_id)\n",
    "    \n",
    "    # document variables\n",
    "    document = index.document(int_document_id)[1]\n",
    "    doc_length = len(document)    \n",
    "    \n",
    "    # Query id tokens without stopwords\n",
    "    query_id_no_stopwords = [word_id for word_id in query_term_ids if word_id > 0]\n",
    "    query_length = len(query_id_no_stopwords)\n",
    "        \n",
    "    # Create a matrix to store all c(w,i)' values that matter\n",
    "#     c_wi_prime_matrix = np.zeros((query_length,doc_length))\n",
    "    \n",
    "    one_sided_kernel = [kernel_func(0, j) for j in range(doc_length)]\n",
    "    # Add 0 pad to create even length kernel\n",
    "    kernel = np.array([0] + list(reversed(one_sided_kernel))[:-1] + one_sided_kernel)\n",
    "    \n",
    "    i_vec = np.array(document)\n",
    "    i_mat = np.tile(i_vec, query_length).reshape(query_length, doc_length)\n",
    "    q_vec = np.tile(np.array(query_id_no_stopwords), doc_length).reshape(doc_length, query_length).T\n",
    "    \n",
    "    c_wi = i_mat == q_vec\n",
    "    c_prime = np.zeros((query_length,doc_length))\n",
    "    \n",
    "    for i, row in enumerate(c_wi):\n",
    "        # First element is edge noise\n",
    "        c_prime[i] = np.convolve(kernel, row)[len(row):(1-len(row))]\n",
    "    \n",
    "    # Go through all positions in the document again now all the \n",
    "    # frequencies have been propagated\n",
    "    Z_vec = np.sum(c_prime, axis=0)\n",
    "\n",
    "    # Dirichlet smoothed probability of a word on a position in the document\n",
    "    try:\n",
    "        p_wc_vec = collection_freq_matrix[np.array(document)] / np.sum(collection_freq_matrix)\n",
    "    except:\n",
    "        print(np.array(document), np.array(document).dtype)\n",
    "        sys.exit()\n",
    "        \n",
    "    smooth_pos_values = (c_prime + mu * p_wc_vec) / (Z_vec + mu)\n",
    "    \n",
    "    # Calculate p(w|Q)\n",
    "    q_language = ml_query_word_language_model(query_term_ids)\n",
    "    \n",
    "    # log( P(w|D,i) ) for all w, i pairs\n",
    "    log_pos_values = np.log(smooth_pos_values)a T\n",
    "    \n",
    "    # Simplified KL-divergence formula (proportional to original for the same query)\n",
    "    kl_divergence_scores = np.sum(q_language * log_pos_values.T, axis=0)\n",
    "            \n",
    "    maxscore = np.amax(kl_divergence_scores)\n",
    "    \n",
    "    return int_document_id, maxscore\n",
    "\n",
    "def quick_mats(i, N, sigma):\n",
    "        normalize = np.sqrt(2*np.pi*sigma**2)\n",
    "        big_area = norm.cdf( (N - i) / sigma )\n",
    "        small_area = norm.cdf( (1 - i) / sigma )\n",
    "        \n",
    "        return normalize * (big_area - small_area)\n",
    "\n",
    "def gaussian_kernel(i, j, sigma=50):\n",
    "    return np.exp(-(i-j)**2 / (2 * sigma**2))\n",
    "\n",
    "def triangle_kernel(i, j, sigma=50):\n",
    "    return 1-abs(i - j)/sigma if abs(i - j) <= sigma else 0\n",
    "\n",
    "def cosine_kernel(i, j, sigma=50):\n",
    "    return 0.5 * (1 + np.cos((abs(i - j) * np.pi) / sigma)) if abs(i - j) <= sigma else 0\n",
    "\n",
    "def circle_kernel(i, j, sigma=50):\n",
    "    return np.sqrt(1-(abs(i - j)/sigma)**2) if abs(i - j) <= sigma else 0\n",
    "\n",
    "def passage_kernel(i, j, sigma=50):\n",
    "    return int(abs(i - j) < sigma/2)\n",
    "\n",
    "def ml_query_word_language_model(query_term_ids):\n",
    "    counts = collections.Counter(query_term_ids)\n",
    "    return np.array([counts[term_id] / len(query_term_ids) for term_id in query_term_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leveraged', 'buyouts']\n",
      "['nwa', 'board', 'rejects', 'davis', 'offer', 'amends', 'court', 'complaint', 'calling', 'parts', 'oilman', 'marvin', 'davis', '2', '7', 'billion', 'bid', 'northwest', 'airlines', 'illegal', 'unfinanced', 'nwa', 's', 'board', 'directors', 'unanimously', 'rejected', 'offer', 'moved', 'take', 'court', 'action', 'block', 'efforts', 'board', 'monday', 'reaffirmed', 'willingness', 'negotiate', 'suitors', 'davis', 'interest', 'producing', 'transaction', 'best', 'interests', 'nwa', 'shareholders', 'seek', 'quick', 'hearing', 'motion', 'enjoin', 'davis', '90', 'share', 'tender', 'offer', 'board', 'working', 'undisclosed', 'number', 'parties', 'indicated', 'interest', 'acquiring', 'northwest', 'nations', 'fourth', 'largest', 'carrier', 'nwa', 'stock', 'closed', '102', '75', 'monday', '25', 'cents', 'share', 'new', 'york', 'stock', 'exchange', 'composite', 'trading', 'amended', 'complaint', 'district', 'court', 'minneapolis', 'nwa', 'board', 'davis', 'financing', 'plan', 'exceed', 'federal', 'margin', 'rules', 'aimed', 'limiting', 'amount', 'credit', 'buy', 'stock', 'board', '1', '25', 'billion', 'preferred', 'stock', 'financing', 'planned', 'davis', 'buyout', 'actually', 'considered', 'debt', 'motion', 'claims', 'davis', 'tender', 'offer', 'actually', 'illegal', 'proxy', 'solicitation', 'provided', 'insufficient', 'information', 'shareholders', 'regarding', 'source', 'availability', '450', 'million', 'equity', 'davis', 'buyout', 'addition', 'minnesota', 'commerce', 'commissioner', 'michael', 'hatch', 'monday', 'found', 'davis', 'acquisition', 's', 'registration', 'statement', 'offering', 'purchase', 'nwa', 's', 'outstanding', 'shares', 'deficient', 'hatch', 'davis', 'failed', 'disclose', 'manner', 'operate', 'nwa', 'expand', 'operation', 'dispose', 'strategic', 'assets', 'company', 'lay', 'employees', 'additionally', 'commissioner', 'davis', 'did', 'disclose', 'plans', 'relating', 'airbus', 'maintenance', 'facility', 'intended', 'locate', 'minnesota', 'hatch', 'scheduled', 'hearing', 'matter', 'evening', 'registration', 'suspended', 'friday', 'deficiencies', 'corrected', 'jim', 'fingeroth', 'davis', 'spokesman', 'acquisition', 'company', 'comment', 'order', 'davis', 'attorneys', 'shoring', 'efforts', 'delaware', 'nwa', 'incorporated', 'lawyers', 'argued', 'wilmington', 'del', 'court', 'provision', 'nwas', 'poison', 'pill', 'takeover', 'defense', 'hindering', 'efforts', 'put', 'new', 'nwa', 'board', 'directors', 'place', 'defense', 'effectively', 'block', 'new', 'board', 'directors', 'selling', 'nwa', '180', 'days', 'prospect', 'long', 'delay', 'discourage', 'shareholders', 'supporting', 'davis', 'proxy', 'fight', 'planned', 'nwas', '15', 'annual', 'meeting', 'attorney', 'gilchrist', 'sparks', 'told', 'hearing', 'nwa', 'chairman', 'steven', 'rothmeier', 'prefers', 'airline', 'remain', 'independent', 'find', 'white', 'knight', 'investor', 'work', 'current', 'management', 'letters', '5', '000', 'union', 'pilots', 'northwest', 'rothmeier', 'alternatives', 'available', 'friendly', 'investors', 'assistance', 'contingent', 'contract', 'settlement', 'pilots', 'working', 'two', 'separate', 'contracts', 'northwest', 'acquired', 'republic', 'airlines', '1986', 'davis', 'delaware', 'court', 'action', 'altered', 'course', 'fight', 'northwest', 'poison', 'pill', 'initially', 'sought', 'invalidate', 'entire', 'poison', 'plan', 'cost', 'potential', 'acquisition', 'hostile', 'bidder', 'flooding', 'market', 'new', 'nwa', 'shares', 'sparks', 'monday', 'davis', 'retain', 'pill', 'event', 'gained', 'control', 'board', 'rothmeier', 'declined', 'identify', 'potential', 'friendly', 'investors', 'company', 'group', 'holding', '4', '9', 'percent', 'nwas', '29', '1', 'million', 'shares', 'outstanding', 'interested', 'leveraged', 'buyout', 'leveraged', 'buyout', 'company', 'bought', 'largely', 'borrowed', 'funds', 'group', 'includes', 'los', 'angeles', 'based', 'investor', 'alfred', 'checchi', 'former', 'nwa', 'board', 'member', 'gary', 'wilson', 'executive', 'walt', 'disney', 'co', 'new', 'york', 'times', 'reported', 'monday', 'kelso', 'co', 'small', 'investment', 'bank', 'based', 'new', 'york', 'considering', 'bid', 'airline', 'spokesman', 'kelso', 'northwest', 'declined', 'comment']\n",
      "-22.68826958525246\n",
      "-17.83796830047362\n",
      "-17.886358718159826\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "Multinominal: p(q|lmodel_d) = prod( p(w_i | lmodel_d) )\n",
    "lmodel_d: argmax p(d | lmodel_d) --> the model for which d has the highest probability of occuring\n",
    "                 p(d | lmodel_d) = prod_over_w_in_doc( p(w | lmodel_d) ) (sum log)\n",
    "                 log_p(d | lmodel_d) = sum_over_w_in_vocab( tf(w;d) * log_p(w | lmodel_d ) )\n",
    "                 p_ml(w | lmodel_d) = tf(w;d) / len(d)\n",
    "'''\n",
    "\n",
    "def unigram_prob(term_id, document_id):\n",
    "    \"\"\"\n",
    "    Maximum Likelihood estimate of p(word|document_model)\n",
    "    \"\"\"\n",
    "    tf = term_frequency(term_id, document_id)\n",
    "    doc_length = len(index.document(document_id)[1])\n",
    "    \n",
    "    return tf / doc_length\n",
    "\n",
    "### Probabilistic score function\n",
    "def log_multinomial_query_language_model(query_term_ids, document_id, word_probability_func=unigram_prob):\n",
    "    \"\"\"\n",
    "    Query-likelihood model assuming uniform document prior\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = collections.Counter(query_term_ids)\n",
    "    \n",
    "    # sum_over_words_in_query( tf(w;d) * log( p(w|d) ) )\n",
    "    return np.sum([counts[term_id]*np.log(word_probability_func(document_id, term_id)) \n",
    "                   for term_id in query_term_ids])\n",
    "\n",
    "def kl_divergenge(query_term_ids, document_id, word_probability_func=unigram_prob):\n",
    "    \n",
    "    p_word_given_query = ml_query_word_language_model(query_term_ids)\n",
    "    p_word_given_document = np.array([word_probability_func(term_id, document_id) for term_id in query_term_ids])\n",
    "    \n",
    "    return np.sum(p_word_given_query * np.log(p_word_given_document))\n",
    "\n",
    "# Document and query share terms\n",
    "test_query = [4886, 48553] \n",
    "test_doc = 251\n",
    "\n",
    "print([id2token[idx] for idx in test_query if idx > 0])\n",
    "print([id2token[idx] for idx in index.document(test_doc)[1] if idx > 0])\n",
    "\n",
    "for smoothing_func in [jelinek_mercer, absolute_discounting, dirichlet_prior_smoothing]:\n",
    "    print(log_multinomial_query_language_model(test_query, test_doc, smoothing_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  120  test queries and  30  validation queries\n"
     ]
    }
   ],
   "source": [
    "# Figure out which queries are relevant\n",
    "\n",
    "validation_queries_ids = []\n",
    "with open('./ap_88_89/qrel_validation', 'r') as validation_queries:\n",
    "    for line in validation_queries:\n",
    "        query_id = line.split(' ')[0]\n",
    "        if query_id not in validation_queries_ids:\n",
    "            validation_queries_ids.append(query_id)\n",
    "            \n",
    "            \n",
    "\n",
    "test_queries_ids = []\n",
    "with open('./ap_88_89/qrel_test', 'r') as test_queries:\n",
    "    for line in test_queries:\n",
    "        query_id = line.split(' ')[0]\n",
    "        if query_id not in test_queries_ids:\n",
    "            test_queries_ids.append(query_id)\n",
    "\n",
    "            \n",
    "print('There are ',len(test_queries_ids),' test queries and ',len(validation_queries_ids),' validation queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 1000 relevant documents for query 51: ['AP880731-0085', 'AP880325-0293', 'AP880706-0311', 'AP880316-0292', 'AP880721-0290', 'AP890630-0056', 'AP880406-0267', 'AP880318-0287', 'AP881118-0209', 'AP890908-0122', 'AP880627-0045', 'AP891126-0082', 'AP881108-0253', 'AP890503-0279', 'AP891010-0258', 'AP890908-0222', 'AP880627-0039', 'AP880412-0268', 'AP880627-0093', 'AP880712-0156', 'AP890629-0275', 'AP891003-0130', 'AP880311-0301', 'AP881128-0075', 'AP880517-0253', 'AP880626-0038', 'AP881028-0294', 'AP890209-0234', 'AP890106-0255', 'AP890327-0161', 'AP890817-0225', 'AP880628-0097', 'AP881129-0057', 'AP890608-0253', 'AP891117-0213', 'AP890913-0264', 'AP880704-0023', 'AP880512-0314', 'AP880831-0223', 'AP890620-0248', 'AP890724-0045', 'AP880628-0170', 'AP880407-0258', 'AP880928-0210', 'AP881219-0271', 'AP890306-0244', 'AP890410-0255', 'AP881203-0143', 'AP880629-0215', 'AP890404-0263', 'AP880520-0261', 'AP880729-0166', 'AP891211-0325', 'AP890817-0223', 'AP890216-0167', 'AP880520-0122', 'AP880626-0003', 'AP891011-0255', 'AP880905-0132', 'AP890328-0202', 'AP891126-0084', 'AP881119-0051', 'AP890216-0288', 'AP880626-0019', 'AP890705-0196', 'AP890104-0259', 'AP881119-0077', 'AP880516-0318', 'AP890418-0241', 'AP880826-0050', 'AP880405-0268', 'AP891127-0256', 'AP891120-0065', 'AP890823-0099', 'AP881231-0042', 'AP891221-0274', 'AP890731-0272', 'AP891214-0204', 'AP880628-0310', 'AP881025-0133', 'AP880915-0047', 'AP880804-0167', 'AP890601-0339', 'AP880215-0180', 'AP890306-0264', 'AP880627-0175', 'AP880714-0190', 'AP880627-0063', 'AP880707-0089', 'AP880627-0115', 'AP880405-0176', 'AP890708-0097', 'AP881207-0184', 'AP890329-0221', 'AP890622-0084', 'AP890206-0170', 'AP881230-0138', 'AP890618-0075', 'AP881104-0232', 'AP890207-0133', 'AP890724-0259', 'AP880901-0073', 'AP881021-0179', 'AP880717-0007', 'AP880814-0005', 'AP880907-0010', 'AP881021-0256', 'AP880516-0328', 'AP880708-0148', 'AP880626-0076', 'AP880301-0271', 'AP880314-0167', 'AP881130-0262', 'AP890920-0246', 'AP890209-0229', 'AP890906-0284', 'AP880716-0076', 'AP891215-0260', 'AP880922-0206', 'AP880711-0107', 'AP880813-0003', 'AP890928-0294', 'AP880707-0127', 'AP880726-0137', 'AP880217-0261', 'AP880703-0077', 'AP880703-0076', 'AP880708-0112', 'AP880706-0194', 'AP890517-0089', 'AP880503-0244', 'AP890127-0117', 'AP880711-0054', 'AP880705-0183', 'AP890427-0306', 'AP880906-0086', 'AP880706-0033', 'AP880312-0036', 'AP880621-0292', 'AP880805-0037', 'AP891221-0216', 'AP880706-0025', 'AP880705-0082', 'AP891219-0249', 'AP890821-0246', 'AP880624-0282', 'AP881221-0144', 'AP891205-0223', 'AP880711-0139', 'AP880831-0183', 'AP890702-0030', 'AP890712-0120', 'AP880819-0168', 'AP890115-0012', 'AP891117-0214', 'AP890729-0071', 'AP890426-0260', 'AP881205-0196', 'AP880706-0117', 'AP890405-0282', 'AP890901-0154', 'AP890410-0121', 'AP890911-0278', 'AP890515-0080', 'AP880909-0107', 'AP880513-0284', 'AP890723-0061', 'AP880705-0189', 'AP880818-0218', 'AP881019-0037', 'AP881222-0106', 'AP880728-0142', 'AP880712-0153', 'AP890428-0090', 'AP880513-0252', 'AP880708-0189', 'AP880622-0052', 'AP880624-0296', 'AP880428-0001', 'AP890207-0230', 'AP890914-0250', 'AP890419-0228', 'AP890418-0313', 'AP890127-0155', 'AP880621-0250', 'AP891009-0123', 'AP890518-0275', 'AP890130-0234', 'AP880831-0109', 'AP880413-0195', 'AP891208-0192', 'AP880706-0213', 'AP890721-0245', 'AP880420-0287', 'AP891017-0271', 'AP890816-0187', 'AP881017-0296', 'AP890310-0265', 'AP881111-0202', 'AP890125-0094', 'AP880708-0179', 'AP880705-0043', 'AP880720-0294', 'AP881203-0116', 'AP890612-0290', 'AP880706-0188', 'AP880718-0153', 'AP890105-0233', 'AP880312-0052', 'AP881124-0128', 'AP891002-0199', 'AP880706-0244', 'AP881004-0246', 'AP880527-0315', 'AP880623-0173', 'AP880617-0267', 'AP890506-0013', 'AP890929-0096', 'AP890523-0235', 'AP890110-0304', 'AP890703-0134', 'AP880422-0219', 'AP880229-0271', 'AP880312-0117', 'AP880624-0025', 'AP890407-0285', 'AP890703-0072', 'AP881130-0272', 'AP890502-0093', 'AP890608-0032', 'AP891121-0110', 'AP880624-0180', 'AP891002-0334', 'AP890816-0062', 'AP891002-0261', 'AP880707-0037', 'AP890614-0287', 'AP891018-0037', 'AP880706-0208', 'AP891220-0125', 'AP880926-0235', 'AP890128-0159', 'AP880728-0023', 'AP880708-0044', 'AP880621-0221', 'AP881204-0092', 'AP890426-0268', 'AP891011-0163', 'AP880803-0157', 'AP890227-0132', 'AP890607-0108', 'AP880705-0069', 'AP881019-0150', 'AP890411-0010', 'AP880708-0119', 'AP881209-0219', 'AP890323-0197', 'AP880705-0169', 'AP880815-0175', 'AP880705-0167', 'AP890723-0009', 'AP880708-0020', 'AP880623-0180', 'AP880714-0180', 'AP880227-0156', 'AP891007-0018', 'AP890606-0198', 'AP880409-0016', 'AP880618-0073', 'AP890607-0014', 'AP881121-0268', 'AP881202-0163', 'AP881117-0116', 'AP880704-0009', 'AP880706-0225', 'AP880616-0202', 'AP890929-0165', 'AP880516-0151', 'AP890428-0054', 'AP891116-0258', 'AP890918-0116', 'AP891207-0121', 'AP880908-0136', 'AP890707-0114', 'AP880818-0272', 'AP890406-0027', 'AP890406-0263', 'AP890224-0066', 'AP890520-0011', 'AP890813-0052', 'AP880623-0015', 'AP890607-0086', 'AP890208-0146', 'AP880820-0096', 'AP890504-0256', 'AP880728-0192', 'AP890428-0002', 'AP880617-0033', 'AP890812-0133', 'AP880708-0092', 'AP890108-0032', 'AP880223-0268', 'AP890727-0042', 'AP891024-0145', 'AP880229-0013', 'AP880622-0040', 'AP880706-0105', 'AP890412-0226', 'AP891003-0103', 'AP891018-0174', 'AP880506-0221', 'AP880504-0042', 'AP890908-0034', 'AP890612-0011', 'AP881229-0171', 'AP890503-0044', 'AP890310-0316', 'AP880713-0028', 'AP890907-0159', 'AP890623-0269', 'AP890305-0051', 'AP881209-0233', 'AP880719-0122', 'AP890314-0278', 'AP890315-0137', 'AP890223-0276', 'AP880720-0216', 'AP880713-0126', 'AP880709-0108', 'AP890223-0281', 'AP890223-0164', 'AP891110-0023', 'AP890628-0005', 'AP890210-0202', 'AP890418-0312', 'AP880409-0180', 'AP890522-0084', 'AP880627-0012', 'AP880513-0032', 'AP881216-0227', 'AP881214-0227', 'AP880621-0255', 'AP890601-0260', 'AP881205-0264', 'AP890103-0117', 'AP881206-0019', 'AP891002-0120', 'AP880618-0111', 'AP880509-0225', 'AP890606-0044', 'AP891209-0045', 'AP880518-0348', 'AP881002-0062', 'AP891122-0148', 'AP880804-0086', 'AP890701-0050', 'AP890119-0027', 'AP881126-0001', 'AP880720-0292', 'AP880619-0070', 'AP890707-0209', 'AP890412-0286', 'AP890509-0269', 'AP890322-0207', 'AP881207-0223', 'AP890330-0021', 'AP890506-0006', 'AP880425-0167', 'AP890306-0253', 'AP880411-0118', 'AP880302-0275', 'AP880426-0025', 'AP880705-0076', 'AP890801-0228', 'AP881223-0194', 'AP880808-0072', 'AP880708-0096', 'AP891005-0149', 'AP890811-0028', 'AP891129-0025', 'AP890404-0239', 'AP890717-0014', 'AP880415-0170', 'AP890428-0191', 'AP890619-0204', 'AP880704-0136', 'AP881115-0114', 'AP880820-0098', 'AP880417-0079', 'AP880705-0038', 'AP880812-0142', 'AP881207-0273', 'AP891128-0218', 'AP880526-0209', 'AP880705-0061', 'AP891130-0010', 'AP880716-0019', 'AP880619-0092', 'AP880617-0306', 'AP890131-0178', 'AP880819-0148', 'AP881102-0255', 'AP880829-0033', 'AP880621-0331', 'AP890519-0076', 'AP881020-0103', 'AP891020-0203', 'AP890523-0040', 'AP880705-0056', 'AP880421-0013', 'AP880905-0043', 'AP880705-0130', 'AP891103-0204', 'AP880706-0087', 'AP880517-0252', 'AP880707-0129', 'AP880418-0064', 'AP890222-0111', 'AP880519-0275', 'AP891219-0108', 'AP890920-0049', 'AP880627-0009', 'AP891004-0231', 'AP890122-0100', 'AP880613-0131', 'AP880708-0100', 'AP880704-0089', 'AP890613-0247', 'AP880729-0058', 'AP890627-0148', 'AP890104-0279', 'AP891117-0018', 'AP891129-0234', 'AP890906-0311', 'AP880921-0220', 'AP890217-0158', 'AP881205-0021', 'AP880415-0302', 'AP891030-0178', 'AP880621-0240', 'AP880818-0158', 'AP880623-0157', 'AP880415-0223', 'AP880817-0180', 'AP880808-0162', 'AP890504-0182', 'AP880520-0288', 'AP880617-0289', 'AP890121-0095', 'AP881020-0170', 'AP890301-0211', 'AP890713-0026', 'AP891018-0047', 'AP891222-0290', 'AP880901-0017', 'AP890302-0175', 'AP891003-0060', 'AP891101-0286', 'AP880519-0303', 'AP890914-0041', 'AP880523-0018', 'AP880711-0173', 'AP880706-0109', 'AP880818-0267', 'AP890413-0014', 'AP880929-0250', 'AP890313-0233', 'AP880809-0246', 'AP890418-0079', 'AP891024-0197', 'AP890117-0018', 'AP890301-0076', 'AP880704-0144', 'AP890719-0300', 'AP880704-0123', 'AP880212-0093', 'AP880729-0198', 'AP890131-0301', 'AP880706-0147', 'AP880329-0252', 'AP880710-0042', 'AP881019-0233', 'AP880621-0247', 'AP890608-0149', 'AP890605-0085', 'AP891218-0055', 'AP890624-0007', 'AP890224-0128', 'AP890609-0270', 'AP890413-0262', 'AP881014-0292', 'AP880617-0270', 'AP880704-0025', 'AP891128-0199', 'AP890724-0067', 'AP880316-0024', 'AP890808-0010', 'AP890419-0227', 'AP890114-0004', 'AP880225-0174', 'AP880709-0078', 'AP880709-0073', 'AP891006-0104', 'AP890526-0222', 'AP890502-0207', 'AP890920-0009', 'AP880610-0282', 'AP890418-0290', 'AP880709-0088', 'AP880320-0104', 'AP890722-0154', 'AP880711-0011', 'AP880503-0016', 'AP890907-0273', 'AP880513-0227', 'AP890819-0020', 'AP890503-0254', 'AP880901-0271', 'AP880621-0226', 'AP880706-0325', 'AP880622-0056', 'AP880708-0109', 'AP891219-0166', 'AP880708-0207', 'AP890920-0131', 'AP880706-0046', 'AP890526-0152', 'AP891014-0013', 'AP880708-0162', 'AP891219-0213', 'AP890210-0048', 'AP880705-0010', 'AP880628-0087', 'AP890511-0035', 'AP890108-0068', 'AP890512-0039', 'AP890103-0010', 'AP880621-0246', 'AP891221-0118', 'AP890518-0116', 'AP880805-0001', 'AP881227-0145', 'AP881222-0089', 'AP890425-0029', 'AP890427-0221', 'AP891027-0233', 'AP890913-0036', 'AP881224-0007', 'AP890623-0029', 'AP890927-0152', 'AP890411-0191', 'AP880516-0312', 'AP880628-0092', 'AP881212-0220', 'AP881126-0138', 'AP880709-0178', 'AP880714-0187', 'AP881117-0172', 'AP880707-0042', 'AP880428-0301', 'AP890726-0019', 'AP891205-0192', 'AP881205-0278', 'AP880703-0078', 'AP890805-0010', 'AP890208-0268', 'AP880928-0011', 'AP890921-0158', 'AP880526-0021', 'AP890112-0243', 'AP890727-0192', 'AP880809-0180', 'AP891221-0129', 'AP880407-0149', 'AP890830-0072', 'AP880411-0165', 'AP880217-0207', 'AP880921-0093', 'AP880301-0282', 'AP881205-0276', 'AP890110-0016', 'AP880428-0017', 'AP891002-0339', 'AP880506-0116', 'AP890510-0266', 'AP891217-0040', 'AP890511-0222', 'AP890307-0140', 'AP880818-0297', 'AP890928-0135', 'AP890329-0113', 'AP881127-0078', 'AP890518-0101', 'AP880621-0237', 'AP881007-0067', 'AP891019-0216', 'AP891002-0137', 'AP880610-0283', 'AP880704-0147', 'AP890227-0288', 'AP880316-0029', 'AP880705-0046', 'AP891006-0222', 'AP880706-0260', 'AP880925-0048', 'AP880705-0202', 'AP880526-0329', 'AP891208-0020', 'AP880512-0019', 'AP880320-0022', 'AP891013-0069', 'AP891026-0232', 'AP881105-0040', 'AP880404-0121', 'AP880622-0287', 'AP881205-0250', 'AP881130-0270', 'AP890427-0275', 'AP880704-0119', 'AP890131-0123', 'AP890111-0078', 'AP881025-0040', 'AP890219-0106', 'AP891117-0100', 'AP891030-0194', 'AP890228-0177', 'AP880921-0119', 'AP880704-0046', 'AP891219-0192', 'AP880921-0232', 'AP880709-0167', 'AP890623-0158', 'AP880711-0074', 'AP890704-0174', 'AP890621-0238', 'AP890323-0256', 'AP881214-0269', 'AP880617-0305', 'AP881019-0005', 'AP880708-0248', 'AP890804-0075', 'AP890117-0087', 'AP891112-0040', 'AP881102-0263', 'AP890130-0021', 'AP881012-0082', 'AP891219-0116', 'AP890525-0093', 'AP880427-0201', 'AP890112-0007', 'AP890605-0084', 'AP891214-0023', 'AP880901-0273', 'AP890408-0097', 'AP880620-0013', 'AP891012-0009', 'AP890125-0185', 'AP880507-0212', 'AP890317-0131', 'AP880518-0281', 'AP881130-0280', 'AP880428-0187', 'AP880827-0035', 'AP890719-0295', 'AP880704-0050', 'AP890202-0195', 'AP880427-0193', 'AP890419-0189', 'AP880708-0232', 'AP880903-0066', 'AP890108-0094', 'AP890119-0193', 'AP880623-0248', 'AP880318-0331', 'AP880317-0180', 'AP881018-0019', 'AP880709-0062', 'AP880709-0043', 'AP890626-0112', 'AP890109-0144', 'AP880729-0309', 'AP880719-0183', 'AP880622-0188', 'AP891111-0084', 'AP880704-0101', 'AP881105-0027', 'AP881213-0151', 'AP890802-0237', 'AP890301-0257', 'AP891129-0271', 'AP890803-0159', 'AP890322-0212', 'AP881222-0124', 'AP891206-0009', 'AP890526-0012', 'AP881123-0294', 'AP880710-0012', 'AP890802-0097', 'AP880712-0034', 'AP880316-0154', 'AP881002-0009', 'AP890426-0083', 'AP880707-0067', 'AP890204-0175', 'AP880415-0035', 'AP880610-0279', 'AP890110-0165', 'AP890724-0005', 'AP880714-0004', 'AP891029-0046', 'AP890117-0252', 'AP880927-0191', 'AP890720-0240', 'AP890929-0181', 'AP881102-0163', 'AP880517-0247', 'AP890215-0002', 'AP880620-0010', 'AP890407-0263', 'AP880709-0109', 'AP880920-0022', 'AP880731-0051', 'AP890726-0269', 'AP880504-0201', 'AP890927-0025', 'AP891111-0099', 'AP890125-0184', 'AP890627-0100', 'AP880613-0144', 'AP880420-0342', 'AP880420-0226', 'AP880213-0157', 'AP890914-0069', 'AP890407-0321', 'AP881031-0103', 'AP880707-0038', 'AP880321-0168', 'AP880616-0243', 'AP890628-0040', 'AP880703-0055', 'AP880909-0111', 'AP880813-0115', 'AP881121-0020', 'AP890110-0048', 'AP880618-0076', 'AP890811-0144', 'AP890812-0081', 'AP891129-0178', 'AP881215-0029', 'AP880813-0216', 'AP890726-0130', 'AP891013-0165', 'AP880423-0034', 'AP890930-0029', 'AP890518-0201', 'AP880412-0086', 'AP890628-0214', 'AP880921-0204', 'AP890201-0003', 'AP881031-0113', 'AP880714-0072', 'AP890314-0027', 'AP880729-0186', 'AP881104-0023', 'AP890111-0138', 'AP880525-0347', 'AP880724-0045', 'AP880702-0018', 'AP881230-0079', 'AP881206-0248', 'AP891102-0199', 'AP881207-0278', 'AP880725-0216', 'AP880712-0202', 'AP890614-0014', 'AP890329-0255', 'AP890111-0217', 'AP890515-0141', 'AP880922-0193', 'AP890619-0259', 'AP890614-0249', 'AP881028-0170', 'AP890624-0023', 'AP880713-0125', 'AP890322-0331', 'AP880712-0223', 'AP881207-0246', 'AP881113-0015', 'AP890427-0258', 'AP881021-0163', 'AP881206-0223', 'AP890109-0198', 'AP890919-0084', 'AP880713-0032', 'AP890107-0160', 'AP881115-0014', 'AP891129-0157', 'AP890714-0212', 'AP881026-0018', 'AP890602-0272', 'AP890823-0072', 'AP880705-0104', 'AP880531-0261', 'AP891005-0185', 'AP890917-0002', 'AP891130-0114', 'AP881123-0218', 'AP890801-0149', 'AP891013-0083', 'AP890428-0224', 'AP890619-0026', 'AP880413-0185', 'AP890127-0163', 'AP890126-0243', 'AP880627-0091', 'AP880920-0270', 'AP880421-0319', 'AP880721-0269', 'AP890109-0346', 'AP880704-0141', 'AP881008-0119', 'AP890929-0032', 'AP880803-0124', 'AP890112-0126', 'AP881212-0248', 'AP880602-0130', 'AP880729-0020', 'AP890207-0277', 'AP890109-0007', 'AP890113-0156', 'AP890228-0134', 'AP881230-0109', 'AP881013-0262', 'AP880704-0065', 'AP880315-0195', 'AP890110-0178', 'AP880704-0146', 'AP880823-0174', 'AP880314-0252', 'AP890114-0003', 'AP891101-0033', 'AP890302-0326', 'AP891111-0010', 'AP891122-0035', 'AP890403-0048', 'AP890927-0158', 'AP891208-0263', 'AP881011-0009', 'AP890816-0147', 'AP890901-0071', 'AP880721-0313', 'AP881228-0066', 'AP891124-0188', 'AP880718-0071', 'AP891130-0232', 'AP890323-0016', 'AP881228-0092', 'AP880709-0054', 'AP890816-0246', 'AP890429-0056', 'AP880714-0130', 'AP891006-0014', 'AP880318-0167', 'AP890622-0003', 'AP880703-0071', 'AP880301-0257', 'AP890628-0021', 'AP891211-0178', 'AP880709-0025', 'AP890603-0150', 'AP891110-0196', 'AP891206-0199', 'AP880607-0109', 'AP890727-0166', 'AP880803-0232', 'AP891010-0009', 'AP891011-0275', 'AP880331-0313', 'AP880706-0214', 'AP890711-0135', 'AP880618-0006', 'AP890425-0230', 'AP881202-0114', 'AP890711-0050', 'AP880616-0188', 'AP881231-0022', 'AP880703-0073', 'AP880309-0405', 'AP880222-0164', 'AP890211-0169', 'AP880927-0014', 'AP891103-0231', 'AP890509-0155', 'AP890420-0016', 'AP890605-0001', 'AP890127-0165', 'AP880923-0126', 'AP880613-0145', 'AP890711-0112', 'AP881222-0243', 'AP880503-0225', 'AP890526-0071', 'AP880708-0068', 'AP890623-0279', 'AP880217-0191', 'AP880624-0268', 'AP880714-0170', 'AP880803-0160', 'AP890807-0163', 'AP880519-0110', 'AP891211-0203', 'AP890505-0021', 'AP881209-0135', 'AP880704-0134', 'AP881110-0148', 'AP880421-0311', 'AP881222-0132', 'AP890823-0168', 'AP881130-0112', 'AP881124-0129', 'AP880705-0131', 'AP890823-0173', 'AP880704-0152', 'AP891106-0065', 'AP880325-0115', 'AP890217-0022', 'AP880711-0163', 'AP890206-0237', 'AP880421-0045', 'AP890209-0158', 'AP890216-0120', 'AP890628-0169', 'AP890516-0077', 'AP890615-0177', 'AP891110-0213', 'AP890120-0254', 'AP891211-0314', 'AP880318-0019', 'AP880224-0215', 'AP891017-0091', 'AP880531-0241', 'AP890410-0049', 'AP891115-0136', 'AP880326-0039', 'AP881118-0186', 'AP880919-0133', 'AP881216-0186', 'AP890406-0151', 'AP880708-0023', 'AP890516-0165', 'AP890521-0093', 'AP880425-0331', 'AP880615-0028', 'AP891207-0233', 'AP880701-0176', 'AP880705-0143', 'AP890310-0209', 'AP891208-0201', 'AP880705-0019', 'AP880921-0115', 'AP880714-0172', 'AP881101-0022', 'AP890129-0060', 'AP890928-0202', 'AP880705-0106', 'AP880627-0304', 'AP890816-0029', 'AP890612-0008', 'AP890524-0049', 'AP890110-0197', 'AP890710-0252', 'AP890310-0235', 'AP880304-0010', 'AP890818-0153', 'AP880325-0120', 'AP881020-0182', 'AP880702-0184', 'AP881229-0067', 'AP891110-0064', 'AP890525-0163', 'AP890811-0205', 'AP891004-0135', 'AP880226-0027', 'AP891125-0070', 'AP880719-0051', 'AP880604-0024', 'AP881124-0016', 'AP880212-0120', 'AP880715-0234', 'AP880225-0004', 'AP880215-0218', 'AP891213-0183', 'AP880707-0114', 'AP891204-0277', 'AP890626-0264', 'AP881222-0201', 'AP891101-0042', 'AP880713-0198', 'AP891012-0312', 'AP881122-0020']\n"
     ]
    }
   ],
   "source": [
    "# Only the TF-IDF top 1000 needs to be evaluated for the PLM and for the Latent Semantic Models\n",
    "from collections import defaultdict \n",
    "top_1000_per_test_query = defaultdict(list)\n",
    "\n",
    "with open('./run/TF-IDF.run', 'r') as top_docs_per_query:\n",
    "    for line in top_docs_per_query:\n",
    "        query_id = line.split(' ')[0]\n",
    "        document_name = line.split(' ')[2]\n",
    "        top_1000_per_test_query[query_id].append(document_name)\n",
    "\n",
    "\n",
    "\n",
    "print('The top 1000 relevant documents for query 51:', top_1000_per_test_query['51'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'num_rel': [('100', '43'), ('101', '27'), ('102', '19'), ('104', '51'), ('105', '33'), ('106', '47'), ('107', '30'), ('108', '94'), ('109', '8'), ('110', '387'), ('112', '12'), ('113', '32'), ('115', '79'), ('116', '17'), ('117', '29'), ('118', '198'), ('119', '239'), ('121', '48'), ('122', '20'), ('124', '58'), ('125', '95'), ('126', '174'), ('127', '165'), ('128', '56'), ('129', '167'), ('130', '228'), ('131', '6'), ('132', '137'), ('133', '13'), ('134', '8'), ('136', '10'), ('137', '54'), ('138', '36'), ('139', '47'), ('140', '25'), ('141', '22'), ('142', '336'), ('145', '103'), ('146', '320'), ('147', '118'), ('148', '220'), ('149', '30'), ('150', '236'), ('152', '311'), ('153', '37'), ('154', '450'), ('156', '330'), ('157', '20'), ('159', '5'), ('160', '26'), ('161', '133'), ('162', '88'), ('163', '78'), ('164', '34'), ('166', '19'), ('168', '63'), ('169', '32'), ('171', '7'), ('172', '8'), ('174', '83'), ('175', '68'), ('176', '140'), ('177', '81'), ('178', '30'), ('179', '41'), ('181', '10'), ('183', '61'), ('184', '44'), ('185', '105'), ('186', '43'), ('187', '147'), ('188', '67'), ('189', '882'), ('190', '31'), ('191', '127'), ('193', '120'), ('194', '76'), ('195', '152'), ('196', '28'), ('197', '63'), ('198', '76'), ('199', '15'), ('200', '39'), ('51', '32'), ('52', '278'), ('54', '92'), ('55', '243'), ('56', '284'), ('58', '99'), ('59', '507'), ('60', '9'), ('61', '67'), ('62', '227'), ('63', '5'), ('64', '285'), ('65', '0'), ('66', '2'), ('67', '442'), ('68', '47'), ('70', '40'), ('71', '308'), ('72', '45'), ('73', '106'), ('75', '13'), ('76', '69'), ('77', '115'), ('79', '128'), ('80', '168'), ('81', '53'), ('82', '123'), ('83', '247'), ('84', '22'), ('85', '614'), ('87', '52'), ('88', '66'), ('91', '5'), ('96', '12'), ('97', '20'), ('98', '18'), ('99', '173'), ('all', '13263')], 'num_rel_ret': [('100', '14'), ('101', '25'), ('102', '13'), ('104', '50'), ('105', '0'), ('106', '36'), ('107', '22'), ('108', '28'), ('109', '0'), ('110', '196'), ('112', '12'), ('113', '24'), ('115', '74'), ('116', '17'), ('117', '29'), ('118', '83'), ('119', '77'), ('121', '35'), ('122', '18'), ('124', '32'), ('125', '81'), ('126', '51'), ('127', '9'), ('128', '47'), ('129', '82'), ('130', '159'), ('131', '6'), ('132', '131'), ('133', '13'), ('134', '6'), ('136', '10'), ('137', '37'), ('138', '27'), ('139', '34'), ('140', '20'), ('141', '14'), ('142', '52'), ('145', '33'), ('146', '196'), ('147', '22'), ('148', '43'), ('149', '20'), ('150', '29'), ('152', '77'), ('153', '28'), ('154', '181'), ('156', '149'), ('157', '20'), ('159', '3'), ('160', '13'), ('161', '121'), ('162', '43'), ('163', '75'), ('164', '32'), ('166', '15'), ('168', '55'), ('169', '24'), ('171', '6'), ('172', '6'), ('174', '82'), ('175', '52'), ('176', '25'), ('177', '58'), ('178', '21'), ('179', '20'), ('181', '7'), ('183', '61'), ('184', '38'), ('185', '74'), ('186', '17'), ('187', '87'), ('188', '58'), ('189', '227'), ('190', '22'), ('191', '7'), ('193', '63'), ('194', '25'), ('195', '65'), ('196', '21'), ('197', '14'), ('198', '56'), ('199', '3'), ('200', '37'), ('51', '32'), ('52', '207'), ('54', '82'), ('55', '221'), ('56', '227'), ('58', '85'), ('59', '99'), ('60', '4'), ('61', '53'), ('62', '35'), ('63', '4'), ('64', '125'), ('65', '0'), ('66', '0'), ('67', '29'), ('68', '18'), ('70', '40'), ('71', '77'), ('72', '13'), ('73', '2'), ('75', '10'), ('76', '16'), ('77', '66'), ('79', '7'), ('80', '9'), ('81', '37'), ('82', '80'), ('83', '58'), ('84', '12'), ('85', '294'), ('87', '3'), ('88', '20'), ('91', '0'), ('96', '9'), ('97', '17'), ('98', '15'), ('99', '152'), ('all', '6053')], 'map': [('100', '0.0224'), ('101', '0.2055'), ('102', '0.0869'), ('104', '0.3027'), ('105', '0.0000'), ('106', '0.0611'), ('107', '0.1315'), ('108', '0.0100'), ('109', '0.0000'), ('110', '0.1078'), ('112', '0.3259'), ('113', '0.0215'), ('115', '0.1151'), ('116', '0.5316'), ('117', '0.2941'), ('118', '0.0593'), ('119', '0.0443'), ('121', '0.0691'), ('122', '0.0525'), ('124', '0.0844'), ('125', '0.1664'), ('126', '0.0325'), ('127', '0.0004'), ('128', '0.2988'), ('129', '0.0556'), ('130', '0.2655'), ('131', '0.0458'), ('132', '0.6335'), ('133', '0.6495'), ('134', '0.6134'), ('136', '0.0964'), ('137', '0.1253'), ('138', '0.0313'), ('139', '0.1124'), ('140', '0.0517'), ('141', '0.0241'), ('142', '0.0082'), ('145', '0.0457'), ('146', '0.1863'), ('147', '0.0250'), ('148', '0.0199'), ('149', '0.0435'), ('150', '0.0030'), ('152', '0.0194'), ('153', '0.1848'), ('154', '0.1791'), ('156', '0.0831'), ('157', '0.2361'), ('159', '0.0586'), ('160', '0.1095'), ('161', '0.5455'), ('162', '0.1090'), ('163', '0.7730'), ('164', '0.4204'), ('166', '0.1472'), ('168', '0.0970'), ('169', '0.0721'), ('171', '0.0188'), ('172', '0.2020'), ('174', '0.4689'), ('175', '0.1786'), ('176', '0.0056'), ('177', '0.0862'), ('178', '0.0376'), ('179', '0.0170'), ('181', '0.0284'), ('183', '0.7114'), ('184', '0.1582'), ('185', '0.0946'), ('186', '0.0255'), ('187', '0.0979'), ('188', '0.2102'), ('189', '0.0650'), ('190', '0.0507'), ('191', '0.0005'), ('193', '0.3206'), ('194', '0.0165'), ('195', '0.0341'), ('196', '0.0388'), ('197', '0.0412'), ('198', '0.1193'), ('199', '0.0005'), ('200', '0.3998'), ('51', '0.4339'), ('52', '0.4976'), ('54', '0.1650'), ('55', '0.4063'), ('56', '0.5198'), ('58', '0.2821'), ('59', '0.0300'), ('60', '0.0024'), ('61', '0.1036'), ('62', '0.0188'), ('63', '0.0385'), ('64', '0.0451'), ('65', '0.0000'), ('66', '0.0000'), ('67', '0.0024'), ('68', '0.0094'), ('70', '0.5689'), ('71', '0.0608'), ('72', '0.0070'), ('73', '0.0001'), ('75', '0.0557'), ('76', '0.0031'), ('77', '0.3235'), ('79', '0.0004'), ('80', '0.0006'), ('81', '0.1197'), ('82', '0.1537'), ('83', '0.0203'), ('84', '0.0121'), ('85', '0.2477'), ('87', '0.0010'), ('88', '0.0042'), ('91', '0.0000'), ('96', '0.0464'), ('97', '0.1596'), ('98', '0.2012'), ('99', '0.1681'), ('all', '0.1436')], 'P_5': [('100', '0.2000'), ('101', '0.4000'), ('102', '0.2000'), ('104', '0.4000'), ('105', '0.0000'), ('106', '0.0000'), ('107', '0.2000'), ('108', '0.0000'), ('109', '0.0000'), ('110', '0.2000'), ('112', '0.2000'), ('113', '0.0000'), ('115', '0.2000'), ('116', '0.8000'), ('117', '0.2000'), ('118', '0.2000'), ('119', '0.0000'), ('121', '0.4000'), ('122', '0.0000'), ('124', '0.6000'), ('125', '0.2000'), ('126', '0.6000'), ('127', '0.0000'), ('128', '0.4000'), ('129', '0.0000'), ('130', '0.8000'), ('131', '0.0000'), ('132', '0.8000'), ('133', '0.8000'), ('134', '0.8000'), ('136', '0.2000'), ('137', '0.4000'), ('138', '0.0000'), ('139', '0.6000'), ('140', '0.0000'), ('141', '0.0000'), ('142', '0.2000'), ('145', '0.4000'), ('146', '0.4000'), ('147', '0.2000'), ('148', '0.0000'), ('149', '0.2000'), ('150', '0.0000'), ('152', '0.0000'), ('153', '0.8000'), ('154', '0.4000'), ('156', '0.0000'), ('157', '0.4000'), ('159', '0.0000'), ('160', '0.4000'), ('161', '1.0000'), ('162', '0.2000'), ('163', '0.8000'), ('164', '0.8000'), ('166', '0.4000'), ('168', '0.0000'), ('169', '0.0000'), ('171', '0.0000'), ('172', '0.2000'), ('174', '0.6000'), ('175', '0.0000'), ('176', '0.0000'), ('177', '0.0000'), ('178', '0.2000'), ('179', '0.0000'), ('181', '0.0000'), ('183', '1.0000'), ('184', '0.6000'), ('185', '0.0000'), ('186', '0.0000'), ('187', '0.2000'), ('188', '0.6000'), ('189', '0.8000'), ('190', '0.2000'), ('191', '0.0000'), ('193', '1.0000'), ('194', '0.0000'), ('195', '0.0000'), ('196', '0.2000'), ('197', '0.4000'), ('198', '0.2000'), ('199', '0.0000'), ('200', '0.6000'), ('51', '0.8000'), ('52', '0.8000'), ('54', '0.0000'), ('55', '0.2000'), ('56', '1.0000'), ('58', '0.4000'), ('59', '0.0000'), ('60', '0.0000'), ('61', '0.4000'), ('62', '0.0000'), ('63', '0.0000'), ('64', '0.2000'), ('65', '0.0000'), ('66', '0.0000'), ('67', '0.0000'), ('68', '0.0000'), ('70', '1.0000'), ('71', '0.4000'), ('72', '0.0000'), ('73', '0.0000'), ('75', '0.0000'), ('76', '0.0000'), ('77', '0.8000'), ('79', '0.0000'), ('80', '0.0000'), ('81', '0.6000'), ('82', '0.6000'), ('83', '0.0000'), ('84', '0.0000'), ('85', '0.8000'), ('87', '0.0000'), ('88', '0.0000'), ('91', '0.0000'), ('96', '0.0000'), ('97', '0.4000'), ('98', '0.4000'), ('99', '0.0000'), ('all', '0.2667')], 'ndcg_cut_10': [('100', '0.1100'), ('101', '0.4374'), ('102', '0.1732'), ('104', '0.4548'), ('105', '0.0000'), ('106', '0.0000'), ('107', '0.1584'), ('108', '0.0000'), ('109', '0.0000'), ('110', '0.2837'), ('112', '0.3558'), ('113', '0.0000'), ('115', '0.1100'), ('116', '0.7670'), ('117', '0.2966'), ('118', '0.3094'), ('119', '0.1357'), ('121', '0.3149'), ('122', '0.0000'), ('124', '0.4537'), ('125', '0.1389'), ('126', '0.4847'), ('127', '0.0000'), ('128', '0.3430'), ('129', '0.0000'), ('130', '0.6995'), ('131', '0.0911'), ('132', '0.7878'), ('133', '0.7569'), ('134', '0.7606'), ('136', '0.1100'), ('137', '0.2048'), ('138', '0.0000'), ('139', '0.4441'), ('140', '0.0000'), ('141', '0.0694'), ('142', '0.1584'), ('145', '0.3301'), ('146', '0.3785'), ('147', '0.0851'), ('148', '0.0636'), ('149', '0.1100'), ('150', '0.0000'), ('152', '0.0000'), ('153', '0.6274'), ('154', '0.6023'), ('156', '0.0784'), ('157', '0.3715'), ('159', '0.1131'), ('160', '0.3590'), ('161', '1.0000'), ('162', '0.2985'), ('163', '0.7799'), ('164', '0.5619'), ('166', '0.3527'), ('168', '0.0734'), ('169', '0.0000'), ('171', '0.0000'), ('172', '0.3373'), ('174', '0.6215'), ('175', '0.1331'), ('176', '0.0000'), ('177', '0.0000'), ('178', '0.0851'), ('179', '0.0000'), ('181', '0.0000'), ('183', '1.0000'), ('184', '0.4441'), ('185', '0.0000'), ('186', '0.0734'), ('187', '0.2201'), ('188', '0.6773'), ('189', '0.4983'), ('190', '0.1389'), ('191', '0.0000'), ('193', '0.9364'), ('194', '0.0663'), ('195', '0.0000'), ('196', '0.0851'), ('197', '0.3938'), ('198', '0.3569'), ('199', '0.0000'), ('200', '0.6570'), ('51', '0.7728'), ('52', '0.8611'), ('54', '0.2115'), ('55', '0.1585'), ('56', '1.0000'), ('58', '0.4643'), ('59', '0.0663'), ('60', '0.0000'), ('61', '0.2048'), ('62', '0.1331'), ('63', '0.0000'), ('64', '0.0851'), ('65', '0.0000'), ('66', '0.0000'), ('67', '0.0734'), ('68', '0.0000'), ('70', '0.7936'), ('71', '0.3979'), ('72', '0.0000'), ('73', '0.0000'), ('75', '0.0000'), ('76', '0.0000'), ('77', '0.7034'), ('79', '0.0000'), ('80', '0.0000'), ('81', '0.4074'), ('82', '0.2900'), ('83', '0.0000'), ('84', '0.0000'), ('85', '0.7695'), ('87', '0.0000'), ('88', '0.0000'), ('91', '0.0000'), ('96', '0.0000'), ('97', '0.3590'), ('98', '0.3964'), ('99', '0.0000'), ('all', '0.2539')], 'recall_1000': [('100', '0.3256'), ('101', '0.9259'), ('102', '0.6842'), ('104', '0.9804'), ('105', '0.0000'), ('106', '0.7660'), ('107', '0.7333'), ('108', '0.2979'), ('109', '0.0000'), ('110', '0.5065'), ('112', '1.0000'), ('113', '0.7500'), ('115', '0.9367'), ('116', '1.0000'), ('117', '1.0000'), ('118', '0.4192'), ('119', '0.3222'), ('121', '0.7292'), ('122', '0.9000'), ('124', '0.5517'), ('125', '0.8526'), ('126', '0.2931'), ('127', '0.0545'), ('128', '0.8393'), ('129', '0.4910'), ('130', '0.6974'), ('131', '1.0000'), ('132', '0.9562'), ('133', '1.0000'), ('134', '0.7500'), ('136', '1.0000'), ('137', '0.6852'), ('138', '0.7500'), ('139', '0.7234'), ('140', '0.8000'), ('141', '0.6364'), ('142', '0.1548'), ('145', '0.3204'), ('146', '0.6125'), ('147', '0.1864'), ('148', '0.1955'), ('149', '0.6667'), ('150', '0.1229'), ('152', '0.2476'), ('153', '0.7568'), ('154', '0.4022'), ('156', '0.4515'), ('157', '1.0000'), ('159', '0.6000'), ('160', '0.5000'), ('161', '0.9098'), ('162', '0.4886'), ('163', '0.9615'), ('164', '0.9412'), ('166', '0.7895'), ('168', '0.8730'), ('169', '0.7500'), ('171', '0.8571'), ('172', '0.7500'), ('174', '0.9880'), ('175', '0.7647'), ('176', '0.1786'), ('177', '0.7160'), ('178', '0.7000'), ('179', '0.4878'), ('181', '0.7000'), ('183', '1.0000'), ('184', '0.8636'), ('185', '0.7048'), ('186', '0.3953'), ('187', '0.5918'), ('188', '0.8657'), ('189', '0.2574'), ('190', '0.7097'), ('191', '0.0551'), ('193', '0.5250'), ('194', '0.3289'), ('195', '0.4276'), ('196', '0.7500'), ('197', '0.2222'), ('198', '0.7368'), ('199', '0.2000'), ('200', '0.9487'), ('51', '1.0000'), ('52', '0.7446'), ('54', '0.8913'), ('55', '0.9095'), ('56', '0.7993'), ('58', '0.8586'), ('59', '0.1953'), ('60', '0.4444'), ('61', '0.7910'), ('62', '0.1542'), ('63', '0.8000'), ('64', '0.4386'), ('65', '0.0000'), ('66', '0.0000'), ('67', '0.0656'), ('68', '0.3830'), ('70', '1.0000'), ('71', '0.2500'), ('72', '0.2889'), ('73', '0.0189'), ('75', '0.7692'), ('76', '0.2319'), ('77', '0.5739'), ('79', '0.0547'), ('80', '0.0536'), ('81', '0.6981'), ('82', '0.6504'), ('83', '0.2348'), ('84', '0.5455'), ('85', '0.4788'), ('87', '0.0577'), ('88', '0.3030'), ('91', '0.0000'), ('96', '0.7500'), ('97', '0.8500'), ('98', '0.8333'), ('99', '0.8786'), ('all', '0.5834')]})\n"
     ]
    }
   ],
   "source": [
    "print(eval_data[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def run_retrieval(model_name, score_fn, model_type, query_set):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    :param model_type: indicates if a vector space or a proabilistic model is run. Takes either \"Vector space\"\n",
    "                        or \"Probabilistic\"\n",
    "    :param query_set: validation_queries_ids or test_queries_ids\n",
    "    \"\"\"\n",
    "    run_out_path = '{}_test_queries_len_C_is_num_words.run'.format(model_name)\n",
    "#     pickle_path = '{}_validation_queries.pickle'.format(model_name)\n",
    "    if os.path.exists(run_out_path):\n",
    "        print(\"File already exists; exiting...\")\n",
    "        return\n",
    "    \n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    \n",
    "\n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    data = {}\n",
    "    if model_name == 'TF-IDF':\n",
    "        weight_fn = tfidf\n",
    "        query_weight_fn = tfidf_query\n",
    "    if model_name == 'BM25':\n",
    "        weight_fn = BM25\n",
    "        query_weight_fn = BM25_query\n",
    "    counter = 1\n",
    "\n",
    "    for query_id, query_terms in tokenized_queries.items():\n",
    "        # Run for uneven queries in query set\n",
    "#         if int(query_id) % 2 == 0:\n",
    "#             continue\n",
    "        if query_id not in query_set:\n",
    "            continue\n",
    "\n",
    "        for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "                        \n",
    "            # For PLM, only run top 1000 documents\n",
    "            if model_name == 'PLM':\n",
    "                if int_doc_id not in top_1000_per_test_query[query_id]:\n",
    "                    continue\n",
    "                    \n",
    "            ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "            # Only calculate score of document if it contains query words,\n",
    "            # otherwise the score is zero for the vector based models\n",
    "            length_query = len(query_terms)\n",
    "\n",
    "            for i, query_term in enumerate(query_terms):\n",
    "                # Only compute score if the query word is in the document\n",
    "                \n",
    "                if int_doc_id in inverted_index[query_term].keys():\n",
    "\n",
    "                    # Calculate score for document and query terms\n",
    "                    if model_type == \"Vector space\":\n",
    "                        score = score_similarity(query_terms, int_doc_id, weight_fn, query_weight_fn)\n",
    "                    elif model_type == \"Probabilistic\":\n",
    "                        print(query_terms)\n",
    "                        score = score_fn(query_terms, int_doc_id)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    if query_id in data.keys():\n",
    "                        data[query_id].append(tuple([float(score), str(ext_doc_id)]))\n",
    "                    else:\n",
    "                        data[query_id] = [((tuple([float(score),str(ext_doc_id)])))]\n",
    "\n",
    "                    # If the document has a query word, than the score is calculated and appended by this point\n",
    "                    break\n",
    "\n",
    "\n",
    "                # If up until the last query word, the query word did not appear in the \n",
    "                # document, the score is the lowerst possible (-infinity)\n",
    "                elif i == length_query-1:\n",
    "                    score = -np.inf\n",
    "                    if query_id in data.keys():\n",
    "                        data[query_id].append(tuple([float(score), str(ext_doc_id)]))\n",
    "                    else:\n",
    "                        data[query_id] = [((tuple([float(score),str(ext_doc_id)])))]\n",
    "\n",
    "\n",
    "\n",
    "        print('queries: ', counter,'/',len(query_set),'\\t this took: ', time.time() - retrieval_start_time, ' seconds in total' )\n",
    "        counter += 1\n",
    "        \n",
    "        # transform the list of tuples to a tuple of tuples, since this is the requered datastructue\n",
    "        data[query_id] = tuple(data[query_id])   \n",
    "\n",
    "    # Overwrite the previous pickle and .run file with the new (accumulated) data variable\n",
    "#     with open(pickle_path, 'wb') as f_pickle:\n",
    "#         pickle.dump(data, f_pickle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using jelinek_0.1\n",
      "queries:  1 / 30 \t this took:  3.416888475418091  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.697065114974976  seconds in total\n",
      "queries:  3 / 30 \t this took:  14.66519570350647  seconds in total\n",
      "queries:  4 / 30 \t this took:  21.994452476501465  seconds in total\n",
      "queries:  5 / 30 \t this took:  25.5604465007782  seconds in total\n",
      "queries:  6 / 30 \t this took:  31.57123875617981  seconds in total\n",
      "queries:  7 / 30 \t this took:  71.21541666984558  seconds in total\n",
      "queries:  8 / 30 \t this took:  84.58218193054199  seconds in total\n",
      "queries:  9 / 30 \t this took:  120.2080647945404  seconds in total\n",
      "queries:  10 / 30 \t this took:  150.47785425186157  seconds in total\n",
      "queries:  11 / 30 \t this took:  155.82314944267273  seconds in total\n",
      "queries:  12 / 30 \t this took:  161.4430012702942  seconds in total\n",
      "queries:  13 / 30 \t this took:  165.6775131225586  seconds in total\n",
      "queries:  14 / 30 \t this took:  170.0045347213745  seconds in total\n",
      "queries:  15 / 30 \t this took:  179.7385859489441  seconds in total\n",
      "queries:  16 / 30 \t this took:  201.67529344558716  seconds in total\n",
      "queries:  17 / 30 \t this took:  212.94965147972107  seconds in total\n",
      "queries:  18 / 30 \t this took:  221.8131685256958  seconds in total\n",
      "queries:  19 / 30 \t this took:  232.41422295570374  seconds in total\n",
      "queries:  20 / 30 \t this took:  284.9357199668884  seconds in total\n",
      "queries:  21 / 30 \t this took:  288.4158914089203  seconds in total\n",
      "queries:  22 / 30 \t this took:  312.1396150588989  seconds in total\n",
      "queries:  23 / 30 \t this took:  359.60684084892273  seconds in total\n",
      "queries:  24 / 30 \t this took:  377.86703872680664  seconds in total\n",
      "queries:  25 / 30 \t this took:  401.5615062713623  seconds in total\n",
      "queries:  26 / 30 \t this took:  405.6410517692566  seconds in total\n",
      "queries:  27 / 30 \t this took:  409.05937218666077  seconds in total\n",
      "queries:  28 / 30 \t this took:  417.2496819496155  seconds in total\n",
      "queries:  29 / 30 \t this took:  428.57210397720337  seconds in total\n",
      "queries:  30 / 30 \t this took:  433.80344438552856  seconds in total\n",
      "Retrieving using jelinek_0.5\n",
      "queries:  1 / 30 \t this took:  3.2713944911956787  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.392726421356201  seconds in total\n",
      "queries:  3 / 30 \t this took:  14.35510802268982  seconds in total\n",
      "queries:  4 / 30 \t this took:  20.597657203674316  seconds in total\n",
      "queries:  5 / 30 \t this took:  23.724166870117188  seconds in total\n",
      "queries:  6 / 30 \t this took:  29.332257509231567  seconds in total\n",
      "queries:  7 / 30 \t this took:  68.42884850502014  seconds in total\n",
      "queries:  8 / 30 \t this took:  81.58713054656982  seconds in total\n",
      "queries:  9 / 30 \t this took:  125.84776782989502  seconds in total\n",
      "queries:  10 / 30 \t this took:  2695.4410107135773  seconds in total\n",
      "queries:  11 / 30 \t this took:  2701.2653028964996  seconds in total\n",
      "queries:  12 / 30 \t this took:  2707.2425441741943  seconds in total\n",
      "queries:  13 / 30 \t this took:  2711.5566971302032  seconds in total\n",
      "queries:  14 / 30 \t this took:  2715.9866852760315  seconds in total\n",
      "queries:  15 / 30 \t this took:  2725.9971780776978  seconds in total\n",
      "queries:  16 / 30 \t this took:  2748.0817189216614  seconds in total\n",
      "queries:  17 / 30 \t this took:  2759.396828174591  seconds in total\n",
      "queries:  18 / 30 \t this took:  2768.315041542053  seconds in total\n",
      "queries:  19 / 30 \t this took:  2779.01127576828  seconds in total\n",
      "queries:  20 / 30 \t this took:  2831.9556226730347  seconds in total\n",
      "queries:  21 / 30 \t this took:  2835.367243051529  seconds in total\n",
      "queries:  22 / 30 \t this took:  2857.3667833805084  seconds in total\n",
      "queries:  23 / 30 \t this took:  2899.292067050934  seconds in total\n",
      "queries:  24 / 30 \t this took:  2917.345059156418  seconds in total\n",
      "queries:  25 / 30 \t this took:  2939.8253972530365  seconds in total\n",
      "queries:  26 / 30 \t this took:  2943.837072610855  seconds in total\n",
      "queries:  27 / 30 \t this took:  2947.222156047821  seconds in total\n",
      "queries:  28 / 30 \t this took:  2955.230706214905  seconds in total\n",
      "queries:  29 / 30 \t this took:  2966.4171028137207  seconds in total\n",
      "queries:  30 / 30 \t this took:  2971.6078300476074  seconds in total\n",
      "Retrieving using jelinek_0.9\n",
      "queries:  1 / 30 \t this took:  3.2284536361694336  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.331223487854004  seconds in total\n",
      "queries:  3 / 30 \t this took:  14.32742691040039  seconds in total\n",
      "queries:  4 / 30 \t this took:  20.543068647384644  seconds in total\n",
      "queries:  5 / 30 \t this took:  23.64393448829651  seconds in total\n",
      "queries:  6 / 30 \t this took:  29.186496019363403  seconds in total\n",
      "queries:  7 / 30 \t this took:  67.80026865005493  seconds in total\n",
      "queries:  8 / 30 \t this took:  81.10146307945251  seconds in total\n",
      "queries:  9 / 30 \t this took:  112.44656324386597  seconds in total\n",
      "queries:  10 / 30 \t this took:  142.6501362323761  seconds in total\n",
      "queries:  11 / 30 \t this took:  148.46870875358582  seconds in total\n",
      "queries:  12 / 30 \t this took:  154.17104291915894  seconds in total\n",
      "queries:  13 / 30 \t this took:  158.31584811210632  seconds in total\n",
      "queries:  14 / 30 \t this took:  162.39745950698853  seconds in total\n",
      "queries:  15 / 30 \t this took:  171.2521231174469  seconds in total\n",
      "queries:  16 / 30 \t this took:  192.85552763938904  seconds in total\n",
      "queries:  17 / 30 \t this took:  204.09829139709473  seconds in total\n",
      "queries:  18 / 30 \t this took:  212.953631401062  seconds in total\n",
      "queries:  19 / 30 \t this took:  223.5280044078827  seconds in total\n",
      "queries:  20 / 30 \t this took:  275.38641476631165  seconds in total\n",
      "queries:  21 / 30 \t this took:  278.76223635673523  seconds in total\n",
      "queries:  22 / 30 \t this took:  300.698344707489  seconds in total\n",
      "queries:  23 / 30 \t this took:  342.51977014541626  seconds in total\n",
      "queries:  24 / 30 \t this took:  362.46495151519775  seconds in total\n",
      "queries:  25 / 30 \t this took:  385.0403153896332  seconds in total\n",
      "queries:  26 / 30 \t this took:  389.04577231407166  seconds in total\n",
      "queries:  27 / 30 \t this took:  392.4241461753845  seconds in total\n",
      "queries:  28 / 30 \t this took:  400.42273926734924  seconds in total\n",
      "queries:  29 / 30 \t this took:  411.6791830062866  seconds in total\n",
      "queries:  30 / 30 \t this took:  416.97077679634094  seconds in total\n",
      "Retrieving using dirichlet_500\n",
      "queries:  1 / 30 \t this took:  3.1879184246063232  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.298223257064819  seconds in total\n",
      "queries:  3 / 30 \t this took:  10.465267419815063  seconds in total\n",
      "queries:  4 / 30 \t this took:  14.272425174713135  seconds in total\n",
      "queries:  5 / 30 \t this took:  17.378443241119385  seconds in total\n",
      "queries:  6 / 30 \t this took:  21.109564781188965  seconds in total\n",
      "queries:  7 / 30 \t this took:  27.065361499786377  seconds in total\n",
      "queries:  8 / 30 \t this took:  31.82423686981201  seconds in total\n",
      "queries:  9 / 30 \t this took:  37.244168519973755  seconds in total\n",
      "queries:  10 / 30 \t this took:  42.5980761051178  seconds in total\n",
      "queries:  11 / 30 \t this took:  46.31094312667847  seconds in total\n",
      "queries:  12 / 30 \t this took:  50.125834226608276  seconds in total\n",
      "queries:  13 / 30 \t this took:  53.65438532829285  seconds in total\n",
      "queries:  14 / 30 \t this took:  57.116909980773926  seconds in total\n",
      "queries:  15 / 30 \t this took:  61.322213888168335  seconds in total\n",
      "queries:  16 / 30 \t this took:  66.36995601654053  seconds in total\n",
      "queries:  17 / 30 \t this took:  70.87054777145386  seconds in total\n",
      "queries:  18 / 30 \t this took:  75.15294098854065  seconds in total\n",
      "queries:  19 / 30 \t this took:  79.51807808876038  seconds in total\n",
      "queries:  20 / 30 \t this took:  85.47456526756287  seconds in total\n",
      "queries:  21 / 30 \t this took:  88.7726697921753  seconds in total\n",
      "queries:  22 / 30 \t this took:  94.06576752662659  seconds in total\n",
      "queries:  23 / 30 \t this took:  100.13621473312378  seconds in total\n",
      "queries:  24 / 30 \t this took:  104.97512483596802  seconds in total\n",
      "queries:  25 / 30 \t this took:  110.78329110145569  seconds in total\n",
      "queries:  26 / 30 \t this took:  114.36522722244263  seconds in total\n",
      "queries:  27 / 30 \t this took:  117.61711025238037  seconds in total\n",
      "queries:  28 / 30 \t this took:  121.80963897705078  seconds in total\n",
      "queries:  29 / 30 \t this took:  126.39342856407166  seconds in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries:  30 / 30 \t this took:  130.0673632621765  seconds in total\n",
      "Retrieving using dirichlet_1000\n",
      "queries:  1 / 30 \t this took:  3.2093024253845215  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.301284551620483  seconds in total\n",
      "queries:  3 / 30 \t this took:  10.451395034790039  seconds in total\n",
      "queries:  4 / 30 \t this took:  14.278336763381958  seconds in total\n",
      "queries:  5 / 30 \t this took:  17.391713857650757  seconds in total\n",
      "queries:  6 / 30 \t this took:  21.097501277923584  seconds in total\n",
      "queries:  7 / 30 \t this took:  27.04846167564392  seconds in total\n",
      "queries:  8 / 30 \t this took:  31.79634952545166  seconds in total\n",
      "queries:  9 / 30 \t this took:  37.209357023239136  seconds in total\n",
      "queries:  10 / 30 \t this took:  42.57454061508179  seconds in total\n",
      "queries:  11 / 30 \t this took:  46.28764462471008  seconds in total\n",
      "queries:  12 / 30 \t this took:  50.11553597450256  seconds in total\n",
      "queries:  13 / 30 \t this took:  53.57543468475342  seconds in total\n",
      "queries:  14 / 30 \t this took:  57.10132575035095  seconds in total\n",
      "queries:  15 / 30 \t this took:  61.34827947616577  seconds in total\n",
      "queries:  16 / 30 \t this took:  66.38619422912598  seconds in total\n",
      "queries:  17 / 30 \t this took:  70.74709296226501  seconds in total\n",
      "queries:  18 / 30 \t this took:  75.03145098686218  seconds in total\n",
      "queries:  19 / 30 \t this took:  79.4124608039856  seconds in total\n",
      "queries:  20 / 30 \t this took:  85.35966444015503  seconds in total\n",
      "queries:  21 / 30 \t this took:  88.646488904953  seconds in total\n",
      "queries:  22 / 30 \t this took:  93.9273533821106  seconds in total\n",
      "queries:  23 / 30 \t this took:  100.01569652557373  seconds in total\n",
      "queries:  24 / 30 \t this took:  104.86095023155212  seconds in total\n",
      "queries:  25 / 30 \t this took:  111.11506414413452  seconds in total\n",
      "queries:  26 / 30 \t this took:  115.21716117858887  seconds in total\n",
      "queries:  27 / 30 \t this took:  118.87448406219482  seconds in total\n",
      "queries:  28 / 30 \t this took:  123.07359838485718  seconds in total\n",
      "queries:  29 / 30 \t this took:  127.78588438034058  seconds in total\n",
      "queries:  30 / 30 \t this took:  131.45995807647705  seconds in total\n",
      "Retrieving using dirichlet_1500\n",
      "queries:  1 / 30 \t this took:  3.1709158420562744  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.265203237533569  seconds in total\n",
      "queries:  3 / 30 \t this took:  10.413283824920654  seconds in total\n",
      "queries:  4 / 30 \t this took:  14.206728458404541  seconds in total\n",
      "queries:  5 / 30 \t this took:  17.295530796051025  seconds in total\n",
      "queries:  6 / 30 \t this took:  20.999103546142578  seconds in total\n",
      "queries:  7 / 30 \t this took:  26.9361732006073  seconds in total\n",
      "queries:  8 / 30 \t this took:  31.71091604232788  seconds in total\n",
      "queries:  9 / 30 \t this took:  37.10131335258484  seconds in total\n",
      "queries:  10 / 30 \t this took:  42.48507595062256  seconds in total\n",
      "queries:  11 / 30 \t this took:  46.208396434783936  seconds in total\n",
      "queries:  12 / 30 \t this took:  50.007176637649536  seconds in total\n",
      "queries:  13 / 30 \t this took:  53.462146282196045  seconds in total\n",
      "queries:  14 / 30 \t this took:  56.90994310379028  seconds in total\n",
      "queries:  15 / 30 \t this took:  61.089866161346436  seconds in total\n",
      "queries:  16 / 30 \t this took:  66.075270652771  seconds in total\n",
      "queries:  17 / 30 \t this took:  70.40781354904175  seconds in total\n",
      "queries:  18 / 30 \t this took:  74.65488362312317  seconds in total\n",
      "queries:  19 / 30 \t this took:  78.99720573425293  seconds in total\n",
      "queries:  20 / 30 \t this took:  84.90478229522705  seconds in total\n",
      "queries:  21 / 30 \t this took:  88.16585445404053  seconds in total\n",
      "queries:  22 / 30 \t this took:  93.43589305877686  seconds in total\n",
      "queries:  23 / 30 \t this took:  99.46626782417297  seconds in total\n",
      "queries:  24 / 30 \t this took:  104.27953743934631  seconds in total\n",
      "queries:  25 / 30 \t this took:  110.03887128829956  seconds in total\n",
      "queries:  26 / 30 \t this took:  113.5836124420166  seconds in total\n",
      "queries:  27 / 30 \t this took:  116.81976318359375  seconds in total\n",
      "queries:  28 / 30 \t this took:  121.00633573532104  seconds in total\n",
      "queries:  29 / 30 \t this took:  125.5401177406311  seconds in total\n",
      "queries:  30 / 30 \t this took:  129.20436549186707  seconds in total\n",
      "Retrieving using absolute_0.1\n",
      "queries:  1 / 30 \t this took:  3.212581157684326  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.283120393753052  seconds in total\n",
      "queries:  3 / 30 \t this took:  12.015497922897339  seconds in total\n",
      "queries:  4 / 30 \t this took:  16.491321086883545  seconds in total\n",
      "queries:  5 / 30 \t this took:  19.58475685119629  seconds in total\n",
      "queries:  6 / 30 \t this took:  23.82409119606018  seconds in total\n",
      "queries:  7 / 30 \t this took:  35.38214373588562  seconds in total\n",
      "queries:  8 / 30 \t this took:  43.32700777053833  seconds in total\n",
      "queries:  9 / 30 \t this took:  52.58265542984009  seconds in total\n",
      "queries:  10 / 30 \t this took:  61.767921686172485  seconds in total\n",
      "queries:  11 / 30 \t this took:  66.09445714950562  seconds in total\n",
      "queries:  12 / 30 \t this took:  70.75033593177795  seconds in total\n",
      "queries:  13 / 30 \t this took:  74.5145857334137  seconds in total\n",
      "queries:  14 / 30 \t this took:  78.24474883079529  seconds in total\n",
      "queries:  15 / 30 \t this took:  84.01413178443909  seconds in total\n",
      "queries:  16 / 30 \t this took:  92.1600341796875  seconds in total\n",
      "queries:  17 / 30 \t this took:  98.12471985816956  seconds in total\n",
      "queries:  18 / 30 \t this took:  104.24799871444702  seconds in total\n",
      "queries:  19 / 30 \t this took:  110.49008202552795  seconds in total\n",
      "queries:  20 / 30 \t this took:  121.49344611167908  seconds in total\n",
      "queries:  21 / 30 \t this took:  124.8580219745636  seconds in total\n",
      "queries:  22 / 30 \t this took:  134.55611872673035  seconds in total\n",
      "queries:  23 / 30 \t this took:  146.94533967971802  seconds in total\n",
      "queries:  24 / 30 \t this took:  154.6232454776764  seconds in total\n",
      "queries:  25 / 30 \t this took:  167.1164309978485  seconds in total\n",
      "queries:  26 / 30 \t this took:  171.02406191825867  seconds in total\n",
      "queries:  27 / 30 \t this took:  174.34877133369446  seconds in total\n",
      "queries:  28 / 30 \t this took:  180.16980934143066  seconds in total\n",
      "queries:  29 / 30 \t this took:  187.28361439704895  seconds in total\n",
      "queries:  30 / 30 \t this took:  191.56105947494507  seconds in total\n",
      "Retrieving using absolute_0.5\n",
      "queries:  1 / 30 \t this took:  3.2048532962799072  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.2872231006622314  seconds in total\n",
      "queries:  3 / 30 \t this took:  12.069818258285522  seconds in total\n",
      "queries:  4 / 30 \t this took:  16.52960729598999  seconds in total\n",
      "queries:  5 / 30 \t this took:  19.61332392692566  seconds in total\n",
      "queries:  6 / 30 \t this took:  23.852800369262695  seconds in total\n",
      "queries:  7 / 30 \t this took:  35.310038805007935  seconds in total\n",
      "queries:  8 / 30 \t this took:  43.234293699264526  seconds in total\n",
      "queries:  9 / 30 \t this took:  52.49171018600464  seconds in total\n",
      "queries:  10 / 30 \t this took:  61.708528995513916  seconds in total\n",
      "queries:  11 / 30 \t this took:  66.02722716331482  seconds in total\n",
      "queries:  12 / 30 \t this took:  70.69468140602112  seconds in total\n",
      "queries:  13 / 30 \t this took:  74.44886112213135  seconds in total\n",
      "queries:  14 / 30 \t this took:  78.20728206634521  seconds in total\n",
      "queries:  15 / 30 \t this took:  84.0107569694519  seconds in total\n",
      "queries:  16 / 30 \t this took:  92.15655064582825  seconds in total\n",
      "queries:  17 / 30 \t this took:  98.12693095207214  seconds in total\n",
      "queries:  18 / 30 \t this took:  104.24467921257019  seconds in total\n",
      "queries:  19 / 30 \t this took:  110.49432158470154  seconds in total\n",
      "queries:  20 / 30 \t this took:  121.51455593109131  seconds in total\n",
      "queries:  21 / 30 \t this took:  124.88034987449646  seconds in total\n",
      "queries:  22 / 30 \t this took:  134.59424924850464  seconds in total\n",
      "queries:  23 / 30 \t this took:  146.99588346481323  seconds in total\n",
      "queries:  24 / 30 \t this took:  154.58039832115173  seconds in total\n",
      "queries:  25 / 30 \t this took:  167.026775598526  seconds in total\n",
      "queries:  26 / 30 \t this took:  170.94609832763672  seconds in total\n",
      "queries:  27 / 30 \t this took:  174.28641033172607  seconds in total\n",
      "queries:  28 / 30 \t this took:  180.1180076599121  seconds in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries:  29 / 30 \t this took:  187.2577884197235  seconds in total\n",
      "queries:  30 / 30 \t this took:  191.562673330307  seconds in total\n",
      "Retrieving using absolute_0.9\n",
      "queries:  1 / 30 \t this took:  3.278108596801758  seconds in total\n",
      "queries:  2 / 30 \t this took:  6.412049055099487  seconds in total\n",
      "queries:  3 / 30 \t this took:  12.158004522323608  seconds in total\n",
      "queries:  4 / 30 \t this took:  16.61941146850586  seconds in total\n",
      "queries:  5 / 30 \t this took:  19.70521855354309  seconds in total\n",
      "queries:  6 / 30 \t this took:  23.936250925064087  seconds in total\n",
      "queries:  7 / 30 \t this took:  35.4056351184845  seconds in total\n",
      "queries:  8 / 30 \t this took:  43.35883355140686  seconds in total\n",
      "queries:  9 / 30 \t this took:  52.61380672454834  seconds in total\n",
      "queries:  10 / 30 \t this took:  61.84353256225586  seconds in total\n",
      "queries:  11 / 30 \t this took:  66.16184592247009  seconds in total\n",
      "queries:  12 / 30 \t this took:  70.81007409095764  seconds in total\n",
      "queries:  13 / 30 \t this took:  74.58201122283936  seconds in total\n",
      "queries:  14 / 30 \t this took:  78.32407355308533  seconds in total\n",
      "queries:  15 / 30 \t this took:  84.10477256774902  seconds in total\n",
      "queries:  16 / 30 \t this took:  92.24094891548157  seconds in total\n",
      "queries:  17 / 30 \t this took:  98.20398926734924  seconds in total\n",
      "queries:  18 / 30 \t this took:  104.32986879348755  seconds in total\n",
      "queries:  19 / 30 \t this took:  110.57472372055054  seconds in total\n",
      "queries:  20 / 30 \t this took:  121.59352111816406  seconds in total\n",
      "queries:  21 / 30 \t this took:  125.04101300239563  seconds in total\n",
      "queries:  22 / 30 \t this took:  134.84127736091614  seconds in total\n",
      "queries:  23 / 30 \t this took:  147.25372004508972  seconds in total\n",
      "queries:  24 / 30 \t this took:  154.82250237464905  seconds in total\n",
      "queries:  25 / 30 \t this took:  167.31230688095093  seconds in total\n",
      "queries:  26 / 30 \t this took:  171.221346616745  seconds in total\n",
      "queries:  27 / 30 \t this took:  174.55956554412842  seconds in total\n",
      "queries:  28 / 30 \t this took:  180.38105177879333  seconds in total\n",
      "queries:  29 / 30 \t this took:  187.4863133430481  seconds in total\n",
      "queries:  30 / 30 \t this took:  191.76417541503906  seconds in total\n",
      "         595583950 function calls in 4808.286 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000 4808.286 2404.143 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2832(run_code)\n",
      "        2    0.000    0.000 4808.286 2404.143 {built-in method builtins.exec}\n",
      "        1    1.939    1.939 4808.286 4808.286 <ipython-input-134-a591ea44b773>:29(<module>)\n",
      "        9  309.772   34.419 4806.347  534.039 <ipython-input-131-7bbf5c262d64>:3(run_retrieval)\n",
      "  6223032   18.709    0.000 3852.585    0.001 <ipython-input-133-1d6e3bc7d6d1>:21(log_multinomial_query_language_model)\n",
      "  6223032   89.130    0.000 3701.345    0.001 <ipython-input-133-1d6e3bc7d6d1>:29(<listcomp>)\n",
      "  9879816   48.877    0.000 3411.449    0.000 <ipython-input-130-0d73b906d01c>:147(jelinek_mercer)\n",
      " 64200822 3409.171    0.000 3409.171    0.000 {method 'document' of 'pyndri.Index' objects}\n",
      "  9879816  674.173    0.000  674.173    0.000 {built-in method builtins.sum}\n",
      "  9879816   44.651    0.000  187.654    0.000 <ipython-input-130-0d73b906d01c>:114(absolute_discounting)\n",
      "  6223032   24.192    0.000   77.708    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:1778(sum)\n",
      "  6223032   18.453    0.000   54.823    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/collections/__init__.py:517(__init__)\n",
      "  6223032    2.734    0.000   51.713    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:31(_sum)\n",
      "  6223032   48.979    0.000   48.979    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "  6223032    8.769    0.000   35.808    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/collections/__init__.py:586(update)\n",
      " 12720420    6.917    0.000   23.691    0.000 {built-in method builtins.isinstance}\n",
      " 29639448   12.313    0.000   18.660    0.000 <ipython-input-31-ae80e85952ef>:5(term_frequency)\n",
      "        9    0.636    0.071   18.310    2.034 <ipython-input-18-51cfbbf3f59d>:5(write_run)\n",
      "      270   17.022    0.063   17.022    0.063 {built-in method builtins.sorted}\n",
      "  6223032    9.990    0.000   16.774    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/abc.py:178(__instancecheck__)\n",
      "209978073   16.491    0.000   16.491    0.000 {method 'keys' of 'dict' objects}\n",
      "  9879816    7.754    0.000   13.111    0.000 <ipython-input-130-0d73b906d01c>:130(dirichlet_prior_smoothing)\n",
      " 76647156    9.049    0.000    9.049    0.000 {built-in method builtins.len}\n",
      " 12446064    6.784    0.000    6.784    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/_weakrefset.py:70(__contains__)\n",
      " 29639448    6.347    0.000    6.347    0.000 {method 'get' of 'dict' objects}\n",
      "  6223032    4.712    0.000    4.712    0.000 {built-in method _collections._count_elements}\n",
      " 44440920    4.512    0.000    4.512    0.000 {method 'append' of 'list' objects}\n",
      "  9879816    4.140    0.000    4.140    0.000 {built-in method builtins.max}\n",
      "  9879816    1.352    0.000    1.352    0.000 {method 'values' of 'dict' objects}\n",
      "   270009    0.494    0.000    0.494    0.000 {method 'format' of 'str' objects}\n",
      "   270000    0.119    0.000    0.119    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      "      279    0.006    0.000    0.101    0.000 {built-in method builtins.print}\n",
      "     3816    0.009    0.000    0.095    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:342(write)\n",
      "     4095    0.037    0.000    0.081    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:180(schedule)\n",
      "     4095    0.031    0.000    0.031    0.000 {built-in method posix.urandom}\n",
      "     4095    0.005    0.000    0.011    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/threading.py:1104(is_alive)\n",
      "     3816    0.001    0.000    0.005    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:297(_schedule_flush)\n",
      "     4095    0.002    0.000    0.005    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/threading.py:1062(_wait_for_tstate_lock)\n",
      "     3816    0.003    0.000    0.003    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:284(_is_master_process)\n",
      "     4095    0.002    0.000    0.002    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        9    0.002    0.000    0.002    0.000 {built-in method io.open}\n",
      "     4095    0.002    0.000    0.002    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:87(_event_pipe)\n",
      "     4095    0.001    0.000    0.001    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/threading.py:506(is_set)\n",
      "     3816    0.001    0.000    0.001    0.000 {built-in method posix.getpid}\n",
      "      270    0.000    0.000    0.000    0.000 {method 'document_base' of 'pyndri.Index' objects}\n",
      "        9    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/genericpath.py:16(exists)\n",
      "      279    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
      "      270    0.000    0.000    0.000    0.000 {method 'maximum_document' of 'pyndri.Index' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/codeop.py:132(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        9    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/_bootlocale.py:23(getpreferredencoding)\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
      "        9    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/codecs.py:185(__init__)\n",
      "       18    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/hooks.py:142(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-134-a591ea44b773>:34(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/utils/ipstruct.py:125(__getattr__)\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:1055(user_global_ns)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/hooks.py:207(pre_run_code_hook)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run run_retrieval\n",
    "\n",
    "'''\n",
    "Jelinek-Mercer (explore different values of  in the range [0.1, 0.5, 0.9]). [5 points]\n",
    "Dirichlet Prior (explore different values of  [500, 1000, 1500]). [5 points]\n",
    "Absolute discounting (explore different values of  in the range [0.1, 0.5, 0.9]). [5 points]\n",
    "'''\n",
    "import functools\n",
    "\n",
    "jelinek_01 = (functools.partial(jelinek_mercer, lambd=0.1), 'jelinek_0.1')\n",
    "jelinek_05 = (functools.partial(jelinek_mercer, lambd=0.5), 'jelinek_0.5')\n",
    "jelinek_09 = (functools.partial(jelinek_mercer, lambd=0.9), 'jelinek_0.9')\n",
    "dirichlet_500 = (functools.partial(dirichlet_prior_smoothing, mu=500), 'dirichlet_500')\n",
    "dirichlet_1000 = (functools.partial(dirichlet_prior_smoothing, mu=1000), 'dirichlet_1000')\n",
    "dirichlet_1500 = (functools.partial(dirichlet_prior_smoothing, mu=1500), 'dirichlet_1500')\n",
    "absolute_01 = (functools.partial(absolute_discounting, delta=0.1), 'absolute_0.1')\n",
    "absolute_05 = (functools.partial(absolute_discounting, delta=0.5), 'absolute_0.5')\n",
    "absolute_09 = (functools.partial(absolute_discounting, delta=0.9), 'absolute_0.9')\n",
    "\n",
    "smoothings_functions = [jelinek_01, jelinek_05, jelinek_09,\n",
    "                        dirichlet_500,dirichlet_1000,dirichlet_1500,\n",
    "                        absolute_01, absolute_05, absolute_09]\n",
    "\n",
    "import cProfile, pstats\n",
    "from io import StringIO\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "for smoothing_func, label in smoothings_functions:\n",
    "    run_retrieval(label, \n",
    "                  functools.partial(log_multinomial_query_language_model, word_probability_func=smoothing_func), \n",
    "                  \"Probabilistic\", validation_queries_ids)\n",
    "\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'P_5': [('100', '0.0000'),\n",
       "              ('101', '0.6000'),\n",
       "              ('102', '0.4000'),\n",
       "              ('104', '0.4000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.4000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.4000'),\n",
       "              ('112', '0.4000'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.0000'),\n",
       "              ('116', '0.4000'),\n",
       "              ('117', '0.2000'),\n",
       "              ('118', '0.4000'),\n",
       "              ('119', '0.2000'),\n",
       "              ('121', '0.2000'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.6000'),\n",
       "              ('125', '0.2000'),\n",
       "              ('126', '0.6000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.2000'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.8000'),\n",
       "              ('131', '0.0000'),\n",
       "              ('132', '0.6000'),\n",
       "              ('133', '0.8000'),\n",
       "              ('134', '0.8000'),\n",
       "              ('136', '0.2000'),\n",
       "              ('137', '0.2000'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.4000'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.0000'),\n",
       "              ('142', '0.0000'),\n",
       "              ('145', '0.4000'),\n",
       "              ('146', '0.6000'),\n",
       "              ('147', '0.2000'),\n",
       "              ('148', '0.4000'),\n",
       "              ('149', '0.2000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.8000'),\n",
       "              ('154', '0.2000'),\n",
       "              ('156', '0.2000'),\n",
       "              ('157', '0.4000'),\n",
       "              ('159', '0.2000'),\n",
       "              ('160', '0.4000'),\n",
       "              ('161', '1.0000'),\n",
       "              ('162', '0.2000'),\n",
       "              ('163', '1.0000'),\n",
       "              ('164', '0.6000'),\n",
       "              ('166', '0.6000'),\n",
       "              ('168', '0.0000'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.2000'),\n",
       "              ('174', '0.6000'),\n",
       "              ('175', '0.2000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.2000'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '1.0000'),\n",
       "              ('184', '0.4000'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.2000'),\n",
       "              ('188', '0.6000'),\n",
       "              ('189', '0.6000'),\n",
       "              ('190', '0.2000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '1.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.0000'),\n",
       "              ('197', '0.4000'),\n",
       "              ('198', '0.4000'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '1.0000'),\n",
       "              ('51', '0.8000'),\n",
       "              ('52', '0.8000'),\n",
       "              ('54', '0.2000'),\n",
       "              ('55', '0.2000'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.4000'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.2000'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.0000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '1.0000'),\n",
       "              ('71', '0.4000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.6000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.4000'),\n",
       "              ('82', '0.6000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.4000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.4000'),\n",
       "              ('98', '0.4000'),\n",
       "              ('99', '0.2000'),\n",
       "              ('all', '0.2650'),\n",
       "              ('100', '0.2000'),\n",
       "              ('101', '0.4000'),\n",
       "              ('102', '0.2000'),\n",
       "              ('104', '0.4000'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0000'),\n",
       "              ('107', '0.2000'),\n",
       "              ('108', '0.0000'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.2000'),\n",
       "              ('112', '0.2000'),\n",
       "              ('113', '0.0000'),\n",
       "              ('115', '0.2000'),\n",
       "              ('116', '0.8000'),\n",
       "              ('117', '0.2000'),\n",
       "              ('118', '0.2000'),\n",
       "              ('119', '0.0000'),\n",
       "              ('121', '0.4000'),\n",
       "              ('122', '0.0000'),\n",
       "              ('124', '0.6000'),\n",
       "              ('125', '0.2000'),\n",
       "              ('126', '0.6000'),\n",
       "              ('127', '0.0000'),\n",
       "              ('128', '0.4000'),\n",
       "              ('129', '0.0000'),\n",
       "              ('130', '0.8000'),\n",
       "              ('131', '0.0000'),\n",
       "              ('132', '0.8000'),\n",
       "              ('133', '0.8000'),\n",
       "              ('134', '0.8000'),\n",
       "              ('136', '0.2000'),\n",
       "              ('137', '0.4000'),\n",
       "              ('138', '0.0000'),\n",
       "              ('139', '0.6000'),\n",
       "              ('140', '0.0000'),\n",
       "              ('141', '0.0000'),\n",
       "              ('142', '0.2000'),\n",
       "              ('145', '0.4000'),\n",
       "              ('146', '0.4000'),\n",
       "              ('147', '0.2000'),\n",
       "              ('148', '0.0000'),\n",
       "              ('149', '0.2000'),\n",
       "              ('150', '0.0000'),\n",
       "              ('152', '0.0000'),\n",
       "              ('153', '0.8000'),\n",
       "              ('154', '0.4000'),\n",
       "              ('156', '0.0000'),\n",
       "              ('157', '0.4000'),\n",
       "              ('159', '0.0000'),\n",
       "              ('160', '0.4000'),\n",
       "              ('161', '1.0000'),\n",
       "              ('162', '0.2000'),\n",
       "              ('163', '0.8000'),\n",
       "              ('164', '0.8000'),\n",
       "              ('166', '0.4000'),\n",
       "              ('168', '0.0000'),\n",
       "              ('169', '0.0000'),\n",
       "              ('171', '0.0000'),\n",
       "              ('172', '0.2000'),\n",
       "              ('174', '0.6000'),\n",
       "              ('175', '0.0000'),\n",
       "              ('176', '0.0000'),\n",
       "              ('177', '0.0000'),\n",
       "              ('178', '0.2000'),\n",
       "              ('179', '0.0000'),\n",
       "              ('181', '0.0000'),\n",
       "              ('183', '1.0000'),\n",
       "              ('184', '0.6000'),\n",
       "              ('185', '0.0000'),\n",
       "              ('186', '0.0000'),\n",
       "              ('187', '0.2000'),\n",
       "              ('188', '0.6000'),\n",
       "              ('189', '0.8000'),\n",
       "              ('190', '0.2000'),\n",
       "              ('191', '0.0000'),\n",
       "              ('193', '1.0000'),\n",
       "              ('194', '0.0000'),\n",
       "              ('195', '0.0000'),\n",
       "              ('196', '0.2000'),\n",
       "              ('197', '0.4000'),\n",
       "              ('198', '0.2000'),\n",
       "              ('199', '0.0000'),\n",
       "              ('200', '0.6000'),\n",
       "              ('51', '0.8000'),\n",
       "              ('52', '0.8000'),\n",
       "              ('54', '0.0000'),\n",
       "              ('55', '0.2000'),\n",
       "              ('56', '1.0000'),\n",
       "              ('58', '0.4000'),\n",
       "              ('59', '0.0000'),\n",
       "              ('60', '0.0000'),\n",
       "              ('61', '0.4000'),\n",
       "              ('62', '0.0000'),\n",
       "              ('63', '0.0000'),\n",
       "              ('64', '0.2000'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0000'),\n",
       "              ('68', '0.0000'),\n",
       "              ('70', '1.0000'),\n",
       "              ('71', '0.4000'),\n",
       "              ('72', '0.0000'),\n",
       "              ('73', '0.0000'),\n",
       "              ('75', '0.0000'),\n",
       "              ('76', '0.0000'),\n",
       "              ('77', '0.8000'),\n",
       "              ('79', '0.0000'),\n",
       "              ('80', '0.0000'),\n",
       "              ('81', '0.6000'),\n",
       "              ('82', '0.6000'),\n",
       "              ('83', '0.0000'),\n",
       "              ('84', '0.0000'),\n",
       "              ('85', '0.8000'),\n",
       "              ('87', '0.0000'),\n",
       "              ('88', '0.0000'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0000'),\n",
       "              ('97', '0.4000'),\n",
       "              ('98', '0.4000'),\n",
       "              ('99', '0.0000'),\n",
       "              ('all', '0.2667')],\n",
       "             'map': [('100', '0.0140'),\n",
       "              ('101', '0.1951'),\n",
       "              ('102', '0.1370'),\n",
       "              ('104', '0.2887'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0564'),\n",
       "              ('107', '0.1676'),\n",
       "              ('108', '0.0095'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1289'),\n",
       "              ('112', '0.3986'),\n",
       "              ('113', '0.0213'),\n",
       "              ('115', '0.1105'),\n",
       "              ('116', '0.2656'),\n",
       "              ('117', '0.2742'),\n",
       "              ('118', '0.0558'),\n",
       "              ('119', '0.0428'),\n",
       "              ('121', '0.0662'),\n",
       "              ('122', '0.0486'),\n",
       "              ('124', '0.0843'),\n",
       "              ('125', '0.1601'),\n",
       "              ('126', '0.0352'),\n",
       "              ('127', '0.0025'),\n",
       "              ('128', '0.2798'),\n",
       "              ('129', '0.0554'),\n",
       "              ('130', '0.2611'),\n",
       "              ('131', '0.0236'),\n",
       "              ('132', '0.6432'),\n",
       "              ('133', '0.6817'),\n",
       "              ('134', '0.6134'),\n",
       "              ('136', '0.1049'),\n",
       "              ('137', '0.1481'),\n",
       "              ('138', '0.0335'),\n",
       "              ('139', '0.1138'),\n",
       "              ('140', '0.0453'),\n",
       "              ('141', '0.0218'),\n",
       "              ('142', '0.0077'),\n",
       "              ('145', '0.0549'),\n",
       "              ('146', '0.1958'),\n",
       "              ('147', '0.0293'),\n",
       "              ('148', '0.0243'),\n",
       "              ('149', '0.0496'),\n",
       "              ('150', '0.0026'),\n",
       "              ('152', '0.0186'),\n",
       "              ('153', '0.1579'),\n",
       "              ('154', '0.1955'),\n",
       "              ('156', '0.0848'),\n",
       "              ('157', '0.2210'),\n",
       "              ('159', '0.1370'),\n",
       "              ('160', '0.1134'),\n",
       "              ('161', '0.5646'),\n",
       "              ('162', '0.1042'),\n",
       "              ('163', '0.8258'),\n",
       "              ('164', '0.4893'),\n",
       "              ('166', '0.1541'),\n",
       "              ('168', '0.0830'),\n",
       "              ('169', '0.0669'),\n",
       "              ('171', '0.0183'),\n",
       "              ('172', '0.1973'),\n",
       "              ('174', '0.4725'),\n",
       "              ('175', '0.1959'),\n",
       "              ('176', '0.0056'),\n",
       "              ('177', '0.0840'),\n",
       "              ('178', '0.0308'),\n",
       "              ('179', '0.0182'),\n",
       "              ('181', '0.0290'),\n",
       "              ('183', '0.7429'),\n",
       "              ('184', '0.1665'),\n",
       "              ('185', '0.0929'),\n",
       "              ('186', '0.0269'),\n",
       "              ('187', '0.0853'),\n",
       "              ('188', '0.1918'),\n",
       "              ('189', '0.0501'),\n",
       "              ('190', '0.0430'),\n",
       "              ('191', '0.0003'),\n",
       "              ('193', '0.2999'),\n",
       "              ('194', '0.0172'),\n",
       "              ('195', '0.0417'),\n",
       "              ('196', '0.0382'),\n",
       "              ('197', '0.0380'),\n",
       "              ('198', '0.1190'),\n",
       "              ('199', '0.0005'),\n",
       "              ('200', '0.5105'),\n",
       "              ('51', '0.4032'),\n",
       "              ('52', '0.5463'),\n",
       "              ('54', '0.1148'),\n",
       "              ('55', '0.4010'),\n",
       "              ('56', '0.5089'),\n",
       "              ('58', '0.2966'),\n",
       "              ('59', '0.0353'),\n",
       "              ('60', '0.0030'),\n",
       "              ('61', '0.1247'),\n",
       "              ('62', '0.0243'),\n",
       "              ('63', '0.0429'),\n",
       "              ('64', '0.0446'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0021'),\n",
       "              ('68', '0.0118'),\n",
       "              ('70', '0.5456'),\n",
       "              ('71', '0.0549'),\n",
       "              ('72', '0.0120'),\n",
       "              ('73', '0.0001'),\n",
       "              ('75', '0.0582'),\n",
       "              ('76', '0.0021'),\n",
       "              ('77', '0.3130'),\n",
       "              ('79', '0.0006'),\n",
       "              ('80', '0.0008'),\n",
       "              ('81', '0.1188'),\n",
       "              ('82', '0.1637'),\n",
       "              ('83', '0.0213'),\n",
       "              ('84', '0.0108'),\n",
       "              ('85', '0.2344'),\n",
       "              ('87', '0.0011'),\n",
       "              ('88', '0.0045'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0395'),\n",
       "              ('97', '0.1133'),\n",
       "              ('98', '0.1559'),\n",
       "              ('99', '0.1857'),\n",
       "              ('all', '0.1440'),\n",
       "              ('100', '0.0224'),\n",
       "              ('101', '0.2055'),\n",
       "              ('102', '0.0869'),\n",
       "              ('104', '0.3027'),\n",
       "              ('105', '0.0000'),\n",
       "              ('106', '0.0611'),\n",
       "              ('107', '0.1315'),\n",
       "              ('108', '0.0100'),\n",
       "              ('109', '0.0000'),\n",
       "              ('110', '0.1078'),\n",
       "              ('112', '0.3259'),\n",
       "              ('113', '0.0215'),\n",
       "              ('115', '0.1151'),\n",
       "              ('116', '0.5316'),\n",
       "              ('117', '0.2941'),\n",
       "              ('118', '0.0593'),\n",
       "              ('119', '0.0443'),\n",
       "              ('121', '0.0691'),\n",
       "              ('122', '0.0525'),\n",
       "              ('124', '0.0844'),\n",
       "              ('125', '0.1664'),\n",
       "              ('126', '0.0325'),\n",
       "              ('127', '0.0004'),\n",
       "              ('128', '0.2988'),\n",
       "              ('129', '0.0556'),\n",
       "              ('130', '0.2655'),\n",
       "              ('131', '0.0458'),\n",
       "              ('132', '0.6335'),\n",
       "              ('133', '0.6495'),\n",
       "              ('134', '0.6134'),\n",
       "              ('136', '0.0964'),\n",
       "              ('137', '0.1253'),\n",
       "              ('138', '0.0313'),\n",
       "              ('139', '0.1124'),\n",
       "              ('140', '0.0517'),\n",
       "              ('141', '0.0241'),\n",
       "              ('142', '0.0082'),\n",
       "              ('145', '0.0457'),\n",
       "              ('146', '0.1863'),\n",
       "              ('147', '0.0250'),\n",
       "              ('148', '0.0199'),\n",
       "              ('149', '0.0435'),\n",
       "              ('150', '0.0030'),\n",
       "              ('152', '0.0194'),\n",
       "              ('153', '0.1848'),\n",
       "              ('154', '0.1791'),\n",
       "              ('156', '0.0831'),\n",
       "              ('157', '0.2361'),\n",
       "              ('159', '0.0586'),\n",
       "              ('160', '0.1095'),\n",
       "              ('161', '0.5455'),\n",
       "              ('162', '0.1090'),\n",
       "              ('163', '0.7730'),\n",
       "              ('164', '0.4204'),\n",
       "              ('166', '0.1472'),\n",
       "              ('168', '0.0970'),\n",
       "              ('169', '0.0721'),\n",
       "              ('171', '0.0188'),\n",
       "              ('172', '0.2020'),\n",
       "              ('174', '0.4689'),\n",
       "              ('175', '0.1786'),\n",
       "              ('176', '0.0056'),\n",
       "              ('177', '0.0862'),\n",
       "              ('178', '0.0376'),\n",
       "              ('179', '0.0170'),\n",
       "              ('181', '0.0284'),\n",
       "              ('183', '0.7114'),\n",
       "              ('184', '0.1582'),\n",
       "              ('185', '0.0946'),\n",
       "              ('186', '0.0255'),\n",
       "              ('187', '0.0979'),\n",
       "              ('188', '0.2102'),\n",
       "              ('189', '0.0650'),\n",
       "              ('190', '0.0507'),\n",
       "              ('191', '0.0005'),\n",
       "              ('193', '0.3206'),\n",
       "              ('194', '0.0165'),\n",
       "              ('195', '0.0341'),\n",
       "              ('196', '0.0388'),\n",
       "              ('197', '0.0412'),\n",
       "              ('198', '0.1193'),\n",
       "              ('199', '0.0005'),\n",
       "              ('200', '0.3998'),\n",
       "              ('51', '0.4339'),\n",
       "              ('52', '0.4976'),\n",
       "              ('54', '0.1650'),\n",
       "              ('55', '0.4063'),\n",
       "              ('56', '0.5198'),\n",
       "              ('58', '0.2821'),\n",
       "              ('59', '0.0300'),\n",
       "              ('60', '0.0024'),\n",
       "              ('61', '0.1036'),\n",
       "              ('62', '0.0188'),\n",
       "              ('63', '0.0385'),\n",
       "              ('64', '0.0451'),\n",
       "              ('65', '0.0000'),\n",
       "              ('66', '0.0000'),\n",
       "              ('67', '0.0024'),\n",
       "              ('68', '0.0094'),\n",
       "              ('70', '0.5689'),\n",
       "              ('71', '0.0608'),\n",
       "              ('72', '0.0070'),\n",
       "              ('73', '0.0001'),\n",
       "              ('75', '0.0557'),\n",
       "              ('76', '0.0031'),\n",
       "              ('77', '0.3235'),\n",
       "              ('79', '0.0004'),\n",
       "              ('80', '0.0006'),\n",
       "              ('81', '0.1197'),\n",
       "              ('82', '0.1537'),\n",
       "              ('83', '0.0203'),\n",
       "              ('84', '0.0121'),\n",
       "              ('85', '0.2477'),\n",
       "              ('87', '0.0010'),\n",
       "              ('88', '0.0042'),\n",
       "              ('91', '0.0000'),\n",
       "              ('96', '0.0464'),\n",
       "              ('97', '0.1596'),\n",
       "              ('98', '0.2012'),\n",
       "              ('99', '0.1681'),\n",
       "              ('all', '0.1436')]})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import subprocess\n",
    "from collections import defaultdict \n",
    "\n",
    "# Run evaluation bash script...\n",
    "subprocess.call([\"./assignment_evals.sh\"])\n",
    "\n",
    "\n",
    "# ... And retrieve again\n",
    "eval_data = defaultdict(list)\n",
    "\n",
    "for file in os.listdir(\"run\"):\n",
    "    if file[-4:] == \".txt\":\n",
    "        with open(\"run/\" + file) as file:\n",
    "            content = [line.strip().split() for line in file.readlines()]\n",
    "            for metric, query, score in content:\n",
    "                eval_data[metric].append((query,score))\n",
    "                \n",
    "# eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['south', 'african', 'sanctions']\n",
      "-inf\n",
      "         1592 function calls in 0.005 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.005    0.002 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2832(run_code)\n",
      "        2    0.000    0.000    0.004    0.002 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.004    0.004 <ipython-input-396-884e48b46823>:12(<module>)\n",
      "        1    0.001    0.001    0.004    0.004 <ipython-input-371-2cf675f70044>:174(PLM_score_old)\n",
      "        3    0.000    0.000    0.001    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/numpy/core/fromnumeric.py:1730(sum)\n",
      "        4    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        3    0.000    0.000    0.001    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/numpy/core/_methods.py:31(_sum)\n",
      "        1    0.000    0.000    0.001    0.001 <ipython-input-371-2cf675f70044>:198(<listcomp>)\n",
      "      709    0.000    0.000    0.001    0.000 <ipython-input-371-2cf675f70044>:322(triangle_kernel)\n",
      "        3    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/ipykernel/iostream.py:342(write)\n",
      "        4    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/ipykernel/iostream.py:180(schedule)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.array}\n",
      "      760    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method posix.urandom}\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/warnings.py:85(_showwarnmsg)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/warnings.py:20(_showwarnmsg_impl)\n",
      "        2    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/codeop.py:132(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'document' of 'pyndri.Index' objects}\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-371-2cf675f70044>:334(query_language_model)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/collections/__init__.py:519(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/collections/__init__.py:588(update)\n",
      "        3    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/ipykernel/iostream.py:297(_schedule_flush)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-371-2cf675f70044>:233(max_usefull_range)\n",
      "        4    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/threading.py:1104(is_alive)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2174(amax)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.zeros}\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/numpy/core/_methods.py:25(_amax)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/warnings.py:398(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/warnings.py:106(_formatwarnmsg)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/abc.py:178(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/warnings.py:35(_formatwarnmsg_impl)\n",
      "        4    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/threading.py:1062(_wait_for_tstate_lock)\n",
      "        3    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/ipykernel/iostream.py:284(_is_master_process)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-371-2cf675f70044>:336(<listcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/IPython/core/hooks.py:142(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/_weakrefset.py:70(__contains__)\n",
      "        4    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/ipykernel/iostream.py:87(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'index' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/linecache.py:15(getline)\n",
      "        2    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/IPython/utils/ipstruct.py:125(__getattr__)\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.locals}\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-396-884e48b46823>:14(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-371-2cf675f70044>:191(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        4    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/threading.py:506(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/linecache.py:37(getlines)\n",
      "        2    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/IPython/core/interactiveshell.py:1055(user_global_ns)\n",
      "        2    0.000    0.000    0.000    0.000 /Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/IPython/core/hooks.py:207(pre_run_code_hook)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/envs/ml1labs/lib/python3.6/site-packages/ipykernel_launcher.py:224: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "import cProfile, pstats\n",
    "from io import StringIO\n",
    "import functools\n",
    "\n",
    "print([id2token[x] for x in test_query if x > 0 ])\n",
    "\n",
    "PLM_triangle = functools.partial(PLM_score_old, kernel_func=triangle_kernel, mu=1000)\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "print(PLM_triangle(test_doc, test_query))\n",
    "\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document and query share terms\n",
    "test_query = tokenized_queries['52']\n",
    "test_doc = 11\n",
    "\n",
    "%time print(score_similarity(test_query, test_doc, weight_fn=tfidf, query_weight_fn=tfidf_query))\n",
    "%time print(score_similarity(test_query, test_doc, weight_fn=BM25, query_weight_fn=BM25_query))\n",
    "%time print(score_similarity(test_query, test_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ext_doc_id, doc_token_ids = index.document(1)\n",
    "\n",
    "# This contains the length of each document in the collection.\n",
    "document_lengths\n",
    "\n",
    "# This contains the number of unique terms per document in the collection.\n",
    "unique_terms_per_document\n",
    "\n",
    "# This contains the frequency in which the query term appears in all the documents\n",
    "collection_frequencies[35]\n",
    "\n",
    "# This contains the frequency in which the query term appears in a given document\n",
    "inverted_index[880]\n",
    "\n",
    "collection_frequencies[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import subprocess\n",
    "from collections import defaultdict \n",
    "\n",
    "# Run evaluation bash script...\n",
    "subprocess.call([\"./assignment_evals.sh\"])\n",
    "\n",
    "\n",
    "# ... And retrieve again\n",
    "eval_data = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for file in os.listdir(\"run\"):\n",
    "    if file[-4:] == \".txt\":\n",
    "        model_name = file[:-4]\n",
    "        with open(\"run/\" + file) as file:\n",
    "            content = [line.strip().split() for line in file.readlines()]\n",
    "            for metric, query, score in content:\n",
    "                eval_data[model_name][metric][query] =score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'103': '0.0364', '111': '0.1339', '114': '0.0488', '120': '0.0250', '123': '0.0286', '135': '0.0533', '143': '0.0258', '144': '0.0000', '151': '0.5750', '155': '0.0370', '158': '0.0000', '165': '0.0000', '167': '0.0833', '170': '0.0000', '173': '0.4384', '180': '0.0233', '182': '0.0962', '192': '0.0112', '53': '0.8259', '57': '1.0000', '69': '0.0000', '74': '0.0703', '78': '1.0000', '86': '0.0556', '89': '0.0000', '90': '0.0000', '92': '0.0217', '93': '0.0000', '94': '0.0429', '95': '0.0909', 'all': '0.1574'}\n"
     ]
    }
   ],
   "source": [
    "print(eval_data['jelinek_0.1_val_queries_len_C_is_num_words']['recall_1000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check which parameter values yield the highest values on the evaluation metrics for the language models\n",
    "\n",
    "We show the  metrics NDCG@10, MAP@1000, Precision@5 and Recall@1000 for 'all' queries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a lambda of 1 the Jellinek-Mercer had a NDCG@10 of 0.0504, a MAP@1000 of 0.0328 a Precission @5 of 0.0533 and a Recall@1000 of 0.1574\n",
      "\n",
      "With a lambda of 5 the Jellinek-Mercer had a NDCG@10 of 0.0504, a MAP@1000 of 0.0328 a Precission @5 of 0.0533 and a Recall@1000 of 0.1574\n",
      "\n",
      "With a lambda of 9 the Jellinek-Mercer had a NDCG@10 of 0.0504, a MAP@1000 of 0.0328 a Precission @5 of 0.0533 and a Recall@1000 of 0.1574\n",
      "\n",
      "With a mu of 500 the Dirichlet smoothing had a NDCG@10 of 0.4055, a MAP@1000 of 0.2399 a Precission @5 of 0.3933 and a Recall@1000 of 0.6269\n",
      "\n",
      "With a mu of 1000 the Dirichlet smoothing had a NDCG@10 of 0.4002, a MAP@1000 of 0.2409 a Precission @5 of 0.4000 and a Recall@1000 of 0.6405\n",
      "\n",
      "With a mu of 1500 the Dirichlet smoothing had a NDCG@10 of 0.4026, a MAP@1000 of 0.2405 a Precission @5 of 0.3933 and a Recall@1000 of 0.6301\n",
      "\n",
      "With a delta of 1 the Absolute discounting had a NDCG@10 of 0.3662, a MAP@1000 of 0.2184 a Precission @5 of 0.3800 and a Recall@1000 of 0.6156\n",
      "\n",
      "With a delta of 5 the Absolute discounting had a NDCG@10 of 0.3889, a MAP@1000 of 0.2275 a Precission @5 of 0.3867 and a Recall@1000 of 0.6599\n",
      "\n",
      "With a delta of 9 the Absolute discounting had a NDCG@10 of 0.3754, a MAP@1000 of 0.2330 a Precission @5 of 0.3867 and a Recall@1000 of 0.6597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [['jelinek_0.1_val_queries_len_C_is_num_words','lambda', '1', 'Jellinek-Mercer'], \n",
    "               ['jelinek_0.5_val_queries_len_C_is_num_words','lambda','5','Jellinek-Mercer' ], \n",
    "               ['jelinek_0.9_val_queries_len_C_is_num_words','lambda','9', 'Jellinek-Mercer'],\n",
    "               ['dirichlet_500_val_queries_len_C_is_num_words','mu', '500','Dirichlet smoothing'],\n",
    "               ['dirichlet_1000_val_queries_len_C_is_num_words','mu', '1000','Dirichlet smoothing'],\n",
    "               ['dirichlet_1500_val_queries_len_C_is_num_words','mu', '1500', 'Dirichlet smoothing'],\n",
    "               ['absolute_0.1_val_queries_len_C_is_num_words','delta','1', 'Absolute discounting'],\n",
    "               ['absolute_0.5_val_queries_len_C_is_num_words','delta','5', 'Absolute discounting'],\n",
    "               ['absolute_0.9_val_queries_len_C_is_num_words','delta','9', 'Absolute discounting']]\n",
    "\n",
    "for model in model_names:\n",
    "    print('With a %s of %s the %s had a NDCG@10 of %s, a MAP@1000 of %s a Precission @5 of %s and a Recall@1000 of %s'%(model[1], model[2], model[3],\n",
    "                                                              eval_data[model[0]]['ndcg_cut_10']['all'],eval_data[model[0]]['map']['all'],\n",
    "                                                            eval_data[model[0]]['P_5']['all'],eval_data[model[0]]['recall_1000']['all']))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that for all models the metrics don't differ much when adjusting the parameters we looked at run files manually. We found a clear difference in the scores when adjusting the parameters, but the rankings were almost identical per model. \n",
    "\n",
    "For two of the three models there is a slight difference in NDCG@10. We consider this the most important metric because it takes into account the fact that ranking the first few documents right is more important than ranking later documents right. \n",
    "\n",
    "For Dirichlet smoothing we thus choose a mu of 500 and for Absolute discounting we choose a delta of 5.\n",
    "\n",
    "For Jellinek-Mercer we choose a lambda of 1. Note that all metrics for the three values of lambda are exactly the same and it seems to not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmYHVWd//H3hySEEAMRCDMmBAIY\nkCDI0oA+PxRUMCzKJsjqgMMyOKCOMhEZHQYDiiMDio9xICCiImQQGIwOGlzAGWRtZImJBkIIZmEJ\nISEsAZLw/f1xTofKze1OVejqvk0+r+fpp2+dOlX1rbpV9a06tVxFBGZmZlWs19sBmJlZ3+PkYWZm\nlTl5mJlZZU4eZmZWmZOHmZlV5uRhZmaVveWSh6SrJV3QzeM8SdId3TnOCtMeJSkk9c/dv5R0Ym/E\nUpWk90ua0dtxrGsk/T9Jj0p6UdJhNU1jtqT96hh3VY3bZ57vbfLnQZJ+Lul5ST/NZRdIelbSU70V\ncxVll3XjvqJufTZ5SLpd0iJJA3s7lqK6E01EHBgRP6xr/G9GXnHf2dEdEf8XEdv30LRvl/SKpJGF\nsv0kzS50z5a0VNILkhZLulPS6ZLWaxjXnpJuyXWek3SvpE8V+g+RdEke30uS/irpBkl7NoxHko6W\n9FtJz0h6StKtko5oEv/5kqZKWi7pvCb9j5P0RJ7ezZI26WJxjAe+GxFvi4ibyyy/rtRxQFanPN+z\ncueRwN8Am0bEUXn9OAsYExF/29Ox5fX0lLfCtPpk8pA0Cng/EMAhvRqMtZKXgH9dQ52PRcQQYCvg\nG8DZwPc7ekp6H/A74PfAO4FNgU8DB+b+A3P/nYCPAhsBOwCTgIMK4+kHXAucCnwd2BYYCZwHnCbp\nCkkqxDUT+CLwP40BS9oRuBz4JGlH+DLwvS7mcStg2hqWQ1N1HLX21JFwJ7YCHomI5YXuhRHxTNUR\n5YOBPrnPrEVE9Lk/4FzgD8AlwC8a+l0NXAb8GniBtBPYKvcT8C3gGeB54GHg3bnfxsCPgAXAE8BX\ngPVyv5OAO/LnUaSk1b8wzduBU0g7kVeAFcCLwOLcfyDwH8BfgadzfIM6mbd+ue6zwCzgjOL0OqaV\nP78zz9/zuf5/FcazY14Gz+Vp/kshlm8D8/Pft4GBjfNZGE8A7yws2wmkHdwLwD3Atrnf/+a6L+V5\nPxrYF5hbGNds4J/zcn8e+C9gg0L/LwJP5rhOKU67xDpxO/BvOa6OePcDZjdMf7+G4fYEXi+sB3cA\nE7qYzik5xsEl1tGrATXppzzvf9ek3zXAeQ1lXweuLXRvC7wGDGky/GN5fpbm72EgMByYnNeFmcCp\nhfrnATfk6S7pWLcK/U8DluXpvQj8fE3fZcf3TkrMTwE/zuUfBR4EFgN3AjsXpjMcuJG0/T0OfLaL\nZbtpnp8lwL3A+RTW2471BvhqjntZjv0f8nJ5PXdfneu/N8ezGHgI2LdhvfoaaX+zNI93Y9IBx5PA\nPOACoF9xGyJtw4vyvByY+32NtG94JU//u53M3ydJ+6CFwJcprLekA/4v5e95IXA9sEnjvqmzaQGX\nAnPysrsfeH/DttCe+z0NXNLlOl5lp90qf6QN4B+B3fOK8TeFfleTdiAfIG04l/LGjn9sXmBDSRvw\nDsA7cr8fAT8DhuQv4RHg5OIK0fgFNaxgpzTWLfT/Nmll3ySP/+fAhZ3M2+nAX0hHqZsAt9F58rgu\nr1zrARsAe+fyIXnFPiuXDwH2yv3GA3cDmwPDSBvN+V3E3pg8nssrWX/gJ8CkZnWLO5FC92zSxj48\nz9ufgdNzvwNIO5odgQ2BHzdM+zjg4S7WidtJO/ZLgGty2RqTRy7/K+nsYkPSBvfBLqYzibzT6aLO\nYNLGPxgYAFyV5+1XwA9I6+YIoL3JsM2Sx8+AsxvKXgR272T6q8wn6QDje3ld2IW0g/5w7nceaRs6\nLK9Hqx3U5O/9gibT6Oy73BdYDvw7aRscBOxGOmjbi3SAdGIex8A83ftJCXd9YBvSgdPYLr6D6/Py\nfTdpB75a8ijM3zVdrJMjSDvhg3Ic++fuYYX16q+k9bJ//j5vJp0JDiZtR/cC/1DYhpaRzjj75fVq\nPvkggsL228m8jcnfbcf+65K8LDuSxz+Rtt8tcv/Lgeua7ZuaTQs4gZR8+5P2D0/xRtK/C/hk/vw2\n4L1dred97hRM0t6kU8/rI+J+UgY+rqHa/0TE/0bEq6Sd6/tyW+cy0o70XaQv888R8WRuYjgaOCci\nXoiI2cDFpCOANxuvSCvS5yPiuYh4gXQkeUwng3wC+HZEzImI54ALuxj9MtKyGB4Rr0REx7WWjwJP\nRcTFufyFiLgn9zseGB8Rz0TEAtLRWZX5vCki7o3UDPAT0s6oiu9ExPw8bz8vDP8J4AcRMS0iXs5x\nrRQR10bEziXGfyHwsdzUU9Z80g7w7aQdyJNd1N2MtMEBIGmXfG1kSeHmgPcBt0fES6SEtgWwHemA\nZyzpjHYeaSMu422ko/ui50nrcpfyer83Kfm8EhEPAley6nd+V0TcHBGvR8TSkjFB598lpKP7f4uI\nV/M4TwUuj4h7ImJFpOt2r5KO+vcg7azHR8Rrka5XXEGTbSRvqx8Hzo2IlyLiT8CbuQZ4AnBLRNyS\n5//XpKPvgwp1rs7r5XLSenIg8E95+s+QWjOKsT4REVdExIoc2ztIzY1lHElqTenYf/0raVl2+Afg\nyxExN/c/DziybNNgRFwTEQsjYnlEXExKQB3XJZcB75S0WUS8GBF3dzWuPpc8SEcst0bEs7n72lxW\nNKfjQ0S8SDpaHh4RvwO+S2p6eVrSREkbkXYI65OOFjs8QToqebOGkY5o7887mcWkI9BhndQfXoy/\nIaZGXySdQd0raZqkv8/lI0lJtbPxN87n8K5nYRXFO1ReJu3Yquhs+Mb5Ln4uLSfE75LOsMoaQVpH\nFpE21Hd0UXdhsX9EPBgRQ4EjSBsipKPRefnzTsDNEbEk7xTvgHTRndTEV8aLpGsrRRuRzrDXZDjQ\ncdDSoXHdXqtlTdfrwoKIeKXQvRVwVsc2kLeDkTm+rYDhDf3+heY73GGko+ay28iabAUc1TDtvVl1\nHZjTUH8A8GSh/uWk77zDyuWSD4Sg/HayynaQD0AWNkz/vwvT/jPpbLlUcpJ0lqQ/57vPFpOa4DbL\nvU8mHeT8RdJ9kj7a1bj6VPKQNIh0hLpPvnPlKeDzwHskvadQtXjHzdtIRwvzASLiOxGxO+k0dDtg\nHOl6QcdRfIcteWMHUNSxwW9YKCvetREN9Z8ltZXuGBFD89/GEdHZyvRkMf4cR1MR8VREnBoRw0lH\nJN/LdzvNIbWLNzOf1edzfv78UnG+JPXk3ShPko7QO4zsrGIJFwEfJDVrdknSHqQd6R15Q7+LdGTb\nmd8CH5E0uIs6z/LGzmcqcFi+Q2tr0o7p7aRmpKvWFF82DVi5fivdhjqQ1LS6JvOBTXKy6tC4bjeu\ns43W1L/MMHOArxW2gaERsWFEXJf7Pd7Qb0hEHLT6aFlAasYptY2UMId0TaY47cER8Y1O5mUO6Yxp\ns0L9jSKi7JnumpblKtu/pA1Z9Qx1DukaSjHeDfKZbJfTkvR+0nWoTwBvzwc9z5MOQImIRyPiWFIi\n/Hfghq7W8z6VPEjtsitI7YK75L8dgP8D/q5Q7yBJe0tan3Qx7Z6ImCNpD0l7SRpA2lG+AqzIp5fX\nA1/LG/lWwBdI7c+ryEe284ATJPXLR/vFHfXTwBZ52kTE66RT8G9J2hxA0ghJYzuZx+uBz0raQtLb\nSRfHmpJ0lKSOHe4i0sqyAvgF8LeS/knSwDxPe+V61wFfkTRM0makduaO+XwI2DE3xWxAOiWu4mlS\ne/XauB74lKQd8gZz7lqOh4hYTGp2/GJndSRtlI+sJpHaxKfmXl8ETpI0TtKmue57JE3K/X9E2sD/\nW9K78zqwAdBWGP1dwAfzwc73SRePZ+bPt+Z5u4t0LawjngF5POsB/SVtkJtoIDUPfkzpuZnBpLOq\nmxrOJjpbFnNI17UuzOPcmXSE+ZM1DVvwZr7XDlcAp+ftT5IGSzo4J7V7gSWSzlZ6LqNfXrZ7NJmf\nFcBNwHmSNpQ0htVbHqq4hrRsx3Z8l5L2LWxXjdN/kvQdXpzXofUkbStpn5LTW9OyvAH4aGH/NZ5V\n99OXkfZTWwHk7fjQktMaQkq8C0jr2LkUzmglnSBpWN5nLc7FKzoLtK8ljxNJ7eJ/zUfdT0XEU6Rm\niuML7X7Xku68eY509Hl8Lt+ItBIv4o27Gf4j9/sMKaF0NC1cS+dHhqeSzlgWks5g7iz0+x3pSPEp\nSR1Na2eTdh53S1oC/IY32hkbXQFMIe3I/0jaUDqzB3CPpBdJF+Q/FxGP553K/sDHSKfQj5KOxCHd\nGdJOuktmap7GBQAR8QhpZf1NHqbq8yrnAT/Mp9SfqDJgRPwS+A7pBoGZpJ0rpKM8JB0vqcrtp5fS\nfMX/uaQXSEdwXyZdkFz5DEdE3Al8KP/NkvQcMBG4Jfd/hbQsp5PuOlsCzCB9F5/IdV4grT/fzm34\nfx8RfxMRH4qIk4A9IuJ7eSPtcAXpDPXYHNdS8nWJiJhGupHiJ6SLzkNI10/KOpZ0MXU+8N+kaxG/\nrjD894Ex+Xtdq+dGIqKdtN18l7T9zSRdXO5ICB8jHQw+Tjpzu5LUpNLMmaRmoKdIF/N/sDYx5WnP\nAQ4lNZMtIK0X4+h63/h3pGbu6XlebqDrps6iS0nXKBZJ+k6TeKaR7rC8lnSQsoh08FEcfjJwa16P\n7ybdhFBmWlOAX5LOWJ8gHTwXm+QOAKbl/cmlwDENTY+r6LgDwKylSNoB+BPpNuLla6rfavKBzE9J\nO6ELSLeobggcRTqY2K3MmYNZq+prZx72FibpcEnr5+a6fyc9U9DnEgdAjvvjpNtsv0U6Sv4L6Yzm\ncCcO6+t85mEtQ9KvSLe5riA9m/CPuY3ZzFqMk4eZmVXmZiszM6usN19Y1q0222yzGDVqVG+HYWbW\np9x///3PRkRnDy136i2TPEaNGkV7e3tvh2Fm1qdIWqsn9N1sZWZmlTl5mJlZZU4eZmZWmZOHmZlV\n5uRhZmaVOXmYmVllTh5mZlaZk4eZmVXm5GFmZpU5eZiZWWVOHmZmVpmTh5mZVebkYWZmlTl5mJlZ\nZU4eZmZWmZOHmZlVVmvykHSApBmSZkr6Uhf1jpQUktoKZefk4WZIGltnnGZmVk1tvyQoqR8wAdgf\nmAvcJ2lyRExvqDcE+CxwT6FsDHAMsCMwHPiNpO0iYkVd8ZqZWXl1nnnsCcyMiFkR8RowCTi0Sb3z\ngW8CrxTKDgUmRcSrEfE4MDOPz8zMWkCdyWMEMKfQPTeXrSRpV2BkRPyi6rB5+NMktUtqX7BgQfdE\nbWZma1Rn8lCTsljZU1oP+BZwVtVhVxZETIyItohoGzZs2FoHamZm1dR2zYN0tjCy0L0FML/QPQR4\nN3C7JIC/BSZLOqTEsGZm1ovqPPO4DxgtaWtJ65MugE/u6BkRz0fEZhExKiJGAXcDh0REe653jKSB\nkrYGRgP31hirmZlVUNuZR0Qsl3QmMAXoB1wVEdMkjQfaI2JyF8NOk3Q9MB1YDpzhO63MzFqHIla7\nlNAntbW1RXt7e2+HYWbWp0i6PyLa1lxzVX7C3MzMKnPyMDOzypw8zMysMicPMzOrzMnDzMwqc/Iw\nM7PKnDzMzKwyJw8zM6vMycPMzCpz8jAzs8qcPMzMrDInDzMzq8zJw8zMKnPyMDOzypw8zMysMicP\nMzOrzMnDzMwqc/IwM7PKnDzMzKwyJw8zM6vMycPMzCpz8jAzs8qcPMzMrDInDzMzq8zJw8zMKnPy\nMDOzypw8zMysMicPMzOrzMnDzMwqc/IwM7PKak0ekg6QNEPSTElfatL/dElTJT0o6Q5JY3L5+pJ+\nkPs9JGnfOuM0M7NqaksekvoBE4ADgTHAsR3JoeDaiNgpInYBvglckstPBYiInYD9gYsl+SzJzKxF\n1LlD3hOYGRGzIuI1YBJwaLFCRCwpdA4GIn8eA/w213kGWAy01RirmZlVUGfyGAHMKXTPzWWrkHSG\npMdIZx6fzcUPAYdK6i9pa2B3YGSTYU+T1C6pfcGCBd0+A2Zm1lz/GsetJmWxWkHEBGCCpOOArwAn\nAlcBOwDtwBPAncDyJsNOBCYCtLW1rTZus9528wPzuGjKDOYvXsrwoYMYN3Z7Dtt1tWMosz6nzuQx\nl1XPFrYA5ndRfxLwnwARsRz4fEcPSXcCj9YQo1ltbn5gHufcNJWly1YAMG/xUs65aSqAE4j1eXU2\nW90HjJa0taT1gWOAycUKkkYXOg8mJwhJG0oanD/vDyyPiOk1xmrW7S6aMmNl4uiwdNkKLpoyo5ci\nMus+tZ15RMRySWcCU4B+wFURMU3SeKA9IiYDZ0raD1gGLCI1WQFsDkyR9DowD/hkXXGa1WX+4qWV\nys36kjqbrYiIW4BbGsrOLXz+XCfDzQa2rzM2s7oNHzqIeU0SxfChg3ohGrPu5WcnzGoybuz2DBrQ\nb5WyQQP6MW6sj4us76v1zMNsXdZxUdx3W9lbkZOHWY0O23WEk4W9JbnZyszMKnPyMDOzypw8zMys\nMicPMzOrzMnDzMwqc/IwM7PKnDzMzKwyJw8zM6vMycPMzCpz8jAzs8qcPMzMrDInDzMzq8zJw8zM\nKnPyMDOzypw8zMysMicPMzOrzMnDzMwqW+MvCUrqD5wMHA4MBwKYD/wM+H5ELKs1QjMzazllfob2\nx8Bi4Dxgbi7bAjgRuAY4upbIzMysZZVJHrtFxPYNZXOBuyU9UkNMZmbW4spc81gk6ShJK+tKWk/S\n0cCi+kIzM7NWVSZ5HAMcCTwt6ZF8tvEUcETuZ2Zm65g1NltFxGzydQ1JmwKKiGdrjsvMzFpYpVt1\nI2JhMXFI2r/7QzIzs1b3Zp/z+H63RGFmZn1Kmec8JnfWC9i0e8MxM7O+oMytuu8HTgBebCgXsGe3\nR2RmZi2vTPK4G3g5In7f2EPSjK4GlHQAcCnQD7gyIr7R0P904AxgBSk5nRYR0yUNAK4Edssx/igi\nLiwRq5mZ9YAyd1sd2EW/D3TWT1I/YAKwP+mhwvskTY6I6YVq10bEZbn+IcAlwAHAUcDAiNhJ0obA\ndEnX5Tu/zMysl1W+YC5p0+IDg13YE5gZEbMi4jVgEnBosUJELCl0Dia9N4v8f3B+r9Yg4DWgWNfM\nzHpRmWYrJL0dOB/YCXgS2ETSXOAzEfFSJ4ONAOYUuucCezUZ9xnAF4D1gQ/l4htIieZJYEPg8xHx\nXJNhTwNOA9hyyy3LzIqZmXWDNZ5BSBoK3ALcGBH7RMQxEfER0gsTvyFpb0lvazZok7JYrSBiQkRs\nC5wNfCUX70m6DjIc2Bo4S9I2TYadGBFtEdE2bNiwNc2KmZl1kzLNT/8K/EdE3Cbpx5IelXQXMJF0\ndrEe8C9NhpsLjCx0b0F6lXtnJgGH5c/HAb+KiGUR8QzwB6CtRKxmZtYDyiSPfSLixvz5VeDYiHgf\n6ZUlC4E7gH2aDHcfMFrS1pLWJ70Ha5VnRiSNLnQeDDyaP/8V+JCSwcB7gb+UnCczM6tZmWseAyUp\nIgLYFXgol/+J9Lr21/MdUauIiOWSzgSmkG7VvSoipkkaD7RHxGTgTEn7ActIb+g9MQ8+AfhBnoaA\nH0TEw2s/m2Zm1p3KJI97gQ8DvwH+E7g1N1u9D7hc0h7AtGYDRsQtpOslxbJzC58/18lwL5Ju1zUz\nsxZUJnl8Dbhe0sERcaWkm4FtSM9kDARu5I0zBjMzWweUeUhwVr6ddrKkW0lPnK8APkq6wH1GRHT5\npLmZmb21lHrOIyLukfQ+UvPVe0jXIe4ExkfE8hrjMzOzFlQqeQBExOvAr/OfmZmtw8o8JHiypHGF\n7rmSlkh6QdKn6w3PzMxaUZnnPE4Hrip0L4iIjYBhwLG1RGVmZi2tTPJYLyIWFrp/ChARr5BeWmhm\nZuuYMslj42JHRHwdIL9Z178kaGa2DiqTPG6VdEGT8vHArd0cj5mZ9QFl7rYaB1wpaSZvvJrkPUA7\ncEpdgZmZWesq85DgS8Cx+ZXoO+bi6RHxWK2RmZnZam5+YB4XTZnB/MVLGT50EOPGbs9hu47o8TjW\nmDwkjQWGRMQNwKxC+fHAMxHh5z7MzHrAzQ/M45ybprJ02QoA5i1eyjk3TQXo8QRS5prHV4HfNyn/\nLem6h5mZ9YCLpsxYmTg6LF22goum9Pwbosokjw0jYkFjYUQ8RfrdcTMz6wHzFy+tVF6nMsljA0mr\nNW9JGoCf8zAz6zHDhzbf5XZWXqcyyeMm4Ir8i34A5M+X5X5mZtYDxo3dnkED+q1SNmhAP8aN3b7H\nYymTPL4CPA08Iel+SX8EZgMLcj8zM+sBh+06gguP2IkRQwchYMTQQVx4xE69creV0q/LlqgoDQLe\nmTtnRkTPN7J1oa2tLdrb23s7DDOzPkXS/RHRVnW4Uq9kl7QpcBzwrlz0Z0nXNbzzyszM1hFlXsm+\nA/AnYHfgEeBRYA9gqqR3dTWsmZm9NZU58zgf+FxEXF8slPRx0u+bf7yOwMzMrHWVuWC+U2PiAIiI\nG4F3d39IZmbW6sokj5fWsp+Zmb1FlWm22lzSF5qUi/RrgmZmto4pkzyuAIZ00u/KbozFzMz6iDKv\nZP9qTwRiZmZ9R5lXsp/bRe+IiPO7MR4zM+sDyjRbNbsoPhg4mfQb5k4eZmbrmDLNVhd3fJY0BPgc\n8ClgEnBxZ8OZmdlbV9nXk2wCfAE4HvghsFtELKozMDMza11lrnlcBBwBTCQ9MPhi7VGZmVlLK/OQ\n4FnAcNLr1+dLWpL/XpC0pKsBJR0gaYakmZK+1KT/6ZKmSnpQ0h2SxuTy43NZx9/rknZZmxk0M7Pu\nV/qV7JVHLPUjvUhxf2AucB9wbERML9TZKCKW5M+HAP8YEQc0jGcn4GcRsU1X0/Mr2c3MqlvbV7KX\nOfNYW3uSfvdjVkS8RrrAfmixQkfiyAYDzTLZscB1tUVpZmaVlbpgvpZGAHMK3XOBvRorSTqDdDF+\nfeBDTcZzNA1JpzDsacBpAFtuueWbDNfMzMqq88xDTcpWO7OIiAkRsS1wNg0/aytpL+DliPhTswlE\nxMSIaIuItmHD/JotM7OeUmfymAuMLHRvAczvov4k4LCGsmNwk5WZWcupM3ncB4yWtLWk9UmJYHKx\ngqTRhc6DSb9S2NFvPeAoUlIxM7MWUts1j4hYLulMYArQD7gqIqZJGg+0R8Rk4ExJ+wHLgEXAiYVR\nfACYGxGz6orRzMzWTm236vY036prZlZdK96qa2Zmb1FOHmZmVpmTh5mZVebkYWZmlTl5mJlZZU4e\nZmZWWZ3vtupzbn5gHhdNmcH8xUsZPnQQ48Zuz2G7jujtsMzMWo6TR3bzA/M456apLF22AoB5i5dy\nzk1TAZxAzMwauNkqu2jKjJWJo8PSZSu4aMqMXorIzKx1OXlk8xcvrVRuZrYuc/LIhg8dVKnczGxd\n5uSRjRu7PYMG9FulbNCAfowbu30vRWRm1rp8wTzruCjuu63MzNbMyaPgsF1HOFmYmZXgZiszM6vM\nycPMzCpz8jAzs8qcPMzMrDInDzMzq8zJw8zMKnPyMDOzypw8zMysMicPMzOrzMnDzMwqc/IwM7PK\nnDzMzKwyJw8zM6vMycPMzCpz8jAzs8qcPMzMrLJak4ekAyTNkDRT0pea9D9d0lRJD0q6Q9KYQr+d\nJd0laVqus0GdsZqZWXm1JQ9J/YAJwIHAGODYYnLIro2InSJiF+CbwCV52P7ANcDpEbEjsC+wrK5Y\nzcysmjrPPPYEZkbErIh4DZgEHFqsEBFLCp2DgcifPwI8HBEP5XoLI2JFjbGamVkFdSaPEcCcQvfc\nXLYKSWdIeox05vHZXLwdEJKmSPqjpC82m4Ck0yS1S2pfsGBBN4dvZmadqTN5qElZrFYQMSEitgXO\nBr6Si/sDewPH5/+HS/pwk2EnRkRbRLQNGzas+yI3M7Mu1Zk85gIjC91bAPO7qD8JOKww7O8j4tmI\neBm4BditlijNzKyyOpPHfcBoSVtLWh84BphcrCBpdKHzYODR/HkKsLOkDfPF832A6TXGamZmFfSv\na8QRsVzSmaRE0A+4KiKmSRoPtEfEZOBMSfuR7qRaBJyYh10k6RJSAgrgloj4n7piNTOzahSx2mWI\nPqmtrS3a29t7Owwzsz5F0v0R0VZ1OD9hbmZmlTl5mJlZZU4eZmZWmZOHmZlV5uRhZmaVOXmYmVll\nTh5mZlaZk4eZmVXm5GFmZpU5eZiZWWVOHmZmVpmTh5mZVebkYWZmlTl5mJlZZU4eZmZWmZOHmZlV\n5uRhZmaVOXmYmVllTh5mZlaZk4eZmVXm5GFmZpU5eZiZWWVOHmZmVpmTh5mZVebkYWZmlTl5mJlZ\nZU4eZmZWmZOHmZlV5uRhZmaVOXmYmVlltSYPSQdImiFppqQvNel/uqSpkh6UdIekMbl8lKSlufxB\nSZfVGaeZmVXTv64RS+oHTAD2B+YC90maHBHTC9WujYjLcv1DgEuAA3K/xyJil7riMzOztVfnmcee\nwMyImBURrwGTgEOLFSJiSaFzMBA1xmNmZt2kzuQxAphT6J6by1Yh6QxJjwHfBD5b6LW1pAck/V7S\n+5tNQNJpktoltS9YsKA7Yzczsy7U1mwFqEnZamcWETEBmCDpOOArwInAk8CWEbFQ0u7AzZJ2bDhT\nISImAhMBJC2Q9EQ3xb4Z8Gw3jau7OKbyWjEux1SOYyqvu+Laam0GqjN5zAVGFrq3AOZ3UX8S8J8A\nEfEq8Gr+fH8+M9kOaO9s4IgY9mYD7iCpPSLaumt83cExldeKcTmmchxTeb0dV53NVvcBoyVtLWl9\n4BhgcrGCpNGFzoOBR3P5sHzBHUnbAKOBWTXGamZmFdR25hERyyWdCUwB+gFXRcQ0SeOB9oiYDJwp\naT9gGbCI1GQF8AFgvKTlwArg9Ih4rq5YzcysmjqbrYiIW4BbGsrOLXz+XCfD3QjcWGdsazCxF6fd\nGcdUXivG5ZjKcUzl9WpcivDdsWZmVo1fT2JmZpU5eZiZWWXrdPIo8e6tD0j6o6Tlko5skZi+IGm6\npIcl/VbSWt2j3c0xNX1HWW+DffgTAAAFGElEQVTGVKh3pKSQVPstjSWW00n5eaSOd7adUndMZeLK\ndT6R16tpkq7t7ZgkfauwnB6RtLgFYtpS0m354eWHJR3UAjFtlfcDD0u6XdIWdce0UkSsk3+kO8Ae\nA7YB1gceAsY01BkF7Az8CDiyRWL6ILBh/vxp4L9aIKaNCp8PAX7V2zHlekOA/wXuBtp6OybgJOC7\nda9HaxHXaOAB4O25e/Pejqmh/mdId2v29nKaCHw6fx4DzG6BmH4KnJg/fwj4cU+tW+vymUeZd2/N\njoiHgddbKKbbIuLl3Hk36eHL3o6pp99RtsaYsvNJr715peZ4qsTU08rEdSowISIWAUTEMy0QU9Gx\nwHUtEFMAG+XPG9P1Q889FdMY4Lf5821N+tdmXU4epd691cOqxnQy8MtaI3rz7yjrlZgk7QqMjIhf\n1BxL6Ziyj+cmhhskjWzSvzfi2g7YTtIfJN0t6QDqVXo9z82yWwO/a4GYzgNOkDSX9AjCZ1ogpoeA\nj+fPhwNDJG1ac1zAup08Sr17q4eVjknSCUAbcFGtEVV4R1lEbAucTXpHWa/FJGk94FvAWTXHUVRm\nOf0cGBUROwO/AX5Ye1Tl4upParral3SUf6Wkob0cU4djgBsiYkWN8UC5mI4Fro6ILYCDgB/nda03\nY/pnYB9JDwD7APOA5TXGtNK6nDyqvnurJ5SKKT+V/2XgkEjvAev1mAomAYfVGtGaYxoCvBu4XdJs\n4L3A5Jovmq9xOUXEwsL3dQWwe43xlI4r1/lZRCyLiMeBGaRk0psxdTiG+pusoFxMJwPXA0TEXcAG\npJcT9lpMETE/Io6IiF1J+wQi4vkaY1pl4uvkH+loaxbplLjjYtSOndS9mp65YL7GmIBdSRfRRrfK\ncirGAnyM9PqZlvjucv3bqf+CeZnl9I7C58OBu1vk+zsA+GH+vBmpqWTT3v7+gO2B2eSHmVtgOf0S\nOCl/3oG0I68ttpIxbQaslz9/DRhf97JaOe2emlAr/pFOPR/JO+Mv57LxpCN6gD1I2f8lYCEwrQVi\n+g3wNPBg/pvcAjFdCkzL8dzW1Y68p2JqqFt78ii5nC7My+mhvJzeVXdMJeMS6Vc8pwNTgWN6O6bc\nfR7wjZ5YRiWX0xjgD/n7exD4SAvEdCTphbKPAFcCA3tqefn1JGZmVtm6fM3DzMzWkpOHmZlV5uRh\nZmaVOXmYmVllTh5mZlaZk4dZRZJW5Le9TpP0UH7TcZfbkqRRkv6UP+/SE29kNatTrT9Da/YWtTQi\ndgGQtDlwLelFef9WcvhdSK+WuWVNFc1alZ/zMKtI0osR8bZC9zbAfeSnfYFvkN4TNZD0ttrLJY0C\nfgHsBswEBpHeQ3Qh8Djw7Vy2FPhURMzoodkxWys+8zB7kyJiVm622pz0SuznI2IPSQOBP0i6lfxC\nu4h4TdK5pCfezwSQtBHwgYhYnt9b9nXeeFOqWUty8jDrHh1vQP0IsHPhlyc3Jr1k8JEuht0Y+KGk\n0aQkM6C2KM26iZOH2ZuUm61WAM+QkshnImJKQ51RXYzifOC2iDg817u9jjjNupPvtjJ7EyQNAy4j\n/bxsAFOAT0sakPtvJ2lww2AvkF4b32Fj0vUPSD9Va9bynDzMqhvUcasu6S3HtwJfzf2uJL2d9o/5\n1tzLWf0M/zZgTB7H0aRfX7xQ0h9Iv1tt1vJ8t5WZmVXmMw8zM6vMycPMzCpz8jAzs8qcPMzMrDIn\nDzMzq8zJw8zMKnPyMDOzyv4/Lbod2Brdv1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc910f8cf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "absdisc_ndcg = [0.3662, 0.3889, 0.3754]\n",
    "absdisc_delta = [1, 5, 9]\n",
    "plt.scatter(absdisc_lamb, absdisc_ndcg)\n",
    "plt.xlabel('Delta')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('Absolute discounting: NDCG@10 for three different deltas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xm8HFWd/vHPQxYgLLIkOBCW4IgI\nrsgVmXFUHIdtREBFAZEhjgzqyA8VxUEHRwSXUVzGBRdAdlk0KgZGiYDC6CjIDYKACEREE9ZACASG\nLfD8/jjnhqa5N7fvrdt0Lnner1e/uuvUqapvVVfXt+pUdZVsExERMVqr9DqAiIgY35JIIiKikSSS\niIhoJIkkIiIaSSKJiIhGkkgiIqKRJJJhSLpZ0j/Uz0dKOr1+3lTS/ZImjME0LpZ0YNPxxPgk6T2S\n7qjr0/pdGP8MSZY0cazHPRqSTpb0yfr5VZKub+m3paTfSloi6RBJq0s6V9K9kr7Xu6gHJ2mmpF92\nadwj2i60bquebittImm68bb9F9tr2n5sLOMaTl1xLemLbeV71vKTn854RqvGerWkVVrKPjkQf8vG\n7/76ukPSeZJ2HGRcb5PUX+vdJuknkv6upf8Wks6StFDSfZJulPRVSRu3jWeKpA9LmitpkaT5kr4j\naZu2epMlzao/XEvaoa2/JH1W0t319TlJGmI5TAK+COxU16e7R740nzLOnm1QRsr2L2xv2VL0YeBi\n22vZ/gqwF/BsYH3bb3m646vf73Of7umONyttIhnn/gjs3baH+U/ADaMZWd3wjcm6MMK93o2AfYap\ns47tNYGXABcAP5Q0s2V6hwL/BXyassHZFPg6sEft/1zgMuBWYBvbawOvpCzD1mTzV8Cvgb8G/hn4\nK2Ar4AfA6ZLe0RbXL4G3A7cPEvNBwJ415hcDuwHvGmL+ng2sBlw7zHJ4irH83trG28sjl8148rLY\nDLjB9tKRjmhFOQJbKdheKV/AxcCB9fNuwJXAYuBXwItb6t0M/EP9fCRwev08AzAwsWV8RwP/CywB\nfgpMbRnP9nXci4GrgB2GiGVD4HfAh4aIeyZlI3Y+8Ppath5lg3YMcPIIpvmpGu+DwHPreE6ibHTv\nAc5pqT/cMvq3GvfDA8tkmOXvOsyNLcvwkwPxty/fluE+BNxB2Ql6FnA/8JblTOd04NwO4vkZ8M9D\n9FsXuAb460H6LWhdrrXsV8BBLd3vBC4dZNjnAQ/U+bwf+Fkt/1vgcuDe+v63y/ve2sZ5GvB47Xc/\nZQ9/YFkeAPwFuAv495ZhjgRm1WV1H3BgXb6HUxLu3cB3gfU6WbcGmc9tgCsov4uzgbOAT9Z+OwAL\nWr6Dx4CHauxnAo8Aj9bud9Z6/wxcR1lH5wCbta1X763r1Z9q2fMpOyGLgOuBt7bUPxk4FvjvGt9l\nA98z8D91fA/U6e891O+xpfvLwPy6HOcCr2pbzt+ry3kJcHVdBz4C3FmH26ntu/4M8Ju6Lvyo7TvY\nH/hz/X7+nSdvq7aj7BgtBm4DvgZMrv0EfKlO817K7/aFjbanTQYez6/6JR0IvKwu0FcAEyg/tpuB\nVWu91i/nSJafSP5YV4zVa/d/1n7T65f9j5Qf6I61e1pbLDMoRxUHLSfumZRE8jbg7Fr2r8C3ePKG\nuJNp/gV4ATARmET5MZ1N2XBOAl5T63ayjK4ENgFWr2VfB76+nPkwsAXlxzaQRDtJJM+p5VsBuwBL\n2+u01b8dmDnMuvAaYE79vAllg3YrZQPzm1q+H/D5QYYdLJHcC7yipbsPWDLEtNvXo/UoG8j96/ey\nb+1ef6jvbZBx3kxdZ9umcTxl3XwJJeFv1bJeP0o5ilql1nk/cCmwMbAqZf06s5N1qy2WyZSN3Qfq\nOrVXndZTEknrb6Gl+0jqb6527wnMq9//ROAI4Fdt69UFdTmuDqxB2UC/o9Z/GSWRvqDWP5mSYLar\n/b8DnNU2vucO9t21/h5but8OrF/H9UHK+rday7w8BOxc+58K/ImSBCYB/0JNfi3L4hbghXU+vs8T\n25+tKcnt1fX7+SLltzCwrdqWkuwn1u//OuD9td/OlN/dOpSkshWwYZPtaZq2ypf3LduX2X7M9imU\nH9n2oxjXSbZvsP0gZQ/upbX87cCPbf/Y9uO2LwD6KT/EAVtTVpyP2z6ug2n9ENhB0rMozVqntvXv\nZJon277WpdlgKrAr8G7b99h+1PYltV4ny+grtufXecf2v9r+12HmwcDHgP+QtGoH8wxlAw9lQ7E+\ncJeX3+wxlZbmJ0kHS1pcz6ccX4t3pOwlA3yesqe9KXAeJQlASZTP7zDGNSnJZMC9wJpDnSdp83rg\nRtun2V5q+0zgD8AbWuos+95sP9phTACfsP2g7asoRxEvaen3a9vn1HXlQUpT3L/bXmD7YcpGcK/a\nXNTJujVge8pG8r/qOjWLcpQ1Wu8CPmP7uvq9fxp4qaTNWup8xvaiOh+7ATfbPqkurysoG+S9Wur/\nwPZv6vi+wxO/2xGzfbrtu+u0vkDZyLeeA/qF7Tl1Wt8DplF2OB+lrIMzJK3TUv8029fYfoDyW3lr\nvcBnL+A82/9Tv5+PUY5EB+KYa/vSGsfNlB2B19TejwJrUdZn1WV522jnGXKOBEob7AfrxmWxpMWU\nvdKNRjGu1vby/6NsUAam8Za2afwdpRlrwH6UvY9ZAwX1ipaBk81PakOvP5L/puyRTbX9v4PM13DT\nnN/yeRNgke17BpmvTpbR/EGGG5btH1P2sA/qcJDp9X0RZS946jBt4XfTMs+2v2Z7Hcp5lUm1eAPK\nsgd4EXBG/QH+hLL3CmV+B+oM535g7ZbutYH7XXcHh7ERZQ++1Z95Yr5hlMuaodfPwca5GeV81MD3\nfR2l2enZdLZuDdgIuKVt3tvnbyQ2A77cMt1FlL3qoZbPZsAr2mLdj3IObMDylsuISPqgpOvqVWaL\nKc2vU1uq3NHy+UHKjtBjLd0w9PfyZ8o6O5WyXJf1q4lm2YUakp5XL065XdJ9lIQ7tdb9GaWp61jg\nDknHSWpdX0csiaR8GZ+yvU7La0rdExzLaZzWNo01bP9nS50jKRutM+oeBy5XtKxZXy8YZLynUg6f\nTxvlNN1Wf722vaHWfsMto042kkM5gnJ4P6WDum+kNLNdT2kDfojS3DGUi4A3DTPOu3hiI3g18DZJ\nEyXtQklUz6Wclzihg/ignCxu3dt/CZ2fTL+VsvFrtSlPTmLDLevRfBftw8wHdm37zlezfQudrVsD\nbgOmtx2NbTqK+FrjelfbtFe3/ash5mU+cElb/TVtv6dBDIOS9CrKeb+3AuvWHZZ7KYlutDZp+bwp\n5WjiLspyXdZP0hTKEfqAb1COZLdwucDko61x2P6K7W0pTaTPAw5rEGMSCaXd+N2SXlGvgllD0usl\nrTWG0zgdeIOknSVNkLSapB3aLj99FHgLpS30tA6vxrmE0izz1VFOc5l6aPsT4OuS1pU0SdKra++u\nLiPbF1M24AcMVUfSsyUdDHwc+EhtUrkX+A/gWJXLn6fUuHeV9Lk66JHAqyR9UdL0Oq6plHbhAT/j\niaaOD1FOds8H9qYs4xOAD9te1iQjaVVJq9XOyXX5DvxQTwUOlTRd0kaUZH9yh4vjx8DzVC5pnihp\nb0qz53kdDg9lr/c5I6g/mG8CnxpoMpI0TdIetd9I1q1fU9ruD6nz8ybK+YgmcX1E0gtqXM+StLzL\ngs+jLM/967oxSdLLJW21nGFajWRZrkWZ14XAREn/wZOPTEfj7ZK2roniKGBWPYKZBewm6e8kTa79\nWrcZa1FO+N8v6fnAssRZ5/8VKpeeP0DZGWv0N4aVPZHYdj/lHMDXKCc151FOoI3lROZTLkf9KGUl\nm0/ZA1ilrd4jlL3nDYATh0smLi6yvWi002yzPyWh/YGy1//+Oq4RLyNJ35T0zeXVaXME5bxHu8WS\nHqAkmn+kXKF14kBP218EDq3DD8znwcA5tf8NlHb6jYGrJC2hXPF0K6VdGdsXAutK2s/lPM/f297Q\n9jtsv5ZyAvOitriupzRFTKdcOfQgTxxJfAs4t8Z8DaUJ8ludLASX/5HsRkk+d1OuutrN9l3LHfDJ\nPgMcUZtyPjSC4Vp9GZgN/LQus0spF1uMaN1qWadnUtadvSmXVI+K7R8CnwXOqk0211DO7Q1Vfwmw\nE+Uy81spzVifpZy76MSRwCl1Wb51mLpzKDtjN1CaoR5i9M2QA06j7ITcTrlM/BAA29dSrk47g3J0\ncg/lwo8BH6JckLOEsiN4dku/tWvZPTxx1dfnmwSpzpptn3kkXQEcZfucXscSvVePVn5K2YM9HriJ\n0tz1/4CX2B5yYxWxslspj0jqYfFWwG97HUusGGrb/99Q9iLPoZzEvYRy+eT+PQwtYoW30h2RSPos\n5fLFz7rcgiEiIhpY6RJJRESMrZWyaSsiIsbOSnFTs6lTp3rGjBm9DiMiYlyZO3fuXbanDVdvpUgk\nM2bMoL+/v9dhRESMK5I6ugtBmrYiIqKRJJKIiGgkiSQiIhpJIomIiEaSSCIiopEkkoiIaCSJJCIi\nGkkiiYiIRpJIIiKikSSSiIhoJIkkIiIaSSKJiIhGkkgiIqKRJJKIiGgkiSQiIhpJIomIiEaSSCIi\nopEkkoiIaCSJJCIiGkkiiYiIRpJIIiKikSSSiIhoJIkkIiIaSSKJiIhGkkgiIqKRriYSSbtIul7S\nPEmHD9J/pqSFkq6srwNb+p0vabGk89qGOVnSn1qGeWk35yEiIpZvYrdGLGkCcCywI7AAuFzSbNu/\nb6t6tu2DBxnFMcAU4F2D9DvM9qwxDTgiIkalm0ck2wHzbN9k+xHgLGCPTge2fRGwpFvBRUTE2Ohm\nIpkOzG/pXlDL2r1Z0u8kzZK0SYfj/lQd5kuSVh2sgqSDJPVL6l+4cOEIQ4+IiE51M5FokDK3dZ8L\nzLD9YuBC4JQOxvsR4PnAy4H1gH8brJLt42z32e6bNm1a51FHRMSIdDORLABajzA2Bm5trWD7btsP\n187jgW2HG6nt21w8DJxEaUKLiIge6WYiuRzYQtLmkiYD+wCzWytI2rClc3fguuFGOjCMJAF7AteM\nWcQRETFiXbtqy/ZSSQcDc4AJwIm2r5V0FNBvezZwiKTdgaXAImDmwPCSfkFpwlpT0gLgnbbnAN+R\nNI3SdHYl8O5uzUNERAxPdvtpi2eevr4+9/f39zqMiIhxRdJc233D1cs/2yMiopEkkoiIaCSJJCIi\nGkkiiYiIRpJIIiKikSSSiIhoJIkkIiIaSSKJiIhGkkgiIqKRJJKIiGgkiSQiIhpJIomIiEaSSCIi\nopEkkoiIaCSJJCIiGkkiiYiIRpJIIiKikSSSiIhoJIkkIiIaSSKJiIhGkkgiIqKRJJKIiGgkiSQi\nIhpJIomIiEaSSCIiopEkkoiIaCSJJCIiGulqIpG0i6TrJc2TdPgg/WdKWijpyvo6sKXf+ZIWSzqv\nbZjNJV0m6UZJZ0ua3M15iIiI5etaIpE0ATgW2BXYGthX0taDVD3b9kvr64SW8mOA/Qep/1ngS7a3\nAO4B3jnGoUdExAh084hkO2Ce7ZtsPwKcBezR6cC2LwKWtJZJEvD3wKxadAqw59iEGxERo9HNRDId\nmN/SvaCWtXuzpN9JmiVpk2HGuT6w2PbSYcaJpIMk9UvqX7hw4Uhjj4iIDnUzkWiQMrd1nwvMsP1i\n4ELKEUbTcZZC+zjbfbb7pk2bNmywERExOt1MJAuA1iOMjYFbWyvYvtv2w7XzeGDbYcZ5F7COpIlD\njTMiIp5e3UwklwNb1KusJgP7ALNbK0jasKVzd+C65Y3QtoGfA3vVogOAH41ZxBERMWJdSyT1PMbB\nwBxKgviu7WslHSVp91rtEEnXSroKOASYOTC8pF8A3wNeJ2mBpJ1rr38DDpU0j3LO5NvdmoeIiBie\nyk7+M1tfX5/7+/t7HUZExLgiaa7tvuHq5Z/tERHRSBJJREQ0kkQSERGNJJFEREQjSSQREdFIEklE\nRDSSRBIREY0kkURERCNJJBER0UgSSURENJJEEhERjSSRREREI0kkERHRSBJJREQ0MnG4CvVphO8E\n3ghsRHm07a2UB0p92/ajXY0wIiJWaMMmEuA0YDFwJOXxuVAecXsAcDqwd1cii4iIcaGTRPIy21u2\nlS0ALpV0QxdiioiIcaSTcyT3SHqLpGV1Ja0iaW/gnu6FFhER40EniWQfYC/gDkk31KOQ24E31X4R\nEbESG7Zpy/bN1PMgktanPOf9ri7HFRER48SILv+1fXdrEpG049iHFBER40nT/5F8e0yiiIiIcauT\n/5HMHqoXsP7YhhMREeNNJ5f/vgp4O3B/W7mA7cY8ooiIGFc6SSSXAv9n+5L2HpKuH/uQIiJiPOnk\nqq1dl9Pv1WMbTkREjDcjPtkuaf3WPydGRMTKraOEIGldSV+TdAlwLHC+pBMlrTHMcLtIul7SPEmH\nD9J/pqSFkq6srwNb+h0g6cb6OqCl/OI6zoFhNuh8diMiYqx1ctXWOsCPgY/aPril/LXAf0o6G7jS\n9v1tw02gJJ0dKffmulzSbNu/b5vE2a3jrcOuB3wc6KPcbXhuHXbgliz72e4fyYxGRER3dHJE8jHg\n87Z/Lum0eoTwa+A4YHodx0cHGW47YJ7tm2w/ApwF7NFhXDsDF9heVJPHBcAuHQ4bERFPo04SyWts\nf79+fhjY1/bfUG6bcjfwS+A1gww3HZjf0r2glrV7s6TfSZolaZMOhz2pNmt9TJIGC1rSQZL6JfUv\nXLhwuHmMiIhR6iSRrNqysd4GuKp+voZyi/nHgSmDDDfYBt5t3ecCM2y/GLgQOKWDYfez/SLK/1te\nBew/WNC2j7PdZ7tv2rRpg1WJiIgx0Eki+Q3wuvr5G8BPJX0amAN8S9LLgWsHGW4BsElL98aUJysu\nU+/d9XDtPB7Ydrhhbd9S35cAZ5A/RUZE9FQnieRTlJPqz7Z9AvAW4Jz6/t/AV4GjBxnucmALSZtL\nmky55fyTbrciacOWzt2B6+rnOcBO9WqxdYGdgDmSJkqaWoedBOxGOTKKiIge6eQPiTdJei8wW9JP\nKf90f4yyEd8TeK/tp/zD3fZSSQdTksIE4ETb10o6Cui3PRs4RNLuwFJgETCzDrtI0tGUZARwVC1b\ng5JQJtVxXkg5khlz5/z2Fo6Zcz23Ln6QjdZZncN23pI9txnsFE/EyGX9im56utcv2e2nLYaoWP6E\n+DrgJZRzGFcDF9pe2rXoxkhfX5/7+zu/Wvic397CR35wNQ8++tiystUnTeAzb3pRfuzRWNav6Kax\nXL8kzbXdN1y9jv+hbvtx2xfY/rztY2yfPx6SyGgcM+f6J30JAA8++hjHzMmtxaK5rF/RTb1Yv4ZN\nJJLeKemwlu4Fku6TtETSe7oWWQ/duvjBEZVHjETWr+imXqxfnRyRvBs4saV7oe21gWnAvl2Jqsc2\nWmf1EZVHjETWr+imXqxfnSSSVWzf3dL9PQDbDwHPyDX/sJ23ZPVJE55UtvqkCRy285Y9iiieSbJ+\nRTf1Yv3q5Hkkz2rtsP1pWHby/Rn5hMSBE1K5qia6IetXdFMv1q9hr9qS9HVgke0j2so/CUy1/e6u\nRTdGRnrVVkREdH7VVidHJIcBJ0iaxxO3R3kJ0A8cOORQERGxUujkD4kPAPtKeg7wglr8e9t/7Gpk\nERExLnTyPJKdgbVszwJuainfD7jT9gVdjC8iIlZwnVy19QngkkHKLwKOGttwIiJivOkkkUyx/ZQH\neti+HVjuo3YjIuKZr5NEspqkpzSB1RsnPiP/RxIREZ3rJJH8ADi+3nkXgPr5m7VfRESsxDpJJEcA\ndwB/ljRX0hXAzcDC2i8iIlZinVz+uxQ4XNIngOfW4nm2c4e5iIjo6A+JSFofeBvw/Fp0naQz2+7B\nFRERK6FObiO/FeVxttsCNwA3Ai8Hrpb0/OUNGxERz3ydHJEcDbzP9ndbCyW9mfI89zd3I7CIiBgf\nOjnZ/qL2JAJg+/vAC8c+pIiIGE86SSQPjLJfRESsBDpp2tpA0qGDlIvylMSIiFiJdZJIjgfWGqLf\nCWMYS0REjEOd/I/kE09HIBERMT51chv5/1hOb9s+egzjiYiIcaaTpq3BTqivAbyT8sz2JJKIiJVY\nJ01bXxj4LGkt4H3AO4CzgC8MNVxERKwcOr1FynrAocB+wCnAy2zf083AIiJifOjkFinHAJcDSyh/\nTjyy0yQiaRdJ10uaJ+nwQfrPlLRQ0pX1dWBLvwMk3VhfB7SUbyvp6jrOr0hSR3MaERFd0ckfEj8I\nbES5Zfytku6rryWS7htqIEkTgGOBXYGtgX0lbT1I1bNtv7S+TqjDrgd8HHgFsB3wcUnr1vrfAA4C\ntqivXTqZ0YiI6I5hE4ntVWyvbnst22u3vNayvfZyBt2Ocrv5m2w/QjmnskeHce0MXGB7UT36uQDY\nRdKGwNq2f23bwKnAnh2OMyIiuqCTI5LRmg7Mb+leUMvavVnS7yTNkrTJMMNOr5+HGyeSDpLUL6l/\n4cKnPHI+IiLGSDcTyWDnLtzWfS4ww/aLgQspJ/KXN2wn4yyF9nG2+2z3TZuWO7lERHRLNxPJAmCT\nlu6NgVtbK9i+2/bDtfN4yjNPljfsgvp5yHFGRMTTq5uJ5HJgC0mbS5oM7APMbq1Qz3kM2B24rn6e\nA+wkad16kn0nYI7t24AlkravV2v9E/CjLs5DREQMo6P/kYyG7aWSDqYkhQnAibavlXQU0G97NnCI\npN2BpcAiYGYddpGkoynJCOAo24vq5/cAJwOrAz+pr4iI6BGVi5+e2fr6+tzf39/rMCIixhVJc233\nDVevm01bERGxEkgiiYiIRpJIIiKikSSSiIhoJIkkIiIaSSKJiIhGkkgiIqKRJJKIiGgkiSQiIhpJ\nIomIiEaSSCIiopEkkoiIaCSJJCIiGkkiiYiIRpJIIiKikSSSiIhoJIkkIiIaSSKJiIhGkkgiIqKR\nJJKIiGgkiSQiIhpJIomIiEaSSCIiopEkkoiIaCSJJCIiGkkiiYiIRrqaSCTtIul6SfMkHb6centJ\nsqS+2j1Z0kmSrpZ0laQdWupeXMd5ZX1t0M15iIiI5ZvYrRFLmgAcC+wILAAulzTb9u/b6q0FHAJc\n1lL8LwC2X1QTxU8kvdz247X/frb7uxV7RER0rptHJNsB82zfZPsR4Cxgj0HqHQ18DniopWxr4CIA\n23cCi4G+LsYaERGj1M1EMh2Y39K9oJYtI2kbYBPb57UNexWwh6SJkjYHtgU2ael/Um3W+pgkdSH2\niIjoUNeatoDBNvBe1lNaBfgSMHOQeicCWwH9wJ+BXwFLa7/9bN9Sm8S+D+wPnPqUiUsHAQcBbLrp\npqOeiYiIWL5uHpEs4MlHERsDt7Z0rwW8ELhY0s3A9sBsSX22l9r+gO2X2t4DWAe4EcD2LfV9CXAG\npQntKWwfZ7vPdt+0adPGeNYiImJANxPJ5cAWkjaXNBnYB5g90NP2vban2p5hewZwKbC77X5JUySt\nASBpR2Cp7d/Xpq6ptXwSsBtwTRfnISIihtG1pi3bSyUdDMwBJgAn2r5W0lFAv+3Zyxl8A2COpMeB\nWyjNVwCr1vJJdZwXAsd3ax4iImJ4sj18rXGur6/P/f25WjgiYiQkzbU97BWz+Wd7REQ0kkQSERGN\nJJFEREQjSSQREdFIEklERDSSRBIREY0kkURERCNJJBER0UgSSURENJJEEhERjSSRREREI0kkERHR\nSBJJREQ0kkQSERGNJJFEREQjSSQREdFIEklERDSSRBIREY0kkURERCNJJBER0UgSSURENJJEEhER\njSSRREREI0kkERHRSBJJREQ0kkQSERGNJJFEREQjXU0kknaRdL2keZIOX069vSRZUl/tnizpJElX\nS7pK0g4tdbet5fMkfUWSujkPERGxfF1LJJImAMcCuwJbA/tK2nqQemsBhwCXtRT/C4DtFwE7Al+Q\nNBDrN4CDgC3qa5duzUNERAyvm0ck2wHzbN9k+xHgLGCPQeodDXwOeKilbGvgIgDbdwKLgT5JGwJr\n2/61bQOnAnt2cR4iImIY3Uwk04H5Ld0LatkykrYBNrF9XtuwVwF7SJooaXNgW2CTOvyC5Y2zZdwH\nSeqX1L9w4cJmcxIREUOa2MVxD3buwst6lqaqLwEzB6l3IrAV0A/8GfgVsHS4cT6p0D4OOA6gr69v\n0DoREdFcNxPJAspRxICNgVtbutcCXghcXM+X/xUwW9LutvuBDwxUlPQr4EbgnjqeocYZERFPs242\nbV0ObCFpc0mTgX2A2QM9bd9re6rtGbZnAJcCu9vulzRF0hoAknYEltr+ve3bgCWStq9Xa/0T8KMu\nzkNERAyja0cktpdKOhiYA0wATrR9raSjgH7bs5cz+AbAHEmPA7cA+7f0ew9wMrA68JP6ioiIHlG5\n+OmZra+vz/39/b0OIyJiXJE013bfcPXyz/aIiGgkiSQiIhpJIomIiEZWinMkkhZS/o8yGlOBu8Yw\nnLGSuEYmcY1M4hqZZ2pcm9meNlyllSKRNCGpv5OTTU+3xDUyiWtkEtfIrOxxpWkrIiIaSSKJiIhG\nkkiGd1yvAxhC4hqZxDUyiWtkVuq4co4kIiIayRFJREQ0kkQSERGNJJEMQdKJku6UdE2vY2klaRNJ\nP5d0naRrJb2v1zEBSFpN0m8kXVXj+kSvY2olaYKk30pqf4haz0i6WdLVkq6UtMLcDE7SOpJmSfpD\nXc/+ZgWIacu6nAZe90l6f6/jApD0gbrOXyPpTEmr9TomAEnvqzFd2+1llXMkQ5D0auB+4FTbL+x1\nPAPq44Y3tH1Ffd79XGBP27/vcVwC1rB9v6RJwC+B99m+tJdxDZB0KNBHeVTzbr2OB0oiAfpsr1B/\nZJN0CvAL2yfUR0BMsb2413ENkDSBclfwV9ge7R+NxyqW6ZR1fWvbD0r6LvBj2yf3OK4XUh5vvh3w\nCHA+8B7bN3ZjejkiGYLt/wEW9TqOdrZvs31F/bwEuI4hHjf8dHJxf+2cVF8rxF6KpI2B1wMn9DqW\nFZ2ktYFXA98GsP3IipREqtcBf+x1EmkxEVhd0kRgCivGw/a2Ai61/X+2lwKXAG/s1sSSSMYxSTOA\nbYDLehtJUZuPrgTuBC6wvUJs71vxAAAD5klEQVTEBfwX8GHg8V4H0sbATyXNlXRQr4OpngMsBE6q\nTYEnDDxkbgWyD3Bmr4MAsH0L8HngL8BtwL22f9rbqAC4Bni1pPUlTQH+kSc/sXZMJZGMU5LWBL4P\nvN/2fb2OB8D2Y7ZfSnkE8nb18LqnJO0G3Gl7bq9jGcQrbb8M2BV4b21O7bWJwMuAb9jeBngAOLy3\nIT2hNrXtDnyv17EASFoX2APYHNgIWEPS23sbFdi+DvgscAGlWesqYGm3ppdEMg7VcxDfB75j+we9\njqddbQq5GNilx6EAvBLYvZ6POAv4e0mn9zakwvat9f1O4IeU9uxeWwAsaDmanEVJLCuKXYErbN/R\n60CqfwD+ZHuh7UeBHwB/2+OYALD9bdsvs/1qSjN9V86PQBLJuFNPan8buM72F3sdzwBJ0yStUz+v\nTvmB/aG3UYHtj9je2PYMSpPIz2z3fI9R0hr1Yglq09FOlOaInrJ9OzBf0pa16HVATy/kaLMvK0iz\nVvUXYHtJU+pv83WU85Y9J2mD+r4p8Ca6uNy69sz28U7SmcAOwFRJC4CP2/52b6MCyh72/sDV9XwE\nwEdt/7iHMQFsCJxSr6hZBfiu7RXmUtsV0LOBH5ZtDxOBM2yf39uQlvl/wHdqM9JNwDt6HA8Ata1/\nR+BdvY5lgO3LJM0CrqA0Hf2WFed2Kd+XtD7wKPBe2/d0a0K5/DciIhpJ01ZERDSSRBIREY0kkURE\nRCNJJBER0UgSSURENJJEEjFCku4fvtaIx3mzpKm9mHZEU0kkERHRSP6QGDEGJL0BOAKYDNwN7Gf7\nDklHUu7DtCHwPOBQYHvKrT5uAd5Qb60BcJik19bPb7M9T9LmwBmU3+r5LdNbE/gRsC7lTstH2P5R\nd+cyYnA5IokYG78Etq83OjyLcrfhAX9NuY39HsDpwM9tvwh4sJYPuM/2dsDXKHcsBvgy5QaKLwdu\nb6n7EPDGetPH1wJfqLfoiHjaJZFEjI2NgTmSrgYOA17Q0u8n9ajjamACTxxZXA3MaKl3Zsv7wFMJ\nX9lSflpLXQGflvQ74ELKM2mePSZzEjFCSSQRY+OrwNfqkca7gNbHrT4MYPtx4FE/cV+ix3ly87I7\n+DxgP2AasG29df8dbdOMeNokkUSMjWdRznkAHDDKcezd8v7r+vl/KXcthpI8Wqd3p+1H63mVzUY5\nzYjGcrI9YuSm1DtCD/gicCTwPUm3AJdSTrCP1KqSLqPs4O1by94HnCHpfZRn0Az4DnCupH7gSlaA\nW/bHyit3/42IiEbStBUREY0kkURERCNJJBER0UgSSURENJJEEhERjSSRREREI0kkERHRyP8HF1Si\n++hpnY8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc881fee908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jelimerc_ndcg = [0.504,0.504,0.504]\n",
    "jelimerc_lambda = [1,5,9]\n",
    "\n",
    "plt.scatter(jelimerc_lambda,jelimerc_ndcg)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('Jellinek-Mercer: NDCG@10 for three different lambdas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xu4XFV9//H3h4RwCcEICWIukIAp\nbYTKZUix/lRULkGRYEUNpQqKUij8aC1QQEC59KKgYC0IgoQiChFB9EjF4A0t5ZaTcgkBI4cQyEm4\nHJBAiNwC3/6x1oSdyZw5c7IzmXOSz+t55jmz1157zVozc/Z39lp7r62IwMzMbE1t1O4KmJnZ4OZA\nYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiQDmKRLJJ3RYP27Jc1vopwjJN3aYP0t\nkj67pvW05kjaTNJPJD0n6Qcteo0zJX23FWWvCUkh6W35+SrfZ0nHSHpS0guStpb0LkkP5eWD21dr\n6y8HkjaRtFDSi5KWSVoq6TZJR0ta+ZlExNERcU5vZUTEf0fETuumxklxx9Bk/iPyNifVpHdL2js/\nP1PSq/m9WCbp95IulPTWmm22lPR1SY/lnU1XXh5VyDNd0p2Slkt6Kj//O0mqKWsnSZdLekTSHyTN\nlXSWpBE1+d4n6dd557+wTvsm5PV/lPQ7Sfs0eDsOAd4CbB0RH2vyLeyVpL0ldZctZ10pfp8lbQyc\nD+wXEVtExDPA2cCFeflH67Juff3YssYcSNrrwxExAtge+DJwMnB5MxtKGtrKiq1lfwBOlrRlgzzf\nz+/FVsBHgG2BOdVgImkY8Evg7cBUYEvgL4FngCk5zwnAvwPn5e3fAhwNvAsYVn0hSX8F3AT8b163\nNXAgEMCdkrYr1Gs5MANYJRAWXAPcncs4DbhO0uhe8m4P/D4iVjR4H+pqxefd5u/QW4BNgXmFtO1r\nlps2yP4f1j8R4UcbHsBCYJ+atCnA68DOefk/gX/Oz/cGuknB5gngqmpaYfvxwA+BHtIO9sKcfgRw\nK/BV4FngEeCAwna3AJ8tLH8GeDDnnQVsn9N/S9rZLgdeAD7RRDurr/0T4EuF9G5g7/z8TOC7NdsN\nAe4FvpqXPws8CWzRy+u8Kdfro33UZyegCxjXy/oPAb+qk74PsLAm7U+Al4ERhbT/Bo6us/1ZwCvA\nq/m9O5L0Q+504FHgKeA7wJty/gn5vT4SeAz4bU15w4EX8/flhfwYk9/La3NZy0g75krN9+5k4L5c\n96F5u+vz9+YR4PhC/o2AU4CH83fqWmCrBu/vScDjwJL8PQrgbcXvc37flud1LwC/yuW/ntv0ArBJ\n/kwvz+UtztsOKXyv/ge4gPRDpfp/Uve7m9cF6YfFQ3n9RYCAPwNeAl7Lr720l7bdkutwW873E9IP\niO8BzwOzgQk1n9/Qev9nwNuA3wDPAU+Tfki1fb+0pg8fkQwgEXEXaQf77l6ybEv6xb49cFRxhaQh\nwI2kndIEYCwws5DlL4D5wCjgXODy2u6eXM7BwBeAvwJGk3aM1+T6vSdne0ek7ofv522WSvp/fTTv\nDODzkrbqIx/5tV4Dfswb78U+wM8i4oVeNnknaefz4z6KPgU4IyK6JX1c0gJJD0o6TdJlEfFfwGuS\ndm6imm8HFkTEskLavTm9tj1fAv6VtMPYIiIuJ+0MjwDeB+wAbAFcWLPpe0k7uv1rylsOHAAsyeVt\nERFL8uqDSJ/9SKCjTpmHkgLmSNLO+ye53mOBDwD/IKn6escDB+d6jOGNHfBqJE0FTgT2BSaRPrPV\nRMTveeM9GhkR74+IHUkB88O5LS8DVwIrSDvd3YD9SD8oqv4CWABsA/xLo+9uwYHAnsA7gI8D+0fE\ng6QAc3t+7ZH16p1NBz5Jeq92BG4HriD9Xz4IfKnBtkXnADcDbwbGAf/R5HYDkgPJwLOE9KWs53XS\nr/qXI+LFmnVTSP/oJ0XE8oh4KSKKfb6PRsRleQd9JfBWUvdCrb8F/i0iHozUBfOvwK6Stu+twhEx\nsua16uW5h/SPc3KjfDWK78XWpF+mvRkFPB2FbqM87rQ0j0VVg+DewPU5oH0T+BiwK+lX8sY5zz3A\nnzZRvy1IvyiLngNG1Mlbz2HA+RGxIAfIU4HpNd00Z+bPs/bzbuTWiPhp/qyvIu00i74REYtymXsC\noyPi7Ih4JSIWAJeRdpiQvg+nRUR33rmfCRzSS1fSx4ErIuL+HOjO7EedVyHpLaRA+Q+5/U+Rjj6m\nF7ItiYj/iIgVuS3NfHe/HBFLI+Ix4Nekz74/roiIhyPiOVL36MMR8Yv8ej8gBbxmvEr6QTimzv/q\noONAMvCMJR2q19MTES/1sm48KVj01v/+RPVJRPwxP92iTr7tgX/PO+CluS7K9Srri8AxkrZtMn/x\nvXiGFPx68wwwqriDi4i/zL8un+GN77oi4hXSr9wFETEn7yC/XyhrPKkrpS8vkMZqirYkdSk1Ywzp\nCLLqUVJXUzHAL2qyrKInCs//CGxas+Mvlrk9MKb6eefP/AuFOmwP3FBY9yCpC6jej5AxNWU/WidP\ns7YnBfbHC6/9LdLRR712VLfp67tb+97U+x9o5MnC8xfrLDdb3j/lut0laZ6kz/SzHgOKA8kAImlP\n0pe+t18njeb8XwRstxYGHRcBf5uPMqqPzSLitpLlEhG/I43hfKGvvPnstQ+TuicAfgHsL2l4L5vc\nTurzn9ZH0a/ngfsuYAdJu0vahPRreoikj5O6Bmf3VUfS+MMONWd6vYPmB4yXkHZ+VduRunKKO6dG\nn/ma3gOiuN0i4JGaz3tERHywsP6AmvWbRkS9QPs4KQhXbVcnT7MWkT7PUYXX3TIiit2Gte0v891d\n2/fTWJ7/bl5IW/kDKiKeiIjPRcQY0pHUN/tzNuRA40AyAOTTWg8k9Wt/NyLmrkExd5H+kb8sabik\nTSW9aw3KuQQ4VdLbc93eJKl4quqTpP78NXUW8GlS//xqJG0s6c9Ifdvbkk4RhdRFs4jULfWnkjZS\nuvbgC5I+GBFLc9nflHSIpC1ynl1JA9NVt5H64f8A/B1pkPk+0tjUu0hjEdOqR3a5jE1Jv46V39dh\nsLKv/x7gSzn9I8Cf5zKbcQ1p3GiipC14Ywyl2bO6ngS2lvSmJvPXcxfwvKSTla5zGSJp5/yjBtL3\n4V+q3UOSRkvqLVhfCxwhabKkzWl+vGA1EfE4qSv0a/n/YyNJO0p6b4PN+vruNvIkMK762ZYVET2k\no9q/ye/pZ0hjKuS6fUzSuLz4LCmQvbY2XrsdHEja6yeSlpF2kKeRdpqfXpOCcn/4h0ldNo+Rdoyf\nWINybgC+AsyU9DxwP6mvuupM4MrcffBxAKVrOno7QaC2/EdIQaH2yOITkl4AlpIGiJ8B9qgOIOfu\np32A3wE/J50lcxdpbOTOnOdc4B9J3QZPkXYO3yKNy1R/lX4Z+DdJ20bEtRExMSJ2iojTSf/on4uI\n4hHBe0hdFj8l/cJ+kbSDq5oOVEg7gy8Dh+SdSDNm5Pfit6SzpV4C/n+T21aP8K4BFuTPY0yz2xbK\nqH5vds11eBr4NumMKUinU3cAN+fv6h2kQe56Zd0EfJ10FlZX/lvGp0inbT9Aen+vo0H3ZhPf3UZ+\nRTqSfELS02UqXfA50llsz5BOLigeGe1JOtX8BdL7+/f5f2NQUoTvkGgbFkmHks6a+SJpwHQZKRic\nB1wSEd9rY/XMBh0HEtsg5S6vk0mnFw8nDSJfHBFXtbViZoOQA4mZmZXiMRIzMytlg5ifZtSoUTFh\nwoR2V8PMbFCZM2fO0xHR29xxK20QgWTChAl0dna2uxpmZoOKpKYuKnXXlpmZldLSQCJpqqT5SveN\nOKVBvkOU7llRycv7SpqjdI+IOZLeX8i7R07vkvQNafWJB83MbN1pWSBRmo32ItIFQZOBQyVNrpNv\nBGmG0TsLyU+Trj7eBTicdNFW1cWkmW8n5cfUljTAzMya0sojkilAV57Z9BXS9B/1plY4hzSt+crJ\nCCPi7sKU2PNIk85tonSToy0j4vZI5y1/hzTFtZmZtUkrA8lYVp2ds5uaGWQl7QaMj4gbG5TzUeDu\nPEXG2FxOr2UWyj5KUqekzp6eZmesMDOz/mplIKk3drHy6sc8u+sFwAm9FpAmX/sKaXbMPstcJTHi\n0oioRERl9Og+z14zM7M11MpA0s2qU0qPI02bXTUC2Bm4RdJCYC+gozDgPg64AfhURDxcKHNcoYza\nMs3MbB1rZSCZDUzKU2QPI82S2lFdGRHPRcSoiJgQERNIs4oeFBGdkkYC/wWcGhH/U9jmcWCZpL3y\n2Vqfou9bq5qZWQu1LJDkeyocB8wiTYh3bUTMk3S2pIP62Pw40nToZ0i6Jz+qd0Y7hjTNdRfwMGn2\nVjMza5MNYtLGSqUSvrLdzKx/JM2JiEpf+Xxlu5mZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBi\nZmalOJCYmVkpDiRmZlaKA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4k\nZmZWigOJmZmV4kBiZmalOJCYmVkpDiRmZlaKA4mZmZXS0kAiaaqk+ZK6JJ3SIN8hkkJSJS9vLenX\nkl6QdGFN3ltymffkxzatbIOZmTU2tFUFSxoCXATsC3QDsyV1RMQDNflGAMcDdxaSXwLOAHbOj1qH\nRURnSypuZmb90sojkilAV0QsiIhXgJnAtDr5zgHOJQUPACJieUTcWkwzM7OBqZWBZCywqLDcndNW\nkrQbMD4ibuxn2Vfkbq0zJKleBklHSeqU1NnT09PP4s3MrFmtDCT1dvCxcqW0EXABcEI/yz0sInYB\n3p0fn6yXKSIujYhKRFRGjx7dz5cwM7NmtTKQdAPjC8vjgCWF5RGk8Y9bJC0E9gI6qgPuvYmIxfnv\nMuBqUheamZm1SSsDyWxgkqSJkoYB04GO6sqIeC4iRkXEhIiYANwBHNRoEF3SUEmj8vONgQOB+1vY\nBjMz60PLztqKiBWSjgNmAUOAGRExT9LZQGdEdDTaPh+lbAkMk3QwsB/wKDArB5EhwC+Ay1rVBjMz\n65siou9cg1ylUonOTp8tbGbWH5LmRETD4Qbwle1mZlaSA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooD\niZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiRmZlaKA4mZmZXiQGJmZqU4\nkJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWSksDiaSpkuZL6pJ0SoN8h0gKSZW8vLWk\nX0t6QdKFNXn3kDQ3l/kNSWplG8zMrLGWBRJJQ4CLgAOAycChkibXyTcCOB64s5D8EnAGcGKdoi8G\njgIm5cfUtVtzMzPrj1YekUwBuiJiQUS8AswEptXJdw5wLil4ABARyyPi1mIagKS3AltGxO0REcB3\ngINb1QAzM+tbKwPJWGBRYbk7p60kaTdgfETc2I8yuxuVWSj7KEmdkjp7enqar7WZmfVLKwNJvbGL\nWLlS2gi4ADhhbZW5SmLEpRFRiYjK6NGj+/ESZmbWH60MJN3A+MLyOGBJYXkEsDNwi6SFwF5AR3XA\nvUGZ4xqUaWZm61grA8lsYJKkiZKGAdOBjurKiHguIkZFxISImADcARwUEZ29FRgRjwPLJO2Vz9b6\nFPDjFrbBzMz6MLRVBUfECknHAbOAIcCMiJgn6WygMyI6Gm2fj1K2BIZJOhjYLyIeAI4B/hPYDLgp\nP8zMrE2UTn5av1Uqlejs7PVAx8zM6pA0JyIaDTcAvrLdzMxKciAxM7NSHEjMzKwUBxIzMyvFgcTM\nzEpxIDEzs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NS+rwf\niaShwJHAR4AxpFvbLiHdUOryiHi1pTU0M7MBrZkbW10FLAXOJN3qFtItbg8Hvgt8oiU1MzOzQaGZ\nQLJ7ROxUk9YN3CHp9y2ok5mZDSLNjJE8K+ljklbmlbSRpE8Az7auamZmNhg0c0QyHfgK8E1J1cAx\nEvh1Xrde+tHdizlv1nyWLH2RMSM346T9d+Lg3ca2u1pmZgNOn4EkIhaSx0EkbU26z/vTLa5XW/3o\n7sWc+sO5vPjqawAsXvoip/5wLoCDiZlZjX6d/hsRzxSDiKR9136V2u+8WfNXBpGqF199jfNmzW9T\njczMBq6y15Fc3milpKmS5kvqknRKg3yHSApJlULaqXm7+ZL2L6QvlDRX0j2SOkvWv64lS1/sV7qZ\n2YasmetIOnpbBWzdYLshwEXAvqSzvGZL6oiIB2ryjQCOB+4spE0mjb+8nXTtyi8k/UlEVA8T3tfK\n7rUxIzdjcZ2gMWbkZq16STOzQauZwfZ3A38DvFCTLmBKg+2mAF0RsQBA0kxgGvBATb5zgHOBEwtp\n04CZEfEy8Iikrlze7U3Ut7ST9t9plTESgM02HsJJ+9eeBW1mZs0EkjuAP0bEb2pXSGo0aDAWWFRY\n7gb+omb73YDxEXGjpBNrtr2jZtvqKHcAN0sK4FsRcWm9F5d0FHAUwHbbbdegmqurDqj7rC0zs741\nc9bWAQ3WvafBpqq3ycqV6bqUC4Aj+rntuyJiiaRtgJ9L+l1E/LZO3S4FLgWoVCpRu74vB+821oHD\nzKwJ/R5sl7R18eLEBrqB8YXlcaQ5uqpGADsDt0haCOwFdOQB9163jYjq36eAG2jcvWZmZi3WVCCR\n9GZJF0r6DWkA/WeSZkga3mCz2cAkSRMlDSMNnq8cuI+I5yJiVERMiIgJpK6sgyKiM+ebLmkTSROB\nScBdkobnwXnya+8H3N/vVpuZ2VrTzFlbI4GfAl+IiOMK6e8Dvizp+8A9EbHKYHxErJB0HDALGALM\niIh5ks4GOiOit7PByPmuJQ3MrwCOjYjXJL0FuEFSte5XR8TP+tlmMzNbixTRePhA0teA2yLieklX\nkbqgngZGAXOBrwNTI+ILra7smqpUKtHZ2ZJLTszM1luS5kREpa98zXRtvTcirs/PXwYOjYh3kqZN\neQa4FXjvGtfUzMwGtWYCySbKfUnAbsC9+fn9pCnmXwc2b0XlzMxs4GvmOpK7gA8AvwAuJl3DcTvw\nTuBbkvYE5rWuimZmNpA1E0j+BbhW0oci4tuSfgTsAJwPbAJcT7pbopmZbYCauSBxgaRjSdd43Ew6\nTfc14EDgYNIZVZ4W18xsA9XMEQkRcaekd5K6uN5BuvL8NuDsiFjRwvqZmdkA11QgAciD6j/PDzMz\nG6DW9R1e+zxrS9KRkk4qLHdLel7SMknHtKxmZmbWb9U7vC5e+iLBG3d4/dHdi1v2ms2c/ns0MKOw\n3BMRWwKjgUNbUiszM1sj7bjDazOBZKOIeKaw/AOAiHgJ8J2ezMwGkHbc4bWZQPKm4kJE/CusnAa+\n1zskmpnZutfbnVxbeYfXZgLJzZL+uU762cDNa7k+ZmZWwkn778RmGw9ZJa3Vd3ht5qytk4Bv59vd\nVqdHeQfQCXy2VRUzM7P+a8cdXpu5IHE5cKikHYC35+QHIuLhltXKzMzW2Lq+w2sz9yPZHxgREdcB\nCwrphwFPRYSvKzEz24A1M0ZyFvCbOum/JI2TmJnZBqyZQLJ5RPTUJkbEE0CjW+2amdkGoJlAsqmk\n1brAJG2MryMxM9vgNRNIfghcJmnl0Ud+fkleZ2ZmG7BmAsnpwJPAo5LmSPpfYCHQk9eZmdkGrJnT\nf1cAp0g6C3hbTu6KiNZdb2+2nlvXs7OatVIzRyRI2pp08eHR+XFkTutru6mS5kvqknRKg3yHSApJ\nlULaqXm7+fkU5H6VaTZQtWN2VrNWamYa+T8D7gf2AH4PPATsCcyV9KcNthsCXAQcAEwmXdQ4uU6+\nEcDxwJ2FtMnAdNIFkFOBb0oa0myZZgNZO2ZnNWulZqZIOQf4+4i4tpgo6aOk+7l/tJftppC6wBbk\n/DOBacADdco/FzixkDYNmBkRLwOP5OlZpuR1zZRpNmC1Y3ZWs1Zqpmtrl9ogAhAR1wM7N9huLLCo\nsNyd01aStBswPiJubHLbPssslH2UpE5JnT09q10GY9Y27Zid1ayVmgkky9dwneqkxcqVaRr6C4AT\n+rFtwzJXSYy4NCIqEVEZPXp0g2qarVvtmJ3VrJWa6draRtI/1kkX6S6JvekGxheWxwFLCssjSEc0\nt0gC2BbokHRQH9s2KtNswGvH7KxmrdRMILmMtNOv59sNtpsNTJI0EVhMGjz/6+rKiHgOGFVdlnQL\ncGJEdEp6Ebha0vnAGGAScBcpePVaptlgsa5nZzVrpWauIzlrTQqOiBWSjgNmAUOAGRExT9LZQGdE\ndDTYdp6ka0mD6CuAYyPiNYB6Za5J/czMbO1QRN0hhjcySF9ssDoi4py1W6W1r1KpRGdnZ7urYWY2\nqEiaExGVvvI107VVb0B9OHAk6Z7tAz6QmJlZ6zTTtfW16vN88eDfA58GZgJf6207MzPbMDRzRIKk\nrYB/BA4DrgR2j4hnW1kxMzMbHJq51e55wF8Bl5IuTnyh5bUyM7NBo5kLEk8gnYJ7OrBE0vP5sUzS\n862tnpmZDXTNjJE0NUOwmZltmBwkzMysFAcSMzMrxYHEzMxKcSAxM7NSHEjMzKwUBxIzMyvFgcTM\nzEpxIDEzs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKaWkgkTRV0nxJ\nXZJOqbP+aElzJd0j6VZJk3P6MElX5HX3Stq7sM0tucx78mObVrbBzMwaa+qe7WtC0hDgImBfoBuY\nLakjIh4oZLs6Ii7J+Q8CzgemAp8DiIhdcqC4SdKeEfF63u6wiOhsVd3NzKx5rTwimQJ0RcSCiHgF\nmAlMK2aIiOKteocDkZ9PBn6Z8zwFLAUqLayrmZmtoVYGkrHAosJyd05bhaRjJT0MnAscn5PvBaZJ\nGippIrAHML6w2RW5W+sMSar34pKOktQpqbOnp2dttMfMzOpoZSCpt4OP1RIiLoqIHYGTgdNz8gxS\n4OkEvg7cBqzI6w6LiF2Ad+fHJ+u9eERcGhGViKiMHj26VEPMzKx3rQwk3ax6FDEOWNIg/0zgYICI\nWBERn4+IXSNiGjASeCivW5z/LgOuJnWhmZlZm7QykMwGJkmaKGkYMB3oKGaQNKmw+CFysJC0uaTh\n+fm+wIqIeCB3dY3K6RsDBwL3t7ANZmbWh5adtRURKyQdB8wChgAzImKepLOBzojoAI6TtA/wKvAs\ncHjefBtglqTXgcW80X21SU7fOJf5C+CyVrXBzMz6pojVhi3WO5VKJTo7fbawmVl/SJoTEX2eMesr\n283MrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NSHEjMzKwUBxIzMyvF\ngcTMzEpxIDEzs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NS\nHEjMzKyUlgYSSVMlzZfUJemUOuuPljRX0j2SbpU0OacPk3RFXnevpL0L2+yR07skfUOSWtkGMzNr\nrGWBRNIQ4CLgAGAycGg1UBRcHRG7RMSuwLnA+Tn9cwARsQuwL/A1SdW6XgwcBUzKj6mtaoOZmfWt\nlUckU4CuiFgQEa8AM4FpxQwR8XxhcTgQ+flk4Jc5z1PAUqAi6a3AlhFxe0QE8B3g4Ba2wczM+tDK\nQDIWWFRY7s5pq5B0rKSHSUckx+fke4FpkoZKmgjsAYzP23f3VWYu9yhJnZI6e3p6SjfGzMzqa2Ug\nqTd2EaslRFwUETsCJwOn5+QZpCDRCXwduA1Y0WyZudxLI6ISEZXRo0evQfXNzKwZQ1tYdjfpKKJq\nHLCkQf6ZpPEPImIF8PnqCkm3AQ8Bz+Zymi3TzMxarJVHJLOBSZImShoGTAc6ihkkTSosfogULJC0\nuaTh+fm+wIqIeCAiHgeWSdorn631KeDHLWyDmZn1oWVHJBGxQtJxwCxgCDAjIuZJOhvojIgO4DhJ\n+wCvko42Ds+bbwPMkvQ6sBj4ZKHoY4D/BDYDbsoPMzNrE6WTn9ZvlUolOjs7210NM7NBRdKciKj0\nlc9XtpuZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiRm\nZlaKA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBi\nZmalOJCYmVkpLQ0kkqZKmi+pS9IpddYfLWmupHsk3Sppck7fWNKVed2Dkk4tbLOwsE1nK+tvZmZ9\nG9qqgiUNAS4C9gW6gdmSOiLigUK2qyPikpz/IOB8YCrwMWCTiNhF0ubAA5KuiYiFebv3RcTTraq7\nmZk1r5VHJFOArohYEBGvADOBacUMEfF8YXE4ENVVwHBJQ4HNgFeAYl4zMxsgWhlIxgKLCsvdOW0V\nko6V9DBwLnB8Tr4OWA48DjwGfDUi/pDXBXCzpDmSjurtxSUdJalTUmdPT0/51piZWV2tDCSqkxar\nJURcFBE7AicDp+fkKcBrwBhgInCCpB3yundFxO7AAcCxkt5T78Uj4tKIqEREZfTo0SWbYmZmvWnZ\nGAnpCGR8YXkcsKRB/pnAxfn5XwM/i4hXgack/Q9QARZExBKAiHhK0g2koPPbRhWZM2fO05IeXbNm\ntM0oYEMbB3KbNwxu8+CxfTOZWhlIZgOTJE0EFgPTSQFiJUmTIuKhvPghoPr8MeD9kr4LbA7sBXxd\n0nBgo4hYlp/vB5zdV0UiYtAdkkjqjIhKu+uxLrnNGwa3ef3TskASESskHQfMAoYAMyJinqSzgc6I\n6ACOk7QP8CrwLHB43vwi4ArgflIX2RURcV/u3rpBUrXuV0fEz1rVBjMz65siVhu2sAFgff8FU4/b\nvGFwm9c/vrJ94Lq03RVoA7d5w+A2r2d8RGJmZqX4iMTMzEpxIDEzs1IcSNpI0khJ10n6XZ6c8p2S\ntpL0c0kP5b9vznkl6Rt5Asz7JO3e7vr3l6TPS5on6X5J10jaVNJESXfm9n5f0rCcd5O83JXXT2hv\n7ZsnaYakpyTdX0jr9+cq6fAkAv8qAAAEoklEQVSc/yFJh9d7rYGgl/ael7/X90m6QdLIwrpTc3vn\nS9q/kN5wkteBpF6bC+tOlBSSRuXlQf8Z9yki/GjTA7gS+Gx+PgwYSZoq5pScdgrwlfz8g8BNpNOh\n9wLubHf9+9nWscAjwGZ5+VrgiPx3ek67BDgmP/874JL8fDrw/Xa3oR9tfQ+wO3B/Ia1fnyuwFbAg\n/31zfv7mdretH+3dDxian3+l0N7JwL3AJqRZKx4mXR4wJD/fIf8v3AtMbnfb+tPmnD6edMnDo8Co\n9eUz7uvhI5I2kbQl6ct4OUBEvBIRS0kTW16Zs10JHJyfTwO+E8kdwEhJb13H1S5rKLBZnoxzc9Jc\nau8nza0Gq7e3+j5cB3xA+QKigS4ifgv8oSa5v5/r/sDPI+IPEfEs8HPSzNgDTr32RsTNEbEiL95B\nmtkCUntnRsTLEfEI0EWanaLPSV4Hkl4+Y4ALgH9i1emgBv1n3BcHkvbZAegBrpB0t6Rv56v13xIR\njwPkv9vk/E1NgjlQRcRi4KukWQseB54D5gBLCzucYptWtjevfw7Yel3WeS3r7+c6qD/vGp8h/SKH\n9bi9SrfCWBwR99asWm/bXOVA0j5DSYfGF0fEbqTZjhv1Czc1CeZAlccEppG6M8aQbhtwQJ2s1TYN\n6vb2Q2/tXC/aL+k0YAXwvWpSnWyDvr1K9006DfhivdV10gZ9m4scSNqnG+iOiDvz8nWkwPJktcsq\n/32qkL8/k2AONPsAj0RET6TJOH8I/CXpML86VU+xTSvbm9e/ifpdCYNFfz/Xwf55kwePDwQOizwo\nwPrb3h1JP5LulbSQVP//lbQt62+bV3IgaZOIeAJYJGmnnPQB4AGggzfmHDsc+HF+3gF8Kp8Bshfw\nXLWrZJB4DNhL0uZ5rKPa3l8Dh+Q8te2tvg+HAL8q7IwGo/5+rrOA/SS9OR/N7ZfTBgVJU0m3hjgo\nIv5YWNUBTM9n5U0EJgF3UZjkNZ+5Nz3nHRQiYm5EbBMREyJiAilI7J7/z9fLz3gV7R7t35AfwK5A\nJ3Af8CPSmRtbA78kzYT8S2CrnFekySwfBuYClXbXfw3aexbwO9JknFeRztzZgbQj6QJ+QLrFMsCm\nebkrr9+h3fXvRzuvIY0DvUraoRy5Jp8raWyhKz8+3e529bO9XaT+/3vy45JC/tNye+cDBxTSPwj8\nPq87rd3t6m+ba9Yv5I2ztgb9Z9zXw1OkmJlZKe7aMjOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NS\nHEjM1oE8G+xVheWhknok3djOepmtDQ4kZuvGcmBnSZvl5X2BxW2sj9la40Bitu7cBHwoPz+UdFEb\nAJLOlHRiYfn+wXQPFtuwOZCYrTszSdODbAr8OXBnH/nNBgUHErN1JCLuAyaQjkZ+2t7amK09Q/vO\nYmZrUQfpvix7s+r9VVaw6g+7TddhncxKcSAxW7dmkGZ/nStp70L6QtKU6+R7ek9c91UzWzPu2jJb\nhyKiOyL+vc6q64GtJN0DHEOaBddsUPDsv2ZmVoqPSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMys\nFAcSMzMrxYHEzMxK+T8Lj5VG/bLAoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8d6e35eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirismooth_ndcg = [0.4055,0.4002,0.4026]\n",
    "dirismooth_mu = [500,1000,1500]\n",
    "\n",
    "plt.scatter(dirismooth_mu, dirismooth_ndcg)\n",
    "plt.xlabel('Mu')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('Dirichlet: NDCG@10 for three different mus')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run the language models again with their optimal parameters on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using jelinek_0.1\n",
      "queries:  1 / 120 \t this took:  3.4456992149353027  seconds in total\n",
      "queries:  2 / 120 \t this took:  14.479824542999268  seconds in total\n",
      "queries:  3 / 120 \t this took:  19.273534297943115  seconds in total\n",
      "queries:  4 / 120 \t this took:  25.48042368888855  seconds in total\n",
      "queries:  5 / 120 \t this took:  39.20231103897095  seconds in total\n",
      "queries:  6 / 120 \t this took:  43.205495834350586  seconds in total\n",
      "queries:  7 / 120 \t this took:  49.64617919921875  seconds in total\n",
      "queries:  8 / 120 \t this took:  57.6219584941864  seconds in total\n",
      "queries:  9 / 120 \t this took:  69.22156572341919  seconds in total\n",
      "queries:  10 / 120 \t this took:  79.5282633304596  seconds in total\n",
      "queries:  11 / 120 \t this took:  83.6898467540741  seconds in total\n",
      "queries:  12 / 120 \t this took:  90.24790596961975  seconds in total\n",
      "queries:  13 / 120 \t this took:  98.53702759742737  seconds in total\n",
      "queries:  14 / 120 \t this took:  103.91424918174744  seconds in total\n",
      "queries:  15 / 120 \t this took:  110.7299976348877  seconds in total\n",
      "queries:  16 / 120 \t this took:  120.11142945289612  seconds in total\n",
      "queries:  17 / 120 \t this took:  123.85513234138489  seconds in total\n",
      "queries:  18 / 120 \t this took:  128.06669282913208  seconds in total\n",
      "queries:  19 / 120 \t this took:  135.64504075050354  seconds in total\n",
      "queries:  20 / 120 \t this took:  159.95468831062317  seconds in total\n",
      "queries:  21 / 120 \t this took:  163.35656452178955  seconds in total\n",
      "queries:  22 / 120 \t this took:  175.62539887428284  seconds in total\n",
      "queries:  23 / 120 \t this took:  178.86072492599487  seconds in total\n",
      "queries:  24 / 120 \t this took:  201.47148442268372  seconds in total\n",
      "queries:  25 / 120 \t this took:  217.8716835975647  seconds in total\n",
      "queries:  26 / 120 \t this took:  226.70636463165283  seconds in total\n",
      "queries:  27 / 120 \t this took:  230.52324056625366  seconds in total\n",
      "queries:  28 / 120 \t this took:  237.08580565452576  seconds in total\n",
      "queries:  29 / 120 \t this took:  248.40563011169434  seconds in total\n",
      "queries:  30 / 120 \t this took:  259.81474447250366  seconds in total\n",
      "queries:  31 / 120 \t this took:  284.39632177352905  seconds in total\n",
      "queries:  32 / 120 \t this took:  294.50281715393066  seconds in total\n",
      "queries:  33 / 120 \t this took:  322.14851117134094  seconds in total\n",
      "queries:  34 / 120 \t this took:  329.67169523239136  seconds in total\n",
      "queries:  35 / 120 \t this took:  333.4220585823059  seconds in total\n",
      "queries:  36 / 120 \t this took:  339.30406427383423  seconds in total\n",
      "queries:  37 / 120 \t this took:  344.859721660614  seconds in total\n",
      "queries:  38 / 120 \t this took:  362.8452425003052  seconds in total\n",
      "queries:  39 / 120 \t this took:  401.644406080246  seconds in total\n",
      "queries:  40 / 120 \t this took:  435.22553157806396  seconds in total\n",
      "queries:  41 / 120 \t this took:  443.44743251800537  seconds in total\n",
      "queries:  42 / 120 \t this took:  470.0936071872711  seconds in total\n",
      "queries:  43 / 120 \t this took:  494.1816532611847  seconds in total\n",
      "queries:  44 / 120 \t this took:  502.80471181869507  seconds in total\n",
      "queries:  45 / 120 \t this took:  508.853089094162  seconds in total\n",
      "queries:  46 / 120 \t this took:  519.8710634708405  seconds in total\n",
      "queries:  47 / 120 \t this took:  584.9680967330933  seconds in total\n",
      "queries:  48 / 120 \t this took:  588.6572904586792  seconds in total\n",
      "queries:  49 / 120 \t this took:  654.9050924777985  seconds in total\n",
      "queries:  50 / 120 \t this took:  680.317140340805  seconds in total\n",
      "queries:  51 / 120 \t this took:  686.2523415088654  seconds in total\n",
      "queries:  52 / 120 \t this took:  704.6433820724487  seconds in total\n",
      "queries:  53 / 120 \t this took:  714.1457967758179  seconds in total\n",
      "queries:  54 / 120 \t this took:  726.4681718349457  seconds in total\n",
      "queries:  55 / 120 \t this took:  734.7791674137115  seconds in total\n",
      "queries:  56 / 120 \t this took:  829.238659620285  seconds in total\n",
      "queries:  57 / 120 \t this took:  834.8584668636322  seconds in total\n",
      "queries:  58 / 120 \t this took:  878.6074545383453  seconds in total\n",
      "queries:  59 / 120 \t this took:  887.2319524288177  seconds in total\n",
      "queries:  60 / 120 \t this took:  929.0562036037445  seconds in total\n",
      "queries:  61 / 120 \t this took:  958.5593867301941  seconds in total\n",
      "queries:  62 / 120 \t this took:  973.188734292984  seconds in total\n",
      "queries:  63 / 120 \t this took:  986.927371263504  seconds in total\n",
      "queries:  64 / 120 \t this took:  1003.8167352676392  seconds in total\n",
      "queries:  65 / 120 \t this took:  1008.0421886444092  seconds in total\n",
      "queries:  66 / 120 \t this took:  1012.5748670101166  seconds in total\n",
      "queries:  67 / 120 \t this took:  1019.8398761749268  seconds in total\n",
      "queries:  68 / 120 \t this took:  1024.1350955963135  seconds in total\n",
      "queries:  69 / 120 \t this took:  1044.6413877010345  seconds in total\n",
      "queries:  70 / 120 \t this took:  1057.735766172409  seconds in total\n",
      "queries:  71 / 120 \t this took:  1081.3225729465485  seconds in total\n",
      "queries:  72 / 120 \t this took:  1095.7233564853668  seconds in total\n",
      "queries:  73 / 120 \t this took:  1114.032982826233  seconds in total\n",
      "queries:  74 / 120 \t this took:  1182.7510645389557  seconds in total\n",
      "queries:  75 / 120 \t this took:  1191.7769882678986  seconds in total\n",
      "queries:  76 / 120 \t this took:  1230.8962452411652  seconds in total\n",
      "queries:  77 / 120 \t this took:  1245.6990287303925  seconds in total\n",
      "queries:  78 / 120 \t this took:  1251.350082397461  seconds in total\n",
      "queries:  79 / 120 \t this took:  1256.4679107666016  seconds in total\n",
      "queries:  80 / 120 \t this took:  1290.5652401447296  seconds in total\n",
      "queries:  81 / 120 \t this took:  1315.6804840564728  seconds in total\n",
      "queries:  82 / 120 \t this took:  1350.7343854904175  seconds in total\n",
      "queries:  83 / 120 \t this took:  1355.756949186325  seconds in total\n",
      "queries:  84 / 120 \t this took:  1376.4642124176025  seconds in total\n",
      "queries:  85 / 120 \t this took:  1380.6316134929657  seconds in total\n",
      "queries:  86 / 120 \t this took:  1390.624900341034  seconds in total\n",
      "queries:  87 / 120 \t this took:  1400.7378633022308  seconds in total\n",
      "queries:  88 / 120 \t this took:  1404.7997076511383  seconds in total\n",
      "queries:  89 / 120 \t this took:  1408.5880074501038  seconds in total\n",
      "queries:  90 / 120 \t this took:  1414.3159363269806  seconds in total\n",
      "queries:  91 / 120 \t this took:  1423.578139781952  seconds in total\n",
      "queries:  92 / 120 \t this took:  1428.2209508419037  seconds in total\n",
      "queries:  93 / 120 \t this took:  1432.0080513954163  seconds in total\n",
      "queries:  94 / 120 \t this took:  1439.2509417533875  seconds in total\n",
      "queries:  95 / 120 \t this took:  1447.7406311035156  seconds in total\n",
      "queries:  96 / 120 \t this took:  1466.9148869514465  seconds in total\n",
      "queries:  97 / 120 \t this took:  1470.8915960788727  seconds in total\n",
      "queries:  98 / 120 \t this took:  1485.4764730930328  seconds in total\n",
      "queries:  99 / 120 \t this took:  1511.6095232963562  seconds in total\n",
      "queries:  100 / 120 \t this took:  1539.0308678150177  seconds in total\n",
      "queries:  101 / 120 \t this took:  1542.5796625614166  seconds in total\n",
      "queries:  102 / 120 \t this took:  1565.9027581214905  seconds in total\n",
      "queries:  103 / 120 \t this took:  1627.2176191806793  seconds in total\n",
      "queries:  104 / 120 \t this took:  1632.479739665985  seconds in total\n",
      "queries:  105 / 120 \t this took:  1644.5248851776123  seconds in total\n",
      "queries:  106 / 120 \t this took:  1663.2593641281128  seconds in total\n",
      "queries:  107 / 120 \t this took:  1709.1624479293823  seconds in total\n",
      "queries:  108 / 120 \t this took:  1716.4242975711823  seconds in total\n",
      "queries:  109 / 120 \t this took:  1719.7996485233307  seconds in total\n",
      "queries:  110 / 120 \t this took:  1728.2457296848297  seconds in total\n",
      "queries:  111 / 120 \t this took:  1735.412833929062  seconds in total\n",
      "queries:  112 / 120 \t this took:  1750.1855030059814  seconds in total\n",
      "queries:  113 / 120 \t this took:  1756.765943288803  seconds in total\n",
      "queries:  114 / 120 \t this took:  1771.908143043518  seconds in total\n",
      "queries:  115 / 120 \t this took:  1793.0913949012756  seconds in total\n",
      "queries:  116 / 120 \t this took:  1845.747866153717  seconds in total\n",
      "queries:  117 / 120 \t this took:  1868.1669030189514  seconds in total\n",
      "queries:  118 / 120 \t this took:  1872.5714583396912  seconds in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries:  119 / 120 \t this took:  1876.2561070919037  seconds in total\n",
      "queries:  120 / 120 \t this took:  1911.3647282123566  seconds in total\n",
      "Retrieving using dirichlet_500\n",
      "queries:  1 / 120 \t this took:  3.2620725631713867  seconds in total\n",
      "queries:  2 / 120 \t this took:  7.664332628250122  seconds in total\n",
      "queries:  3 / 120 \t this took:  11.58545732498169  seconds in total\n",
      "queries:  4 / 120 \t this took:  15.552948951721191  seconds in total\n",
      "queries:  5 / 120 \t this took:  20.389599800109863  seconds in total\n",
      "queries:  6 / 120 \t this took:  23.867961168289185  seconds in total\n",
      "queries:  7 / 120 \t this took:  27.693176746368408  seconds in total\n",
      "queries:  8 / 120 \t this took:  31.65121364593506  seconds in total\n",
      "queries:  9 / 120 \t this took:  36.19721007347107  seconds in total\n",
      "queries:  10 / 120 \t this took:  40.35960102081299  seconds in total\n",
      "queries:  11 / 120 \t this took:  43.68974018096924  seconds in total\n",
      "queries:  12 / 120 \t this took:  47.55888366699219  seconds in total\n",
      "queries:  13 / 120 \t this took:  51.59917211532593  seconds in total\n",
      "queries:  14 / 120 \t this took:  55.29118275642395  seconds in total\n",
      "queries:  15 / 120 \t this took:  59.23661971092224  seconds in total\n",
      "queries:  16 / 120 \t this took:  63.442522048950195  seconds in total\n",
      "queries:  17 / 120 \t this took:  66.632319688797  seconds in total\n",
      "queries:  18 / 120 \t this took:  70.10294389724731  seconds in total\n",
      "queries:  19 / 120 \t this took:  74.13625526428223  seconds in total\n",
      "queries:  20 / 120 \t this took:  79.22546315193176  seconds in total\n",
      "queries:  21 / 120 \t this took:  82.31294441223145  seconds in total\n",
      "queries:  22 / 120 \t this took:  86.80797004699707  seconds in total\n",
      "queries:  23 / 120 \t this took:  89.91062092781067  seconds in total\n",
      "queries:  24 / 120 \t this took:  94.81529808044434  seconds in total\n",
      "queries:  25 / 120 \t this took:  99.47033643722534  seconds in total\n",
      "queries:  26 / 120 \t this took:  103.72675704956055  seconds in total\n",
      "queries:  27 / 120 \t this took:  107.00089049339294  seconds in total\n",
      "queries:  28 / 120 \t this took:  110.85274291038513  seconds in total\n",
      "queries:  29 / 120 \t this took:  115.36932754516602  seconds in total\n",
      "queries:  30 / 120 \t this took:  119.5478310585022  seconds in total\n",
      "queries:  31 / 120 \t this took:  126.31651496887207  seconds in total\n",
      "queries:  32 / 120 \t this took:  130.98520064353943  seconds in total\n",
      "queries:  33 / 120 \t this took:  136.61500716209412  seconds in total\n",
      "queries:  34 / 120 \t this took:  140.71592497825623  seconds in total\n",
      "queries:  35 / 120 \t this took:  144.4279601573944  seconds in total\n",
      "queries:  36 / 120 \t this took:  148.45999264717102  seconds in total\n",
      "queries:  37 / 120 \t this took:  152.39783358573914  seconds in total\n",
      "queries:  38 / 120 \t this took:  157.6699676513672  seconds in total\n",
      "queries:  39 / 120 \t this took:  164.43709683418274  seconds in total\n",
      "queries:  40 / 120 \t this took:  171.74605536460876  seconds in total\n",
      "queries:  41 / 120 \t this took:  176.24307489395142  seconds in total\n",
      "queries:  42 / 120 \t this took:  182.17731952667236  seconds in total\n",
      "queries:  43 / 120 \t this took:  188.6820306777954  seconds in total\n",
      "queries:  44 / 120 \t this took:  194.07125568389893  seconds in total\n",
      "queries:  45 / 120 \t this took:  197.99720168113708  seconds in total\n",
      "queries:  46 / 120 \t this took:  202.2607147693634  seconds in total\n",
      "queries:  47 / 120 \t this took:  208.69578504562378  seconds in total\n",
      "queries:  48 / 120 \t this took:  212.18512058258057  seconds in total\n",
      "queries:  49 / 120 \t this took:  218.79948616027832  seconds in total\n",
      "queries:  50 / 120 \t this took:  224.4564082622528  seconds in total\n",
      "queries:  51 / 120 \t this took:  228.37654399871826  seconds in total\n",
      "queries:  52 / 120 \t this took:  234.10460114479065  seconds in total\n",
      "queries:  53 / 120 \t this took:  238.3766610622406  seconds in total\n",
      "queries:  54 / 120 \t this took:  242.95770168304443  seconds in total\n",
      "queries:  55 / 120 \t this took:  247.46793293952942  seconds in total\n",
      "queries:  56 / 120 \t this took:  256.05725717544556  seconds in total\n",
      "queries:  57 / 120 \t this took:  260.08871960639954  seconds in total\n",
      "queries:  58 / 120 \t this took:  266.11038422584534  seconds in total\n",
      "queries:  59 / 120 \t this took:  270.3665883541107  seconds in total\n",
      "queries:  60 / 120 \t this took:  276.69580149650574  seconds in total\n",
      "queries:  61 / 120 \t this took:  282.14567589759827  seconds in total\n",
      "queries:  62 / 120 \t this took:  287.01157093048096  seconds in total\n",
      "queries:  63 / 120 \t this took:  292.14823150634766  seconds in total\n",
      "queries:  64 / 120 \t this took:  297.2892575263977  seconds in total\n",
      "queries:  65 / 120 \t this took:  301.00827717781067  seconds in total\n",
      "queries:  66 / 120 \t this took:  304.6969258785248  seconds in total\n",
      "queries:  67 / 120 \t this took:  308.8715829849243  seconds in total\n",
      "queries:  68 / 120 \t this took:  312.8008496761322  seconds in total\n",
      "queries:  69 / 120 \t this took:  318.48640298843384  seconds in total\n",
      "queries:  70 / 120 \t this took:  323.32829785346985  seconds in total\n",
      "queries:  71 / 120 \t this took:  329.28704810142517  seconds in total\n",
      "queries:  72 / 120 \t this took:  334.7508397102356  seconds in total\n",
      "queries:  73 / 120 \t this took:  340.5574562549591  seconds in total\n",
      "queries:  74 / 120 \t this took:  348.3623785972595  seconds in total\n",
      "queries:  75 / 120 \t this took:  352.84573888778687  seconds in total\n",
      "queries:  76 / 120 \t this took:  358.95669078826904  seconds in total\n",
      "queries:  77 / 120 \t this took:  364.02975940704346  seconds in total\n",
      "queries:  78 / 120 \t this took:  368.13178873062134  seconds in total\n",
      "queries:  79 / 120 \t this took:  372.3091456890106  seconds in total\n",
      "queries:  80 / 120 \t this took:  379.3753595352173  seconds in total\n",
      "queries:  81 / 120 \t this took:  385.6603503227234  seconds in total\n",
      "queries:  82 / 120 \t this took:  391.856600522995  seconds in total\n",
      "queries:  83 / 120 \t this took:  395.6011371612549  seconds in total\n",
      "queries:  84 / 120 \t this took:  401.2127995491028  seconds in total\n",
      "queries:  85 / 120 \t this took:  404.9696731567383  seconds in total\n",
      "queries:  86 / 120 \t this took:  409.590185880661  seconds in total\n",
      "queries:  87 / 120 \t this took:  414.7492744922638  seconds in total\n",
      "queries:  88 / 120 \t this took:  419.0554804801941  seconds in total\n",
      "queries:  89 / 120 \t this took:  423.2900972366333  seconds in total\n",
      "queries:  90 / 120 \t this took:  427.5021176338196  seconds in total\n",
      "queries:  91 / 120 \t this took:  432.007821559906  seconds in total\n",
      "queries:  92 / 120 \t this took:  435.79529309272766  seconds in total\n",
      "queries:  93 / 120 \t this took:  439.2729847431183  seconds in total\n",
      "queries:  94 / 120 \t this took:  443.5197160243988  seconds in total\n",
      "queries:  95 / 120 \t this took:  448.1269762516022  seconds in total\n",
      "queries:  96 / 120 \t this took:  453.6827735900879  seconds in total\n",
      "queries:  97 / 120 \t this took:  457.33928990364075  seconds in total\n",
      "queries:  98 / 120 \t this took:  462.5141975879669  seconds in total\n",
      "queries:  99 / 120 \t this took:  468.73354029655457  seconds in total\n",
      "queries:  100 / 120 \t this took:  474.3498842716217  seconds in total\n",
      "queries:  101 / 120 \t this took:  478.0272641181946  seconds in total\n",
      "queries:  102 / 120 \t this took:  483.5182044506073  seconds in total\n",
      "queries:  103 / 120 \t this took:  491.36178636550903  seconds in total\n",
      "queries:  104 / 120 \t this took:  495.270140171051  seconds in total\n",
      "queries:  105 / 120 \t this took:  500.0422487258911  seconds in total\n",
      "queries:  106 / 120 \t this took:  505.56750893592834  seconds in total\n",
      "queries:  107 / 120 \t this took:  512.5952966213226  seconds in total\n",
      "queries:  108 / 120 \t this took:  517.3906466960907  seconds in total\n",
      "queries:  109 / 120 \t this took:  520.799957036972  seconds in total\n",
      "queries:  110 / 120 \t this took:  525.0927178859711  seconds in total\n",
      "queries:  111 / 120 \t this took:  529.3942012786865  seconds in total\n",
      "queries:  112 / 120 \t this took:  534.3537902832031  seconds in total\n",
      "queries:  113 / 120 \t this took:  538.5460891723633  seconds in total\n",
      "queries:  114 / 120 \t this took:  543.7568919658661  seconds in total\n",
      "queries:  115 / 120 \t this took:  549.2699201107025  seconds in total\n",
      "queries:  116 / 120 \t this took:  557.9070949554443  seconds in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries:  117 / 120 \t this took:  564.2083096504211  seconds in total\n",
      "queries:  118 / 120 \t this took:  568.1786298751831  seconds in total\n",
      "queries:  119 / 120 \t this took:  571.8173744678497  seconds in total\n",
      "queries:  120 / 120 \t this took:  578.4491362571716  seconds in total\n",
      "Retrieving using absolute_0.5\n",
      "queries:  1 / 120 \t this took:  3.732961893081665  seconds in total\n",
      "queries:  2 / 120 \t this took:  9.693864107131958  seconds in total\n",
      "queries:  3 / 120 \t this took:  14.155230522155762  seconds in total\n",
      "queries:  4 / 120 \t this took:  18.617367267608643  seconds in total\n",
      "queries:  5 / 120 \t this took:  25.983790397644043  seconds in total\n",
      "queries:  6 / 120 \t this took:  29.767910718917847  seconds in total\n",
      "queries:  7 / 120 \t this took:  34.96047830581665  seconds in total\n",
      "queries:  8 / 120 \t this took:  40.18694877624512  seconds in total\n",
      "queries:  9 / 120 \t this took:  47.098936319351196  seconds in total\n",
      "queries:  10 / 120 \t this took:  52.96819853782654  seconds in total\n",
      "queries:  11 / 120 \t this took:  56.62206196784973  seconds in total\n",
      "queries:  12 / 120 \t this took:  61.71388840675354  seconds in total\n",
      "queries:  13 / 120 \t this took:  67.67885327339172  seconds in total\n",
      "queries:  14 / 120 \t this took:  72.89863348007202  seconds in total\n",
      "queries:  15 / 120 \t this took:  78.89156031608582  seconds in total\n",
      "queries:  16 / 120 \t this took:  86.31146907806396  seconds in total\n",
      "queries:  17 / 120 \t this took:  90.21832036972046  seconds in total\n",
      "queries:  18 / 120 \t this took:  94.6364324092865  seconds in total\n",
      "queries:  19 / 120 \t this took:  100.8125364780426  seconds in total\n",
      "queries:  20 / 120 \t this took:  110.0541570186615  seconds in total\n",
      "queries:  21 / 120 \t this took:  113.46774768829346  seconds in total\n",
      "queries:  22 / 120 \t this took:  121.22161722183228  seconds in total\n",
      "queries:  23 / 120 \t this took:  124.68938612937927  seconds in total\n",
      "queries:  24 / 120 \t this took:  133.02853798866272  seconds in total\n",
      "queries:  25 / 120 \t this took:  142.27952408790588  seconds in total\n",
      "queries:  26 / 120 \t this took:  149.7208330631256  seconds in total\n",
      "queries:  27 / 120 \t this took:  153.74459671974182  seconds in total\n",
      "queries:  28 / 120 \t this took:  159.51306891441345  seconds in total\n",
      "queries:  29 / 120 \t this took:  168.14206767082214  seconds in total\n",
      "queries:  30 / 120 \t this took:  174.88633346557617  seconds in total\n",
      "queries:  31 / 120 \t this took:  186.89499163627625  seconds in total\n",
      "queries:  32 / 120 \t this took:  193.92887568473816  seconds in total\n",
      "queries:  33 / 120 \t this took:  206.7997932434082  seconds in total\n",
      "queries:  34 / 120 \t this took:  213.1436505317688  seconds in total\n",
      "queries:  35 / 120 \t this took:  217.2438769340515  seconds in total\n",
      "queries:  36 / 120 \t this took:  222.59769916534424  seconds in total\n",
      "queries:  37 / 120 \t this took:  227.6296091079712  seconds in total\n",
      "queries:  38 / 120 \t this took:  236.69353938102722  seconds in total\n",
      "queries:  39 / 120 \t this took:  250.35100650787354  seconds in total\n",
      "queries:  40 / 120 \t this took:  263.00033617019653  seconds in total\n",
      "queries:  41 / 120 \t this took:  268.92446875572205  seconds in total\n",
      "queries:  42 / 120 \t this took:  276.6855149269104  seconds in total\n",
      "queries:  43 / 120 \t this took:  287.37203431129456  seconds in total\n",
      "queries:  44 / 120 \t this took:  293.7975504398346  seconds in total\n",
      "queries:  45 / 120 \t this took:  299.23009395599365  seconds in total\n",
      "queries:  46 / 120 \t this took:  305.9839520454407  seconds in total\n",
      "queries:  47 / 120 \t this took:  321.756121635437  seconds in total\n",
      "queries:  48 / 120 \t this took:  325.8669946193695  seconds in total\n",
      "queries:  49 / 120 \t this took:  340.95865535736084  seconds in total\n",
      "queries:  50 / 120 \t this took:  350.978821516037  seconds in total\n",
      "queries:  51 / 120 \t this took:  356.57857871055603  seconds in total\n",
      "queries:  52 / 120 \t this took:  367.645231962204  seconds in total\n",
      "queries:  53 / 120 \t this took:  374.20080518722534  seconds in total\n",
      "queries:  54 / 120 \t this took:  381.4548614025116  seconds in total\n",
      "queries:  55 / 120 \t this took:  387.2479703426361  seconds in total\n",
      "queries:  56 / 120 \t this took:  408.2339732646942  seconds in total\n",
      "queries:  57 / 120 \t this took:  413.56925225257874  seconds in total\n",
      "queries:  58 / 120 \t this took:  426.3073699474335  seconds in total\n",
      "queries:  59 / 120 \t this took:  433.0776786804199  seconds in total\n",
      "queries:  60 / 120 \t this took:  450.5515172481537  seconds in total\n",
      "queries:  61 / 120 \t this took:  461.76133584976196  seconds in total\n",
      "queries:  62 / 120 \t this took:  470.03517413139343  seconds in total\n",
      "queries:  63 / 120 \t this took:  479.4332492351532  seconds in total\n",
      "queries:  64 / 120 \t this took:  489.2237403392792  seconds in total\n",
      "queries:  65 / 120 \t this took:  493.63573002815247  seconds in total\n",
      "queries:  66 / 120 \t this took:  498.3273763656616  seconds in total\n",
      "queries:  67 / 120 \t this took:  504.4401741027832  seconds in total\n",
      "queries:  68 / 120 \t this took:  508.95731496810913  seconds in total\n",
      "queries:  69 / 120 \t this took:  519.5640659332275  seconds in total\n",
      "queries:  70 / 120 \t this took:  528.3030245304108  seconds in total\n",
      "queries:  71 / 120 \t this took:  540.4876770973206  seconds in total\n",
      "queries:  72 / 120 \t this took:  549.1440424919128  seconds in total\n",
      "queries:  73 / 120 \t this took:  559.5580127239227  seconds in total\n",
      "queries:  74 / 120 \t this took:  578.7681238651276  seconds in total\n",
      "queries:  75 / 120 \t this took:  585.2324945926666  seconds in total\n",
      "queries:  76 / 120 \t this took:  597.5892486572266  seconds in total\n",
      "queries:  77 / 120 \t this took:  606.0775442123413  seconds in total\n",
      "queries:  78 / 120 \t this took:  611.0320246219635  seconds in total\n",
      "queries:  79 / 120 \t this took:  615.5910425186157  seconds in total\n",
      "queries:  80 / 120 \t this took:  627.4002923965454  seconds in total\n",
      "queries:  81 / 120 \t this took:  639.7662482261658  seconds in total\n",
      "queries:  82 / 120 \t this took:  655.0927727222443  seconds in total\n",
      "queries:  83 / 120 \t this took:  660.3661594390869  seconds in total\n",
      "queries:  84 / 120 \t this took:  670.8969411849976  seconds in total\n",
      "queries:  85 / 120 \t this took:  675.2327964305878  seconds in total\n",
      "queries:  86 / 120 \t this took:  681.9585514068604  seconds in total\n",
      "queries:  87 / 120 \t this took:  689.6128239631653  seconds in total\n",
      "queries:  88 / 120 \t this took:  693.459255695343  seconds in total\n",
      "queries:  89 / 120 \t this took:  697.3316280841827  seconds in total\n",
      "queries:  90 / 120 \t this took:  702.3645751476288  seconds in total\n",
      "queries:  91 / 120 \t this took:  708.5460364818573  seconds in total\n",
      "queries:  92 / 120 \t this took:  712.6992287635803  seconds in total\n",
      "queries:  93 / 120 \t this took:  716.6479613780975  seconds in total\n",
      "queries:  94 / 120 \t this took:  722.6637597084045  seconds in total\n",
      "queries:  95 / 120 \t this took:  728.6258900165558  seconds in total\n",
      "queries:  96 / 120 \t this took:  739.0640466213226  seconds in total\n",
      "queries:  97 / 120 \t this took:  742.7674231529236  seconds in total\n",
      "queries:  98 / 120 \t this took:  750.9020483493805  seconds in total\n",
      "queries:  99 / 120 \t this took:  760.3191344738007  seconds in total\n",
      "queries:  100 / 120 \t this took:  769.6486859321594  seconds in total\n",
      "queries:  101 / 120 \t this took:  773.372967004776  seconds in total\n",
      "queries:  102 / 120 \t this took:  783.1026809215546  seconds in total\n",
      "queries:  103 / 120 \t this took:  803.7650084495544  seconds in total\n",
      "queries:  104 / 120 \t this took:  808.4743752479553  seconds in total\n",
      "queries:  105 / 120 \t this took:  815.9729034900665  seconds in total\n",
      "queries:  106 / 120 \t this took:  824.9270288944244  seconds in total\n",
      "queries:  107 / 120 \t this took:  839.7159855365753  seconds in total\n",
      "queries:  108 / 120 \t this took:  845.628927230835  seconds in total\n",
      "queries:  109 / 120 \t this took:  849.6664516925812  seconds in total\n",
      "queries:  110 / 120 \t this took:  855.2926652431488  seconds in total\n",
      "queries:  111 / 120 \t this took:  861.0017454624176  seconds in total\n",
      "queries:  112 / 120 \t this took:  870.4348232746124  seconds in total\n",
      "queries:  113 / 120 \t this took:  875.4196133613586  seconds in total\n",
      "queries:  114 / 120 \t this took:  882.811826467514  seconds in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries:  115 / 120 \t this took:  891.9328374862671  seconds in total\n",
      "queries:  116 / 120 \t this took:  910.6588609218597  seconds in total\n",
      "queries:  117 / 120 \t this took:  922.196252822876  seconds in total\n",
      "queries:  118 / 120 \t this took:  926.782007932663  seconds in total\n",
      "queries:  119 / 120 \t this took:  930.5165557861328  seconds in total\n",
      "queries:  120 / 120 \t this took:  944.747136592865  seconds in total\n",
      "         832268763 function calls in 3464.042 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000 3464.039 1732.019 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2832(run_code)\n",
      "        2    0.001    0.001 3464.038 1732.019 {built-in method builtins.exec}\n",
      "        1    2.693    2.693 3464.037 3464.037 <ipython-input-160-e1324048017e>:16(<module>)\n",
      "        3  467.657  155.886 3461.344 1153.781 <ipython-input-159-0a2890c56f74>:3(run_retrieval)\n",
      "  8631249   32.282    0.000 2044.767    0.000 <ipython-input-133-1d6e3bc7d6d1>:21(log_multinomial_query_language_model)\n",
      "  8631249  153.429    0.000 1789.187    0.000 <ipython-input-133-1d6e3bc7d6d1>:29(<listcomp>)\n",
      " 88701760 1323.689    0.000 1323.689    0.000 {method 'document' of 'pyndri.Index' objects}\n",
      " 14723420   75.980    0.000 1298.708    0.000 <ipython-input-130-0d73b906d01c>:147(jelinek_mercer)\n",
      " 14723420  986.111    0.000  986.111    0.000 {built-in method builtins.sum}\n",
      " 14723420   74.209    0.000  316.055    0.000 <ipython-input-130-0d73b906d01c>:114(absolute_discounting)\n",
      "  8631249   41.498    0.000  131.833    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:1778(sum)\n",
      "  8631249   31.178    0.000   91.465    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/collections/__init__.py:517(__init__)\n",
      "  8631249    4.325    0.000   87.419    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:31(_sum)\n",
      "  8631249   83.093    0.000   83.093    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "  8631249   14.207    0.000   59.440    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/collections/__init__.py:586(update)\n",
      " 17628270   11.404    0.000   39.413    0.000 {built-in method builtins.isinstance}\n",
      " 44170260   19.717    0.000   30.873    0.000 <ipython-input-31-ae80e85952ef>:5(term_frequency)\n",
      "  8631249   16.618    0.000   28.009    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/abc.py:178(__instancecheck__)\n",
      "        3    0.885    0.295   26.771    8.924 <ipython-input-18-51cfbbf3f59d>:5(write_run)\n",
      "288314805   25.075    0.000   25.075    0.000 {method 'keys' of 'dict' objects}\n",
      "      360   24.972    0.069   24.972    0.069 {built-in method builtins.sorted}\n",
      " 14723420   12.415    0.000   20.995    0.000 <ipython-input-130-0d73b906d01c>:130(dirichlet_prior_smoothing)\n",
      "105964618   14.341    0.000   14.341    0.000 {built-in method builtins.len}\n",
      " 17262498   11.391    0.000   11.391    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/_weakrefset.py:70(__contains__)\n",
      " 44170260   11.157    0.000   11.157    0.000 {method 'get' of 'dict' objects}\n",
      "  8631249    8.075    0.000    8.075    0.000 {built-in method _collections._count_elements}\n",
      " 14723420    7.492    0.000    7.492    0.000 {built-in method builtins.max}\n",
      " 59254560    6.963    0.000    6.963    0.000 {method 'append' of 'list' objects}\n",
      " 14723420    2.179    0.000    2.179    0.000 {method 'values' of 'dict' objects}\n",
      "   360003    0.707    0.000    0.707    0.000 {method 'format' of 'str' objects}\n",
      "   360000    0.149    0.000    0.149    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      "      363    0.008    0.000    0.144    0.000 {built-in method builtins.print}\n",
      "     5052    0.013    0.000    0.136    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:342(write)\n",
      "     5415    0.052    0.000    0.117    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:180(schedule)\n",
      "     5415    0.048    0.000    0.048    0.000 {built-in method posix.urandom}\n",
      "     5415    0.006    0.000    0.013    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/threading.py:1104(is_alive)\n",
      "     5052    0.002    0.000    0.007    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:297(_schedule_flush)\n",
      "     5415    0.003    0.000    0.006    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/threading.py:1062(_wait_for_tstate_lock)\n",
      "     5052    0.003    0.000    0.004    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:284(_is_master_process)\n",
      "     5415    0.003    0.000    0.003    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.003    0.002 /home/jasper-ubuntu/miniconda3/lib/python3.6/codeop.py:132(__call__)\n",
      "        2    0.003    0.002    0.003    0.002 {built-in method builtins.compile}\n",
      "     5415    0.003    0.000    0.003    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py:87(_event_pipe)\n",
      "     5052    0.001    0.000    0.001    0.000 {built-in method posix.getpid}\n",
      "     5415    0.001    0.000    0.001    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/threading.py:506(is_set)\n",
      "        3    0.001    0.000    0.001    0.000 {built-in method io.open}\n",
      "      360    0.001    0.000    0.001    0.000 {method 'document_base' of 'pyndri.Index' objects}\n",
      "      363    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "      360    0.000    0.000    0.000    0.000 {method 'maximum_document' of 'pyndri.Index' objects}\n",
      "        3    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/genericpath.py:16(exists)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
      "        3    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/_bootlocale.py:23(getpreferredencoding)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
      "        3    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/codecs.py:185(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-160-e1324048017e>:21(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/hooks.py:142(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/utils/ipstruct.py:125(__getattr__)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/hooks.py:207(pre_run_code_hook)\n",
      "        2    0.000    0.000    0.000    0.000 /home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:1055(user_global_ns)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run run_retrieval\n",
    "\n",
    "'''\n",
    "Jelinek-Mercer (explore different values of  in the range [0.1, 0.5, 0.9]). [5 points]\n",
    "Dirichlet Prior (explore different values of  [500, 1000, 1500]). [5 points]\n",
    "Absolute discounting (explore different values of  in the range [0.1, 0.5, 0.9]). [5 points]\n",
    "'''\n",
    "\n",
    "best_smoothings_functions = [jelinek_01, dirichlet_500, absolute_05]\n",
    "\n",
    "import cProfile, pstats\n",
    "from io import StringIO\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "for smoothing_func, label in best_smoothings_functions:\n",
    "    run_retrieval(label, \n",
    "                  functools.partial(log_multinomial_query_language_model, word_probability_func=smoothing_func), \n",
    "                  \"Probabilistic\", test_queries_ids)\n",
    "\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Positional based language model (PLM)\n",
    "The PLM only has to be run on the top 1000 documents per query (ranked by the TF-IDF). We run it for all values of mu on the test queries instead of just for the mu of 1500. We do this to see if there might be a difference for this model or maybe if a subset is used. If a difference is found, we run the the other language models again, but just on these top-1000 queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using PLM_PASSAGE_500\n",
      "[5872, 3066]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-cff2ab7a7916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     run_retrieval(label, \n\u001b[1;32m     31\u001b[0m                   \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_multinomial_query_language_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_probability_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoothing_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                   \"Probabilistic\", test_queries_ids)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-165-4b91ae9bbf9c>\u001b[0m in \u001b[0;36mrun_retrieval\u001b[0;34m(model_name, score_fn, model_type, query_set)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Probabilistic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_doc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-1d6e3bc7d6d1>\u001b[0m in \u001b[0;36mlog_multinomial_query_language_model\u001b[0;34m(query_term_ids, document_id, word_probability_func)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# sum_over_words_in_query( tf(w;d) * log( p(w|d) ) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     return np.sum([counts[term_id]*np.log(word_probability_func(document_id, term_id)) \n\u001b[0;32m---> 30\u001b[0;31m                    for term_id in query_term_ids])\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkl_divergenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_term_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_probability_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munigram_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-1d6e3bc7d6d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# sum_over_words_in_query( tf(w;d) * log( p(w|d) ) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     return np.sum([counts[term_id]*np.log(word_probability_func(document_id, term_id)) \n\u001b[0;32m---> 30\u001b[0;31m                    for term_id in query_term_ids])\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkl_divergenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_term_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_probability_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munigram_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-0d73b906d01c>\u001b[0m in \u001b[0;36mPLM_score_old\u001b[0;34m(int_document_id, query_id_tokens, kernel_func, mu, sigma)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# Query id tokens without stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mquery_id_no_stopwrds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_id_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mquery_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_id_no_stopwrds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "## Generate all PLM Values\n",
    "PLM_GAUS_500 = (functools.partial(PLM_score_old, kernel_func = gaussian_kernel, mu = 500), 'PLM_Gaussian_500')\n",
    "PLM_GAUS_1000 = (functools.partial(PLM_score_old, kernel_func = gaussian_kernel, mu = 1000), 'PLM_Gaussian_500')\n",
    "PLM_GAUS_1500 = (functools.partial(PLM_score_old, kernel_func = gaussian_kernel, mu = 1500), 'PLM_Gaussian_500')\n",
    "\n",
    "PLM_TRIANGLE_500 = (functools.partial(PLM_score_old, kernel_func = triangle_kernel, mu = 500), 'PLM_TRIANGLE_500')\n",
    "PLM_TRIANGLE_1000 = (functools.partial(PLM_score_old, kernel_func = triangle_kernel, mu = 1000), 'PLM_TRIANGLE_1000')\n",
    "PLM_TRIANGLE_1500 = (functools.partial(PLM_score_old, kernel_func = triangle_kernel, mu = 1500), 'PLM_TRIANGLE_1500')\n",
    "\n",
    "PLM_COSINE_500 = (functools.partial(PLM_score_old, kernel_func = cosine_kernel, mu = 500), 'PLM_COSINE_500')\n",
    "PLM_COSINE_1000 = (functools.partial(PLM_score_old, kernel_func = cosine_kernel, mu = 1000), 'PLM_COSINE_1000')\n",
    "PLM_COSINE_1500 = (functools.partial(PLM_score_old, kernel_func = cosine_kernel, mu = 1500), 'PLM_COSINE_1500')\n",
    "\n",
    "PLM_CIRCLE_500 = (functools.partial(PLM_score_old, kernel_func = circle_kernel, mu = 500), 'PLM_CIRCLE_500')\n",
    "PLM_CIRCLE_1000 = (functools.partial(PLM_score_old, kernel_func = circle_kernel, mu = 1000), 'PLM_CIRCLE_1000')\n",
    "PLM_CIRCLE_1500 = (functools.partial(PLM_score_old, kernel_func = circle_kernel, mu = 1500), 'PLM_CIRCLE_1500')\n",
    "\n",
    "PLM_PASSAGE_500 = (functools.partial(PLM_score_old, kernel_func = passage_kernel, mu = 500), 'PLM_PASSAGE_500')\n",
    "PLM_PASSAGE_1000 = (functools.partial(PLM_score_old, kernel_func = passage_kernel, mu = 1000), 'PLM_PASSAGE_1000')\n",
    "PLM_PASSAGE_1500 = (functools.partial(PLM_score_old, kernel_func = passage_kernel, mu = 1500), 'PLM_PASSAGE_1500')\n",
    "\n",
    "\n",
    "smoothings_functions = [PLM_PASSAGE_500, PLM_PASSAGE_1000, PLM_PASSAGE_1500,\n",
    "                        PLM_TRIANGLE_500,PLM_TRIANGLE_1000,PLM_TRIANGLE_1500,\n",
    "                        PLM_COSINE_500, PLM_COSINE_1000, PLM_COSINE_1500,\n",
    "                        PLM_CIRCLE_500,PLM_CIRCLE_1000,PLM_CIRCLE_1500,\n",
    "                        PLM_PASSAGE_500,PLM_PASSAGE_1000,PLM_PASSAGE_1500]\n",
    "\n",
    "for smoothing_func, label in smoothings_functions:\n",
    "    run_retrieval(label, \n",
    "                  functools.partial(log_multinomial_query_language_model, word_probability_func=smoothing_func), \n",
    "                  \"Probabilistic\", test_queries_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import gensim\n",
    "    \n",
    "# Under construction:\n",
    "def LSI(corpus, dictionary, num_topics):\n",
    "    return gensim.models.lsimodel.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "# Pending:\n",
    "def LDA(arg1, arg2):\n",
    "    return gensim.models.ldamodel.LdaModel(corpus=sentences, id2word=words, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the connector that ports Pyndri objects in a Gensim-friendly way.\n",
    "# Source: http://www.pythonexample.com/code/gensim-pyro/\n",
    "# Only updated return values from _doc2bow func; swapped key, value\n",
    "\n",
    "class LDALSISentences(gensim.interfaces.CorpusABC):\n",
    "    # Dictionary through task 2 = vocabulary\n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    " \n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    " \n",
    "        self.max_documents = max_documents\n",
    " \n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    " \n",
    "    def _doc2bow(self, doc):\n",
    " \n",
    "        di = collections.defaultdict(int)\n",
    " \n",
    "        for token_id in doc:\n",
    "            di[token_id] += 1\n",
    "    \n",
    "        return [(key, value) for key, value in di.items()]\n",
    " \n",
    "    def __iter__(self):\n",
    "\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, tokens = self.index.document(int_doc_id)\n",
    " \n",
    "            tokens = tuple(\n",
    "                token_id\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary)\n",
    "\n",
    "            yield self._doc2bow(tokens)\n",
    " \n",
    "    def __len__(self):\n",
    "        return self._maximum_document() - self.index.document_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "    \n",
    "def LSI(corpus, dictionary, num_topics, store=False):\n",
    "    \n",
    "    lsi = gensim.models.lsimodel.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "    \n",
    "    if store == True:\n",
    "        file_name = \"\"\n",
    "        full_file_name = file_name + \".p\"\n",
    "        \n",
    "        if full_file_name.exists():\n",
    "            print(\"This file already exists, pick another...\")\n",
    "            return\n",
    "        \n",
    "        pickle.dump(lsi, open(full_file_name, \"wb\"))\n",
    "        \n",
    "    return lsi\n",
    "\n",
    "\n",
    "def LDA(corpus, dictionary, num_topics, update_every=1, chunksize=1000, passes=1, store=False):\n",
    "    \n",
    "    print(\"Start training LDA model...\")\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                          id2word=dictionary, \n",
    "                                          num_topics=num_topics, \n",
    "                                          update_every=update_every, \n",
    "                                          chunksize=chunksize, \n",
    "                                          passes=passes)\n",
    "    \n",
    "    print(\"Done training\")\n",
    "    \n",
    "    if store == True:\n",
    "        file_name = \"lda_model_tfidf100\"\n",
    "        full_file_name = file_name + \".p\"\n",
    "        \n",
    "        if full_file_name.exists():\n",
    "            print(\"This file already exists, pick another...\")\n",
    "            return\n",
    "        \n",
    "        try:    \n",
    "            pickle.dump(lda, open(full_file_name, \"wb\" ))\n",
    "        except:\n",
    "            print(\"Storing LDA model failed...\")\n",
    "        \n",
    "    return lda\n",
    "    \n",
    "def print_model_topics(model, nr_of_topics, words_per_topic):\n",
    "    print(model.print_topics(nr_of_topics, num_words=words_per_topic))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import dependencies:\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy import spatial\n",
    "\n",
    "# Step 0: Convert corpus into gensim dictionary object\n",
    "def init_corpus(store=False):\n",
    "    \n",
    "    #index = pyndri.Index('index/')\n",
    "    print(\"Building dictionary...\")\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    \n",
    "    print(\"Building corpus...\")\n",
    "    corpus = LDALSISentences(index, dictionary)\n",
    "    \n",
    "    # For our own TF IDF, skip those doc ids that are not in given list.\n",
    "    print(\"Building TF-Corpus...\")\n",
    "    corpus = [doc for doc in corpus]\n",
    "    \n",
    "    print(\"Storing dictionary and corpus...\")\n",
    "    \n",
    "    if store == True:\n",
    "        pickle.dump(dictionary, open( \"dictionary.p\", \"wb\" ))\n",
    "        pickle.dump(corpus, open( \"corpus_tf.p\", \"wb\" ))\n",
    "    \n",
    "    return corpus, dictionary\n",
    "\n",
    "# Create our own TFIDF corpus based on task 1\n",
    "def transform_corpus(corpus, store=False):\n",
    "    print(\"Initializing TFIDF model...\")\n",
    "    tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    print(\"Training TFIDF model...\")\n",
    "    corpus_tfidf = tfidf_model[corpus]\n",
    "    \n",
    "    if store == True:\n",
    "        pickle.dump(corpus_tfidf, open( \"corpus_tfidf.p\", \"wb\" ))\n",
    "    \n",
    "    return corpus_tfidf\n",
    "\n",
    "def load_from_pickle(corp_bow=\"\", q_bow=\"\", sem_model=\"\", sim_model=\"\"):\n",
    "    \n",
    "    corpus_bow, query_bows, semantic_model, similarity_model = None, None, None, None\n",
    "    \n",
    "    if corp_bow != \"\":\n",
    "        try:\n",
    "            print(\"Loading corpus BOW: %s...\".format(corp_bow))\n",
    "            with open(corp_bow, 'rb') as f:\n",
    "                corpus_bow = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccesful\")\n",
    "            \n",
    "    if q_bow != \"\":\n",
    "        try:\n",
    "            print(\"Loading query BOW: %s...\".format(q_bow))\n",
    "            with open(q_bow, 'rb') as f:\n",
    "                query_bows = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccessful\")\n",
    "    \n",
    "    if sem_model != \"\":\n",
    "        try:\n",
    "            print(\"Loading semantic model: %s...\".format(sem_model))\n",
    "            with open(sem_model, 'rb') as f:\n",
    "                semantic_model = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccessful\")\n",
    "        \n",
    "    if sim_model != \"\":\n",
    "        try:\n",
    "            print(\"Loading similarity model: %s...\".format(sim_model))\n",
    "            with open(sem_model, 'rb') as f:\n",
    "                similarity_model = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccessful\")\n",
    "        \n",
    "    return corpus_bow, query_bows, semantic_model, similarity_model\n",
    "\n",
    "\n",
    "# Step 1: Build a representation q of the query\n",
    "def query_representation(dictionary):\n",
    "    \n",
    "    with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "        queries = parse_topics([f_topics])\n",
    "        \n",
    "    tokenized_queries = {\n",
    "        query_id: [dictionary.translate_token(token)\n",
    "                   for token in index.tokenize(query_string)\n",
    "                   if dictionary.has_token(token)]\n",
    "        for query_id, query_string in queries.items()}\n",
    "\n",
    "    unique_query_ids = set(\n",
    "        query_term_id\n",
    "        for query_term_ids in tokenized_queries.values()\n",
    "        for query_term_id in query_term_ids)\n",
    "    \n",
    "    query_bows = [dictionary.doc2bow(query)\n",
    "                  for query in tokenized_queries.values()]\n",
    "    \n",
    "    return tokenized_queries, unique_query_ids, query_bows\n",
    "\n",
    "# Step 2: Build a representation d of the documents i.e. by scoring them\n",
    "def document_representation(index, dictionary, method=\"tfidf\"):\n",
    "    \"\"\" Returns a documents representation; \"\"\"\n",
    "    X = dok_matrix((len(dictionary), len(index) + 1), dtype=np.float32)\n",
    "    start_time = time.time()\n",
    "    # TODO: You only care about words occurring in the query, since otherwise IF-IDF == 0 (?)\n",
    "\n",
    "    if method == \"tfidf\":\n",
    "        for query_id in inverted_index.keys():\n",
    "            for doc_id in inverted_index[query_id]:\n",
    "                X[query_id, doc_id] = tfidf(doc_id, query_id)\n",
    "\n",
    "    elif method == \"tf\":\n",
    "        for query_id in inverted_index.keys():\n",
    "            for doc_id in inverted_index[query_id]:\n",
    "                X[query_id, doc_id] = term_frequency(query_id, doc_id)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Step 3: Build a function that is able to calculate similarity between q and d.\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    \n",
    "    #vector_a = X[:,1].toarray().flatten()\n",
    "    #vector_b = X[:,2].toarray().flatten()\n",
    "    \n",
    "    return 1 - spatial.distance.cosine(vector_a, vector_b)\n",
    "\n",
    "def init_similarity_model(model, corpus, num_features=300, store=False):\n",
    "    \n",
    "    sim_index = gensim.similarities.MatrixSimilarity(model[corpus], num_features=num_features)\n",
    "    \n",
    "    if store == True:\n",
    "        pickle.dump(sim_index, open(\"sim_index_lsi_tf.p\", \"wb\"))\n",
    "    \n",
    "    return sim_model\n",
    "    \n",
    "# Generates a weird error:\n",
    "def gensim_cosine_similarity(sim_model, model, query_bow, top_num=10):\n",
    "    \n",
    "    query_representation = model[query_bow]\n",
    "    \n",
    "    similarities = sim_model[query_representation]\n",
    "    \n",
    "    sorted_similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    "    \n",
    "    results = sorted_similarities[:top_n]\n",
    "\n",
    "    return similarities\n",
    "\n",
    "# Step 4: Use the representations to train LSMs - choose hyperparams by testing\n",
    "\n",
    "# Step 5: Use TF-IDF to rank top 1000 documents, for each query\n",
    "\n",
    "# Step 6: Use both LSMs to re-rank top 1000 documents and apply TREC to test results. Based on:\n",
    "## - NDCG@10\n",
    "## - MAP@1000\n",
    "## - Precision@5\n",
    "## - Recall@1000\n",
    "\n",
    "#corpus_tfidf, _, _, _ = load_from_pickle(corp_bow=\"corpus_tfidf.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n",
      "Building TF-Corpus...\n",
      "Storing dictionary and corpus...\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf, _ = init_corpus()\n",
    "\n",
    "# query_bow = query_bows[27]\n",
    "\n",
    "# print(len(query_bow))\n",
    "\n",
    "# query_lsi = lsi_model_tfidf[query_bow]\n",
    "\n",
    "# similarities = sim_index_lsi_tfidf[query_lsi]\n",
    "\n",
    "# print(len(similarities))\n",
    "\n",
    "# sorted_similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    "\n",
    "# top_n = 1000\n",
    "\n",
    "# results = sorted_similarities[:top_n]\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 2.86 s, total: 1min 29s\n",
      "Wall time: 1min 31s\n",
      "Epoch  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-bc62aefe7dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mw2v_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml1labs/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml1labs/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml1labs/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyndri.compat\n",
    "import copy\n",
    "\n",
    "word2vec = gensim.models.Word2Vec(\n",
    "    size=300,  # Embedding size\n",
    "    window=5,  # One-sided window size\n",
    "    sg=True,  # Skip-gram.\n",
    "    min_count=5,  # Minimum word frequency.\n",
    "    sample=1e-3,  # Sub-sample threshold.\n",
    "    hs=False,  # Hierarchical softmax.\n",
    "    negative=10,  # Number of negative examples.\n",
    "    iter=1,  # Number of iterations.\n",
    "    workers=8,  # Number of workers.\n",
    ")\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "%time word2vec.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "w2v_models = [word2vec]\n",
    "\n",
    "N_EPOCHS = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print('Epoch ', epoch)\n",
    "\n",
    "    w2v_model = copy.deepcopy(w2v_models[-1])\n",
    "    w2v_model.train(sentences, total_examples=word2vec.corpus_count, epochs=1)\n",
    "\n",
    "    w2v_models.append(w2v_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda_model_tfidf, open(\"lda_model_tfidf100.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible features:\n",
    "# - Query/Document Feature\n",
    "#  - TF/IDF\n",
    "#  - BM25\n",
    "#  - PLM-score\n",
    "#  - a Language Model\n",
    "#  - Topical matching\n",
    "#  - Correlation of relevance matrix?\n",
    "# - Document Features\n",
    "#  - Document Length\n",
    "# - Query Features\n",
    "#  - Presence of given name in query\n",
    "#  - Query Length\n",
    "\n",
    "# Normalization:\n",
    "# We compute all the features and then scale to zero-mean and unit variance\n",
    "# Can be done with StandardScaler from scikit learn.\n",
    "# For D/Q features: read from .run files\n",
    "# Query level feature normalization is needed\n",
    "\n",
    "# MACHINE LEARNING\n",
    "\n",
    "# Computing Loss:\n",
    "# Read from ap_88_89/qrel_*\n",
    "\n",
    "def read_run(path_to_run):\n",
    "    '''\n",
    "    Read document scores from a .run file\n",
    "    \n",
    "    :param path_to_run: where to find the .run file\n",
    "    '''\n",
    "    \n",
    "    scores = defaultdict(dict)\n",
    "    \n",
    "    with open(path_to_run) as run_file:\n",
    "        for i, line in enumerate(run_file.readlines()):\n",
    "            query_id, _, ext_doc_id, document_rank, score, label = line.split()\n",
    "            \n",
    "            # Skip q/d pairs that don't share words\n",
    "            if score == '0' or score == '-inf':\n",
    "                continue\n",
    "                \n",
    "            scores[int(query_id)][ext2int[ext_doc_id]] = float(score)\n",
    "            \n",
    "    return matrix_from_two_level_dict(scores)\n",
    "    \n",
    "def build_relevance_matrix(path_to_relavance_labels = 'ap_88_89/qrel_validation'):\n",
    "    '''\n",
    "    Constructs a sparse matrix where the relevant query/document pairs are 1's and the rest 0's\n",
    "    \n",
    "    :param path_to_relevance_labels: where to find the relevance file\n",
    "    '''\n",
    "    \n",
    "    relevance_labels_validation = defaultdict(dict)\n",
    "\n",
    "    # Read from relevance file\n",
    "    with open(path_to_relavance_labels) as valid_file:\n",
    "        for i, line in enumerate(valid_file.readlines()):\n",
    "            query_id, _, ext_doc_id, relevance = line.split()\n",
    "            relevance_labels_validation[int(query_id)][ext2int[ext_doc_id]] = int(relevance)\n",
    "\n",
    "    return matrix_from_two_level_dict(relevance_labels_validation)\n",
    "\n",
    "def matrix_from_two_level_dict(dictionary):\n",
    "    \n",
    "    matrix = np.zeros((max(dictionary.keys()) + 1, len(ext2int)))\n",
    "    \n",
    "    print(\"Matrix size: \", matrix.shape)\n",
    "    \n",
    "    for key, sub_dict in dictionary.items():\n",
    "        for sub_key, value in sub_dict.items():\n",
    "            matrix[key, sub_key] = value\n",
    "            \n",
    "    return matrix #.tocsr()\n",
    "\n",
    "def normalize_log_probs(matrix):\n",
    "    real_probs = np.exp(matrix)\n",
    "    return real_probs\n",
    "    return normalize(real_probs)\n",
    "\n",
    "def feature_vector(query_id, int_doc_id):\n",
    "    '''Build a vector based on features of the given query/document pair'''\n",
    "    \n",
    "    # Query level features\n",
    "    \n",
    "    # Query independent features\n",
    "\n",
    "    # Query dependent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n",
      "Matrix size:  (193, 164630)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def is_validation_run(file_name):\n",
    "    return '.run' in file_name and 'test' not in file_name and 'TF' not in file_name\n",
    "\n",
    "run_file_names = ['run/'+file_name for file_name in os.listdir('run') \n",
    "                  if is_validation_run(file_name)]\n",
    "\n",
    "features_per_pair = [read_run(file_name) for file_name in run_file_names]\n",
    "normalized_feat_per_pair = [normalize_log_probs(matrix) for matrix in features_per_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 875 ms, sys: 3.72 s, total: 4.59 s\n",
      "Wall time: 5.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00570932,  0.00704123,  0.00837314,  0.00862766,  0.0086518 ,\n",
       "        0.00856489,  0.0078355 ,  0.00435306,  0.00087061])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = len(features_per_pair)\n",
    "n_queries = features_per_pair[0].shape[0]\n",
    "n_docs = features_per_pair[0].shape[1]\n",
    "\n",
    "%time features = np.concatenate(normalized_feat_per_pair, axis=0).reshape(n_features, n_queries, n_docs)\n",
    "\n",
    "transposed_features = np.transpose(features, (1, 2, 0))\n",
    "\n",
    "assert transposed_features.shape == (n_queries, n_docs, n_features)\n",
    "\n",
    "transposed_features[53, ext2int['AP881216-0235'], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
