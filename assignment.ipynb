{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] \n",
    "\n",
    "## Functions for reading/writing files to do with TREC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import all relevant libraries'''\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pyndri\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import io\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.models\n",
    "import pickle\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy import spatial\n",
    "import gensim.models\n",
    "import pyndri.compat\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' Routine provided with the assignment instructions. Needed to write run statistics to a file that can\n",
    "be interpreterd by trec_eval'''\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trec_results(from_path, filter_metrics=[]):\n",
    "    \n",
    "    '''Reads the results out put by the assignment_evals.sh bash script into a dictionary'''\n",
    "    \n",
    "    if not from_path[-4:] == '.txt': return None\n",
    "\n",
    "    eval_data = defaultdict(dict)\n",
    "\n",
    "    with open(from_path) as file_open:\n",
    "        \n",
    "        content = [line.strip().split() for line in file_open.readlines()]\n",
    "        for metric, query, score in content:\n",
    "            \n",
    "            # If a filter is provided, skip those not in it\n",
    "            if len(filter_metrics) > 0 and metric not in filter_metrics:\n",
    "                continue\n",
    "                \n",
    "            file_name = from_path.split('/')[-1]\n",
    "            eval_data[metric][query] = float(score)\n",
    "\n",
    "    return eval_data\n",
    "\n",
    "def read_trec_directory(from_path, filter_metrics=[]):\n",
    "\n",
    "    '''Reads the results out put by the assignment_evals.sh bash script into a dictionary\n",
    "     for an entire directory'''\n",
    "\n",
    "    eval_data = {}\n",
    "    for file_name in os.listdir(from_path):\n",
    "        results = read_trec_results(from_path + file_name, filter_metrics=filter_metrics)\n",
    "\n",
    "        if results:\n",
    "            eval_data[file_name[:-4]] = results\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = pyndri.Index('index/')\n",
    "token2id, id2token, _ = index.get_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                print(\"Max capacity reached... Breaking...\")\n",
    "                break\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ùõå in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ùõç [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ùõÖ in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of ‚Äúsoft‚Äù passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ùõî equal to 50, and Dirichlet smoothing with ùõç optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand how the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don‚Äôt forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  164597\n",
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 67.74565291404724 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''Read the topic file and gather usefull statistics like document lengths, the inverted index and\n",
    "query dictionary'''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "print('Total number of documents: ', num_documents)\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "# Used to build query/document relevance matrix in task 4\n",
    "ext2int = defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    \n",
    "    ext2int[ext_doc_id] = int_doc_id\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "# inverse of ext2int which was built in the loop above\n",
    "int2ext = {i : ext for ext, i in ext2int.items()}\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queries:  150\n",
      "Average document length:  256.4381975370147\n"
     ]
    }
   ],
   "source": [
    "print('Total number of queries: ', len(queries))\n",
    "print('Average document length: ', avg_doc_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"Helper functions\"\"\"\n",
    "\n",
    "def term_frequency(term_id, document_id):\n",
    "    \"\"\"\n",
    "    Retrieves the term frequency a given word in a given document\n",
    "    \n",
    "    NB: depends on global variable 'inverted_index'\n",
    "    \n",
    "    :param term_id: The id of the word in question\n",
    "    :param document_id: The internal document_id of the document in question\n",
    "    \"\"\"\n",
    "    \n",
    "    return inverted_index[term_id].get(document_id, 0)\n",
    "\n",
    "def compute_BM25(doc_length, tf_raw, term_id, k=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    Helper function to isolate computations shared by both \n",
    "    query + document vector versions and the original version of the BM25 method\n",
    "    \"\"\"\n",
    "    \n",
    "    inv_norm = doc_length / avg_doc_length\n",
    "    document_freq = len(inverted_index[term_id])\n",
    "    \n",
    "    tf_normalized = tf_raw * (avg_doc_length / doc_length)\n",
    "\n",
    "    # Note: slides are not particularly clear about whether to still use the normalized TF or not.\n",
    "    # In this case we will continue with this normalized variable.\n",
    "    tf = ((1.0 + k) * tf_normalized) / (k * ((b - 1.0) + b * (inv_norm)) + tf_normalized)\n",
    "\n",
    "    idf = np.log(num_documents / document_freq)\n",
    "    \n",
    "    return tf * idf\n",
    "    \n",
    "\n",
    "def build_document_vector(document_id, vocabulary_size):\n",
    "    '''\n",
    "    Creates a vector of length V with the term frequency for each word in the vocabulary at its\n",
    "    corresponding index\n",
    "    \n",
    "    :param document_id: The internal document id\n",
    "    :param vocabulary_size: The number of words in the vocabulary\n",
    "    '''\n",
    "    \n",
    "    vector = np.zeros(vocabulary_size)\n",
    "    \n",
    "    for token_id in inverted_index:\n",
    "        documents_for_term = inverted_index[token_id]\n",
    "        \n",
    "        if document_id in documents_for_term:\n",
    "            vector[token_id] = documents_for_term[document_id]\n",
    "        \n",
    "    return vector\n",
    "        \n",
    "def build_query_vector(query_terms, vocabulary_size):\n",
    "    '''\n",
    "    Creates a vector of length V with the term frequency for each word in the vocabulary at its\n",
    "    corresponding index\n",
    "    \n",
    "    :param query_terms: A list of query term ids\n",
    "    :param vocabulary_size: The number of words in the vocabulary\n",
    "    '''\n",
    "    \n",
    "    vector = np.zeros(vocabulary_size)\n",
    "    counts = collections.Counter(query_terms)\n",
    "    \n",
    "    for token_id in counts:\n",
    "        vector[token_id] = counts[token_id]\n",
    "        \n",
    "    return vector\n",
    "\n",
    "# Vector representation of the collection frequencies dictionary, used\n",
    "# in the PLM function for fast matrix operations\n",
    "collection_freq_matrix = np.zeros(len(id2token) + 1)\n",
    "for key in sorted(collection_frequencies.keys()):\n",
    "    if key > 0:\n",
    "        collection_freq_matrix[key] = collection_frequencies[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Scoring Functions\n",
    "\n",
    "#### TF-IDF and BM25\n",
    "Defines the functions used to compute the TF-IDF, BM25 and corresponding query/document scores. Because we have more data on the documents than on the queries, the functions have seperate interfaces for both types (one takes an id, one takes a list of terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_similarity(query_terms, document_id, weight_fn = None, query_weight_fn = None):\n",
    "    '''\n",
    "    Scoring function for a document and a query\n",
    "    \n",
    "    :param query_terms: A list of query term ids\n",
    "    :param document_id: The internal document id\n",
    "    :param weight_fn: A function to re-weight the document vector\n",
    "    :param query_weight_fn: A function to re-weight the query vector\n",
    "    \n",
    "    return: a score (float)\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(id2token)\n",
    "    \n",
    "    # Create query vector\n",
    "    # Create document vector\n",
    "    doc_vec = build_document_vector(document_id, vocab_size)\n",
    "    query_vec = build_query_vector(query_terms, vocab_size)\n",
    "    \n",
    "    # Combine query and document into a matrix with sparse representation (only save indices of non-zero entries)\n",
    "    paired_matrix = np.array([query_vec, doc_vec])\n",
    "    paired_matrix = sparse.csr_matrix(paired_matrix)\n",
    "    \n",
    "    if weight_fn and query_weight_fn:\n",
    "        # nonzero() returns a tuple of two lists\n",
    "        # 0 -> i dimension of non zero entries in array where i == 0 is the query vector\n",
    "        # 1 -> j dimension of non zero entries in array array where j corresponds to the term_id\n",
    "        document_type, term_ids = paired_matrix.nonzero()\n",
    "        \n",
    "        for document_type, term_id in zip(document_type, term_ids):\n",
    "            if document_type == 0:\n",
    "                # Re-weight query vector\n",
    "                paired_matrix[document_type, term_id] = query_weight_fn(query_terms, term_id)\n",
    "            else:\n",
    "                # Re-weight document vecotr\n",
    "                paired_matrix[document_type, term_id] = weight_fn(document_id, term_id)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(paired_matrix)\n",
    "    \n",
    "    # Only first row is relevant (query document similarities)\n",
    "    similarities = similarity_matrix[0, 1:]\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def tfidf_query(query_terms, query_term_id):\n",
    "    '''A seperate function to compute tf-idf score for a query word in a query'''\n",
    "    counts = collections.Counter(query_terms)\n",
    "    \n",
    "    tf = np.log(1 + counts[query_term_id])\n",
    "    \n",
    "    df = len(inverted_index[query_term_id])\n",
    "    idf = np.log(num_documents / df)\n",
    "    \n",
    "    score = tf * idf\n",
    "    \n",
    "    return score\n",
    "\n",
    "def tfidf(int_document_id, query_term_id):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_term_id: the query term id (assuming you have split the query to tokens)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check: RAW COUNT OR NORMALIZED TF???\n",
    "    tf = np.log(1 + term_frequency(query_term_id, int_document_id))\n",
    "    \n",
    "    # inverted_index[query_term_id] consists of all the documents this term appears in\n",
    "    df = len(inverted_index[query_term_id])\n",
    "    idf = np.log(num_documents / df)\n",
    "    \n",
    "    score = tf * idf \n",
    "    \n",
    "    return score\n",
    "\n",
    "# Here we will normalize the raw TF and a different kind of damping: (1 + k)/k\n",
    "def BM25(int_document_id, query_term_id, k=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25 weight of a term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_term_id: Token id of the term to be re-weighted\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    doc_length = len(index.document(int_document_id)[1])\n",
    "    tf_raw = term_frequency(query_term_id, int_document_id)\n",
    "    \n",
    "    return compute_BM25(doc_length, tf_raw, query_term_id, k, b)\n",
    "\n",
    "def BM25_query(query_terms, query_term_id, k=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and all terms of a single query\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_term_id: Token id of the term to be re-weighted\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    doc_length = len(query_terms) \n",
    "    counts = collections.Counter(query_terms)\n",
    "    tf_raw = counts[query_term_id]\n",
    "    \n",
    "    return compute_BM25(doc_length, tf_raw, query_term_id, k, b)\n",
    "\n",
    "def score_BM25(query_term_ids, int_document_id):\n",
    "    '''\n",
    "    Sum of unique query term scores of bm25 for the given document\n",
    "    \n",
    "    :param query_term_ids: ids of query terms\n",
    "    :int_document_id: The ID of the document to be scored\n",
    "    \n",
    "    return: Document score for the given query (float)\n",
    "    '''\n",
    "    \n",
    "    unique_terms = set(query_term_ids)\n",
    "    doc_length = len(index.document(int_document_id)[1])\n",
    "    tf_raw = term_frequency(query_term_id, int_document_id)\n",
    "    \n",
    "    # Sum over the set of all (unique) query terms\n",
    "    score = np.sum([compute_BM25(doc_length, term_frequency(term_id, int_document_id), term_id) for term_id in unique_terms])\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "Here are the implementation for the following smoothing functions:\n",
    "\n",
    "* Absolute Discounting\n",
    "* Dirichlet-Prior Smoothing\n",
    "* Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_discounting(int_document_id, query_term_id, delta=0.5):\n",
    "    \n",
    "    # TODO: figure our if tf should be raw or not. Max to ensure tf in range [0,1]\n",
    "    tf_raw = max(term_frequency(query_term_id, int_document_id) - delta, 0)\n",
    "    tot_nr_terms_in_doc = len(index.document(int_document_id)[1])\n",
    "    unique_terms_in_doc = unique_terms_per_document[int_document_id]\n",
    "    \n",
    "    corpus_size = total_terms\n",
    "    word_frequency = collection_frequencies[query_term_id]\n",
    "    p_w_c = word_frequency / corpus_size\n",
    "    \n",
    "    score = tf_raw / tot_nr_terms_in_doc + ((delta * tot_nr_terms_in_doc) / tot_nr_terms_in_doc) * p_w_c\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def dirichlet_prior_smoothing(int_document_id, query_term_id, mu=1000):\n",
    "    ''' input: one document id, one query term id, mu\n",
    "        output: for one query word , the dirichlet smoothed prior\n",
    "    '''\n",
    "\n",
    "    # Calculate the components of the model: tf, |d|, p(w|c)\n",
    "    tf = term_frequency(query_term_id, int_document_id)\n",
    "    document_length = document_lengths[int_document_id]\n",
    "    p_wc = collection_frequencies[query_term_id]/total_terms\n",
    "    \n",
    "    signal = ((document_length/(document_length+mu))*(tf/document_length))\n",
    "    background = ((mu/(document_length + mu))*p_wc)\n",
    "    \n",
    "    dir_smoothed_prior =  signal + background \n",
    "    \n",
    "    return dir_smoothed_prior\n",
    "\n",
    "def jelinek_mercer(int_document_id, query_term_id, lambd=0.5):\n",
    "    \"\"\"\n",
    "    Calculate probabilty of a term by linearly interpolating with background language model.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param lambd: the lambda paramter for the function\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_length = len(index.document(int_document_id)[1])\n",
    "    \n",
    "    term_freq_doc = term_frequency(query_term_id, int_doc_id)\n",
    "    term_freq_corpus = sum(inverted_index[query_term_id].values())\n",
    "    \n",
    "    signal = (term_freq_doc / doc_length)\n",
    "    background = (term_freq_corpus / total_terms)\n",
    "    \n",
    "    return lambd * signal  + (1 - lambd) * background\n",
    "\n",
    "\n",
    "def ml_query_word_language_model(query_term_ids):\n",
    "    counts = collections.Counter(query_term_ids)\n",
    "    return np.array([counts[term_id] / len(query_term_ids) for term_id in query_term_ids])\n",
    "\n",
    "def unigram_prob(term_id, document_id):\n",
    "    \"\"\"\n",
    "    Maximum Likelihood estimate of p(word|document_model)\n",
    "    \"\"\"\n",
    "    tf = term_frequency(term_id, document_id)\n",
    "    doc_length = len(index.document(document_id)[1])\n",
    "    \n",
    "    return tf / doc_length\n",
    "\n",
    "def log_multinomial_query_language_model(query_term_ids, document_id, word_probability_func=unigram_prob):\n",
    "    \"\"\"\n",
    "    Query-likelihood model assuming uniform document prior\n",
    "    \"\"\"\n",
    "\n",
    "    counts = collections.Counter(query_term_ids)\n",
    "    \n",
    "    # sum_over_words_in_query( tf(w;d) * log( p(w|d) ) )\n",
    "    return np.sum([counts[term_id]*np.log(word_probability_func(document_id, term_id)) \n",
    "                   for term_id in query_term_ids])\n",
    "\n",
    "def kl_divergenge(query_term_ids, document_id, word_probability_func=unigram_prob):\n",
    "    '''\n",
    "    Kullback-Leiber divergence for query-language model and document-language model\n",
    "    '''\n",
    "    \n",
    "    p_word_given_query = ml_query_word_language_model(query_term_ids)\n",
    "    p_word_given_document = np.array([word_probability_func(term_id, document_id) for term_id in query_term_ids])\n",
    "    \n",
    "    return np.sum(p_word_given_query * np.log(p_word_given_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postition Based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLM_score(query_id_tokens, int_document_id, kernel_func, mu, sigma = 50, return_argmax=False):\n",
    "    '''\n",
    "    :param query_id_tokens: The query to be matched against\n",
    "    :param int_document_id: The id of the document to score\n",
    "    :param kernec_func: A kernel function that takes two indices (i,j)\n",
    "    :param mu: parameter for dirichlet-prior smoothing\n",
    "    :param sigma: the width of the kernel\n",
    "    :param return_argmax: Indicate argmax mode (will return position of best-score instead of score)\n",
    "     '''\n",
    "    \n",
    "    # document variables\n",
    "    document = index.document(int_document_id)[1]\n",
    "    doc_length = len(document)\n",
    "    \n",
    "    # Query id tokens without stopwords\n",
    "\n",
    "    query_id_no_stopwrds = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "    query_length = len(query_id_no_stopwrds)\n",
    "        \n",
    "    # Create a matrix to store all c(w,i)' values that matter\n",
    "    c_wi_prime_matrix = np.zeros((query_length,doc_length))\n",
    "    \n",
    "    # Pre calculate kernel\n",
    "    one_sided_kernel = [kernel_func(0, j) for j in range(doc_length)]\n",
    "    kernel = np.array(list(reversed(one_sided_kernel))[:-1] + one_sided_kernel)\n",
    "    \n",
    "    # Loop over all positions in the document\n",
    "    for i, word_id_i in enumerate(document):\n",
    "        \n",
    "        # If a word occurs in the query we should propagate its frequency to other positions\n",
    "        for q, wrd in enumerate(query_id_no_stopwrds):\n",
    "            \n",
    "            if word_id_i == wrd:\n",
    "                # Loop over all positions in the document as to have a distance from position i to j\n",
    "                # for positions where i is a query word, and for which kernel(i,j) != 0\n",
    "                for j in max_usefull_range(one_sided_kernel, i, doc_length):\n",
    "                    c_wi_prime_matrix[q][j] += kernel[i - j]\n",
    "    \n",
    "    # Go through all positions in the document again now all the \n",
    "    # frequencies have been propagated\n",
    "    Zi_vec = np.sum(c_wi_prime_matrix,axis=0)\n",
    "\n",
    "    # Dirichlet smoothed probability of a query word on a position in the document\n",
    "    p_wc_vec = collection_freq_matrix[np.array(query_id_tokens)] / total_terms\n",
    "    p_wc_vec = np.expand_dims(p_wc_vec, axis=1)\n",
    "    \n",
    "    smooth_pos_values = (c_wi_prime_matrix + mu * p_wc_vec) / (Zi_vec + mu)\n",
    "    \n",
    "    # Calculate p(w|Q)\n",
    "    q_language = ml_query_word_language_model(query_id_tokens) \n",
    "    \n",
    "    # log( P(w|D,i) ) for all w, i pairs\n",
    "    log_pos_values = np.log(smooth_pos_values)\n",
    "    \n",
    "    # Simplified KL-divergence formula (proportional to original for the same query)\n",
    "    kl_divergence_scores = np.sum(q_language * log_pos_values.T, axis=1)\n",
    "    \n",
    "    # Offers the possibility to return the position of the maximum instead of the actual score.\n",
    "    # this is used as a feature in LETOR.\n",
    "    if return_argmax:\n",
    "        argmax = np.argmax(kl_divergence_scores)\n",
    "        if np.argmax:\n",
    "            return argmax\n",
    "        else:\n",
    "            return doc_length\n",
    "        \n",
    "    else:\n",
    "        return np.amax(kl_divergence_scores)\n",
    "\n",
    "def max_usefull_range(kernel_vector, i, N):\n",
    "    '''Compute the range of indices around position i in the doucment for which the \n",
    "       kernel has a postive value'''\n",
    "    \n",
    "    # index() throws an exception when the element is not in the array (in this case, for the guassian)\n",
    "    try:\n",
    "        max_distance = kernel_vector.index(0)\n",
    "    except:\n",
    "        max_distance = len(kernel_vector)\n",
    "        \n",
    "    minimum = max(0, i - max_distance)\n",
    "    maximum = min(N, i + max_distance)\n",
    "    \n",
    "    return range(minimum, maximum)\n",
    "\n",
    "'''Implementation of the kernel functions'''\n",
    "\n",
    "def gaussian_kernel(i, j, sigma=50):\n",
    "    return np.exp(-(i-j)**2 / (2 * sigma**2))\n",
    "\n",
    "def triangle_kernel(i, j, sigma=50):\n",
    "    return 1-abs(i - j)/sigma if abs(i - j) <= sigma else 0\n",
    "\n",
    "def cosine_kernel(i, j, sigma=50):\n",
    "    return 0.5 * (1 + np.cos((abs(i - j) * np.pi) / sigma)) if abs(i - j) <= sigma else 0\n",
    "\n",
    "def circle_kernel(i, j, sigma=50):\n",
    "    return np.sqrt(1-(abs(i - j)/sigma)**2) if abs(i - j) <= sigma else 0\n",
    "\n",
    "def passage_kernel(i, j, sigma=50):\n",
    "    return int(abs(i - j) < sigma/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookkeeping\n",
    "\n",
    "We need to have an account of the queries in the test/validation set and their corresponding top1000 TF-IDF ranked document list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  120  test queries and  30  validation queries\n"
     ]
    }
   ],
   "source": [
    "'''Read the validation/test files and save the ids into a list. This way we can easily check to which\n",
    "set a query belong.'''\n",
    "\n",
    "validation_queries_ids = []\n",
    "with open('./ap_88_89/qrel_validation', 'r') as validation_queries:\n",
    "    for line in validation_queries:\n",
    "        query_id = line.split(' ')[0]\n",
    "        if query_id not in validation_queries_ids:\n",
    "            validation_queries_ids.append(query_id)\n",
    "            \n",
    "            \n",
    "\n",
    "test_queries_ids = []\n",
    "with open('./ap_88_89/qrel_test', 'r') as test_queries:\n",
    "    for line in test_queries:\n",
    "        query_id = line.split(' ')[0]\n",
    "        if query_id not in test_queries_ids:\n",
    "            test_queries_ids.append(query_id)\n",
    "\n",
    "            \n",
    "print('There are ',len(test_queries_ids),' test queries and ',len(validation_queries_ids),' validation queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the ids of the top 1000 documents (ranked by TF-IDF) per query for the test and the validation set'''\n",
    "\n",
    "top_1000_per_test_query = defaultdict(list)\n",
    "\n",
    "with open('./run/already_run/TF-IDF.run', 'r') as top_docs_per_query:\n",
    "    for line in top_docs_per_query:\n",
    "        query_id = line.split(' ')[0]\n",
    "        document_name = line.split(' ')[2]\n",
    "        top_1000_per_test_query[query_id].append(document_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, model_type, query_set,name_set):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    :param model_type: indicates if a vector space or a proabilistic model is run. Takes either \"Vector space\"\n",
    "                        or \"Probabilistic\"\n",
    "    :param query_set: validation_queries_ids or test_queries_ids\n",
    "    \"\"\"\n",
    "\n",
    "    run_out_path = '{}_{}.run'.format(model_name,name_set)\n",
    "    if os.path.exists(run_out_path):\n",
    "        print(\"File already exists; exiting...\")\n",
    "        return\n",
    "    \n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "\n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    data = {}\n",
    "    \n",
    "    if model_name == 'TF-IDF':\n",
    "        weight_fn = tfidf\n",
    "        query_weight_fn = tfidf_query\n",
    "    if model_name == 'BM25':\n",
    "        weight_fn = BM25\n",
    "        query_weight_fn = BM25_query\n",
    "        \n",
    "    counter = 1\n",
    "\n",
    "    for query_id, query_terms in tokenized_queries.items():\n",
    "\n",
    "        # skip queries of the set we are not interested in\n",
    "        if query_id not in query_set:\n",
    "            continue\n",
    "\n",
    "        for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "            \n",
    "            ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "            \n",
    "            if ext_doc_id not in top_1000_per_test_query[query_id]:\n",
    "                continue\n",
    "\n",
    "            # Only calculate score of document if it contains query words,\n",
    "            # otherwise the score is zero for the vector based models\n",
    "            length_query = len(query_terms)\n",
    "\n",
    "            for i, query_term in enumerate(query_terms):\n",
    "                # Only compute score if the query word is in the document\n",
    "                \n",
    "                if int_doc_id in inverted_index[query_term].keys():\n",
    "\n",
    "                    # Calculate score for document and query terms\n",
    "                    if model_type == \"Vector space\":\n",
    "                        score = score_similarity(query_terms, int_doc_id, weight_fn, query_weight_fn)\n",
    "                    elif model_type == \"Probabilistic\":\n",
    "                        score = score_fn(query_terms, int_doc_id)\n",
    "                                        \n",
    "                \n",
    "                    if query_id in data.keys():\n",
    "                        data[query_id].append(tuple([float(score), str(ext_doc_id)]))\n",
    "                    else:\n",
    "                        data[query_id] = [((tuple([float(score),str(ext_doc_id)])))]\n",
    "\n",
    "                    # If the document has a query word, than the score is calculated and appended by this point\n",
    "                    break\n",
    "\n",
    "\n",
    "                # If up until the last query word, the query word did not appear in the \n",
    "                # document, the score is the lowerst possible (-infinity)\n",
    "                elif i == length_query-1:\n",
    "                    score = -np.inf\n",
    "                    if query_id in data.keys():\n",
    "                        data[query_id].append(tuple([float(score), str(ext_doc_id)]))\n",
    "                    else:\n",
    "                        data[query_id] = [((tuple([float(score),str(ext_doc_id)])))]\n",
    "\n",
    "\n",
    "\n",
    "        print('queries: ', counter,'/',len(query_set),'\\t this took: ', time.time() - retrieval_start_time, ' seconds in total' )\n",
    "        counter += 1\n",
    "        \n",
    "        # transform the list of tuples to a tuple of tuples, since this is the requered datastructue\n",
    "        data[query_id] = tuple(data[query_id])\n",
    "\n",
    "    # Write to file\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of task 1\n",
    "\n",
    "#### Check which parameter values yield the highest values on the evaluation metrics for the language models\n",
    "\n",
    "We show the  metrics NDCG@10, MAP@1000, Precision@5 and Recall@1000 for 'all' queries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_dir = 'run/TREC_results_on_validation_set/'\n",
    "\n",
    "eval_data = read_trec_directory(target_dir)\n",
    "\n",
    "model_names = [['jelinek_0.1_val_queries_len_C_is_num_words','lambda', '1', 'Jellinek-Mercer'], \n",
    "               ['jelinek_0.5_val_queries_len_C_is_num_words','lambda','5','Jellinek-Mercer' ], \n",
    "               ['jelinek_0.9_val_queries_len_C_is_num_words','lambda','9', 'Jellinek-Mercer'],\n",
    "               ['dirichlet_500_val_queries_len_C_is_num_words','mu', '500','Dirichlet smoothing'],\n",
    "               ['dirichlet_1000_val_queries_len_C_is_num_words','mu', '1000','Dirichlet smoothing'],\n",
    "               ['dirichlet_1500_val_queries_len_C_is_num_words','mu', '1500', 'Dirichlet smoothing'],\n",
    "               ['absolute_0.1_val_queries_len_C_is_num_words','delta','1', 'Absolute discounting'],\n",
    "               ['absolute_0.5_val_queries_len_C_is_num_words','delta','5', 'Absolute discounting'],\n",
    "               ['absolute_0.9_val_queries_len_C_is_num_words','delta','9', 'Absolute discounting']]\n",
    "\n",
    "for model in model_names:\n",
    "    print('With a %s of %s the %s had a NDCG@10 of %s, a MAP@1000 of %s a Precission @5 of %s and a Recall@1000 of %s'%(model[1], model[2], model[3],\n",
    "                                                              eval_data[model[0]]['ndcg_cut_10']['all'],eval_data[model[0]]['map']['all'],\n",
    "                                                            eval_data[model[0]]['P_5']['all'],eval_data[model[0]]['recall_1000']['all']))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that for all models the metrics don't differ much when adjusting the parameters we looked at run files manually. We found a clear difference in the scores when adjusting the parameters, but the rankings were almost identical per model. \n",
    "\n",
    "For two of the three models there is a slight difference in NDCG@10. We consider this the most important metric because it takes into account the fact that ranking the first few documents right is more important than ranking later documents right. \n",
    "\n",
    "For Dirichlet smoothing we thus choose a mu of 500 and for Absolute discounting we choose a delta of 5.\n",
    "\n",
    "For Jellinek-Mercer we choose a lambda of 1. Note that all metrics for the three values of lambda are exactly the same and it seems to not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCcQMzMrxQnEzMxKcQIxM7NS/j/dia3pcWz50wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125121160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "absdisc_ndcg = [0.3662, 0.3889, 0.3754]\n",
    "absdisc_delta = [1, 5, 9]\n",
    "plt.scatter(absdisc_delta, absdisc_ndcg)\n",
    "plt.xlabel('Delta')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('Absolute discounting: NDCG@10 for three different deltas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndnD/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125101c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jelimerc_ndcg = [0.0504,0.0504,0.0504]\n",
    "jelimerc_lambda = [1,5,9]\n",
    "\n",
    "plt.scatter(jelimerc_lambda,jelimerc_ndcg)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('Jellinek-Mercer: NDCG@10 for three different lambdas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12511f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirismooth_ndcg = [0.4055,0.4002,0.4026]\n",
    "dirismooth_mu = [500,1000,1500]\n",
    "\n",
    "plt.scatter(dirismooth_mu, dirismooth_ndcg)\n",
    "plt.xlabel('Mu')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('Dirichlet: NDCG@10 for three different mus')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Positional based language model (PLM)\n",
    "The PLM only has to be run on the top 1000 documents per query (ranked by the TF-IDF). We run it for all values of mu on the test queries instead of just for the mu of 1500. We do this to see if there might be a difference for this model or maybe if a subset is used. If a difference is found, we run the the other language models again, but just on these top-1000 queries TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test which PLM $\\mu$-kernel combination works best on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQklv8P7XBqoJB/YOYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e958fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best kernel is the Gaussian kernel. The ndcg@10 for the gaussian kernels are: [0.3629, 0.3525, 0.3489]\n",
      "The best kernel-mu combination is thus a gaussian kernel with a mu of 500\n"
     ]
    }
   ],
   "source": [
    "PLM_mus = [500,1000,1500]\n",
    "\n",
    "eval_data_PLM = read_trec_directory('run/TREC_results_on_validation_set/PLM/')\n",
    "\n",
    "passage_ndcg10 = [float(eval_data_PLM['PLM_PASSAGE_500_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_PASSAGE_1000_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_PASSAGE_1500_']['ndcg_cut_10']['all'])]\n",
    "\n",
    "gaussian_ndcg10 = [float(eval_data_PLM['PLM_GAUS_500_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_GAUS_1000_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_GAUS_1500_']['ndcg_cut_10']['all'])]\n",
    "\n",
    "triangle_ndcg10 = [float(eval_data_PLM['PLM_TRIANGLE_500_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_TRIANGLE_1000_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_TRIANGLE_1500_']['ndcg_cut_10']['all'])]\n",
    "\n",
    "cosine_ndcg10 = [float(eval_data_PLM['PLM_COSINE_500_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_COSINE_1000_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_COSINE_1500_']['ndcg_cut_10']['all'])]\n",
    "\n",
    "circle_ndcg10 = [float(eval_data_PLM['PLM_CIRCLE_500_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_CIRCLE_1000_']['ndcg_cut_10']['all']),\n",
    "                 float(eval_data_PLM['PLM_CIRCLE_1500_']['ndcg_cut_10']['all'])]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(PLM_mus, passage_ndcg10, label = 'Passage')\n",
    "plt.plot(PLM_mus, gaussian_ndcg10, label = 'Gaussian')\n",
    "plt.plot(PLM_mus, triangle_ndcg10, label = 'Triangle')\n",
    "plt.plot(PLM_mus, cosine_ndcg10, label = 'Cosine')\n",
    "plt.plot(PLM_mus, circle_ndcg10, label = 'Circle')\n",
    "\n",
    "plt.xlabel('MU')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('PLM: NDCG@10 for all kernel-mu combinations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('The best kernel is the Gaussian kernel. The ndcg@10 for the gaussian kernels are:', gaussian_ndcg10)\n",
    "print('The best kernel-mu combination is thus a gaussian kernel with a mu of 500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datastructure(queryset, model = None):\n",
    "    '''\n",
    "    input: queryset: test or validation\n",
    "    optional input: create it for a specific model\n",
    "    output: dict of dict of dicts. All trec output for either the test or validation TREC results\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    if queryset == 'validation':\n",
    "        path = 'run/already_run'\n",
    "    if queryset == 'test':\n",
    "        path = 'run/TREC_results_on_test_set'\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file[-4:] == \".txt\":\n",
    "            chunks = file.split('_')\n",
    "            if queryset == 'test':\n",
    "                model_name = chunks[0]\n",
    "                print(model_name)\n",
    "            if queryset == 'validation':\n",
    "                if chunks[2] or chunks[3] == 'val':\n",
    "                    if chunks[0] == 'PLM':\n",
    "                        model_name = chunks[0]+chunks[1]+chunks[2]\n",
    "                    else:\n",
    "                        model_name = chunks[0]+chunks[1]\n",
    "                    model_name = chunks\n",
    "            \n",
    "            with open(path +'/'+ file) as file:\n",
    "                content = [line.strip().split() for line in file.readlines()]\n",
    "                for metric, query, score in content:\n",
    "                    data[model_name][metric][query] =float(score)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def pairwise_significance_tests(test_data, metric = None, alpha = 0.05):\n",
    "    '''\n",
    "    input: \n",
    "    - the TREC test values of all models.\n",
    "    \n",
    "    optional input: \n",
    "    - specific metric to perform the t-test on. If none => it will be performed on all\n",
    "    - the alpha value \n",
    "    \n",
    "    output: \n",
    "    - print statement(s) with t-tests\n",
    "    - txt file with the mean difference and whether its significant or not\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    models = ['BM25','TF-IDF', 'jelinek', 'dirichlet','absolute','W2V']\n",
    "#     metrics = ['P_5','ndcg_cut_10','map','recall_1000']\n",
    "    metrics = ['ndcg_cut_10']\n",
    "     \n",
    "    # Correctig alpha because of the multiple comparison problem\n",
    "    m= calculate_m() # number of hypothesis tests \n",
    "    \n",
    "    print('The number of hypotheses that will be tested is:', m)\n",
    "    \n",
    "    # Sidak correction (slightly less conservative, solves for family wise err rate)\n",
    "    sidak_alpha = 1-(1-alpha)**(1/m)\n",
    "    \n",
    "    # Bonferoni correction (free of dependence and distrivutional assumptions)\n",
    "    bonferoni_alpha = alpha/m \n",
    "    \n",
    "    alphas = [bonferoni_alpha]\n",
    "    # empty it again\n",
    "    tested_combinations = []\n",
    "        \n",
    "    # Test all model combinations\n",
    "    statistically_different = []\n",
    "    for metric in metrics:\n",
    "        \n",
    "        #headers\n",
    "        headers = ['BM25','TF-IDF', 'Jellinek-Mercer', 'Dirichlet',\n",
    "                      'Absolute-Discounting','Word2Vec']\n",
    "\n",
    "        no_cols = len(headers)\n",
    "\n",
    "        file = open(metric+'_t-tests.txt','w')\n",
    "\n",
    "        for counter1, model_1 in enumerate(models):             \n",
    "\n",
    "            for counter2, model_2 in enumerate(models):\n",
    "                \n",
    "                if counter1 == 0 and counter2 ==0:\n",
    "                    file.write(\" \".join(headers))\n",
    "                    row = []\n",
    "                    file.write(\"\\n\")\n",
    "                    \n",
    "                print(counter2,no_cols)\n",
    "                print(row)\n",
    "\n",
    "                if model_1 == model_2:\n",
    "                    row.append('x')\n",
    "                    if counter1==no_cols-1 and counter2 == no_cols-1:\n",
    "                        print('writing',row)\n",
    "                        \n",
    "                        # Append row\n",
    "                        file.write(\" \".join(row))\n",
    "                        row = []\n",
    "                        file.write(\"\\n\")\n",
    "                        \n",
    "                    continue\n",
    "                \n",
    "                print(metric, model_1, model_2)\n",
    "                t_test = scipy.stats.ttest_rel(list(test_data[model_1][metric].values())[:-1],\n",
    "                                               list(test_data[model_2][metric].values())[:-1])\n",
    "                t_statistic = t_test[0]\n",
    "                p_value = t_test[1]\n",
    "\n",
    "                # append to tested combinations so it won't test the same thing twice\n",
    "#                 tested_combinations.append((model_1,model_2,metric))\n",
    "#                 tested_combinations.append((model_2,model_1,metric))\n",
    "\n",
    "                # Print for the different corrections\n",
    "                for num, corrected_alpha in enumerate(alphas):\n",
    "                    if num == 0:\n",
    "                        correction_method = 'Bonferoni'\n",
    "                    if num == 1:\n",
    "                        correction_method = 'Sidak'\n",
    "                    print()\n",
    "                    print('______________________________________________')\n",
    "                    print(\"The t-test of %s and %s for the %s metric, conducted with an alpha of %f and a %s corrected alpha of %.4f has a p-value of: %.4f\"%(model_1, model_2, metric, float(alpha),correction_method, float(corrected_alpha), float(p_value)))\n",
    "\n",
    "                    if p_value < corrected_alpha:\n",
    "                        print('The means of these models for this metric are thus statistically significant different')\n",
    "                        statistically_different.append((model_1, model_2, metric, correction_method))\n",
    "                        difference = (float(list(test_data[model_1][metric].values())[-1])-float(list(test_data[model_2][metric].values())[-1]))\n",
    "                        if str(difference)[0] == '-':\n",
    "                            row.append(str(difference)[:7]+'*')\n",
    "                        else:\n",
    "                            row.append(str(difference)[:6]+'*')\n",
    "                    else:\n",
    "                        print('The means of these models for this metric are thus NOT statistically significant different')\n",
    "                        difference = (float(list(test_data[model_1][metric].values())[-1])-float(list(test_data[model_2][metric].values())[-1]))\n",
    "                        if str(difference)[0] == '-':\n",
    "                            row.append(str(difference)[:7])\n",
    "                        else:\n",
    "                            row.append(str(difference)[:6])\n",
    "                        print()\n",
    "                        print('The difference in means of %s and %s is: %.4f - %.4f = %.4f '%(model_1, model_2,\n",
    "                                                                       float(list(test_data[model_1][metric].values())[-1]),\n",
    "                                                                        float(list(test_data[model_2][metric].values())[-1]),\n",
    "                                                                          float(list(test_data[model_1][metric].values())[-1])-float(list(test_data[model_2][metric].values())[-1])))\n",
    "                if counter2 == no_cols-1:\n",
    "                    print('writing',row)\n",
    "                    # Append row\n",
    "                    file.write(\" \".join(row))\n",
    "                    row = []\n",
    "                    file.write(\"\\n\")\n",
    "    return statistically_different, df\n",
    "    \n",
    "    \n",
    "def calculate_m():\n",
    "    \n",
    "    m=0\n",
    "    \n",
    "    # First calculate m in the ugliest way thinkable\n",
    "    # Test all model combinations\n",
    "    models = ['BM25','TF-IDF', 'jelinek', 'dirichlet','absolute','PLM','W2V','LSI','LSItfidf','LDA']\n",
    "    metrics = ['P_5','ndcg_cut_10','map','recall_1000']\n",
    "    tested_combinations = []\n",
    "    for metric in metrics:\n",
    "        for model_1 in models:\n",
    "            for model_2 in models:\n",
    "                if model_1 != model_2 and (model_1,model_2,metric) not in tested_combinations and (model_2,model_1,metric) not in tested_combinations:\n",
    "                    # append to tested combinations so it won't test the same thing twice\n",
    "                    tested_combinations.append((model_1,model_2,metric))\n",
    "                    tested_combinations.append((model_2,model_1,metric))\n",
    "                    \n",
    "                    # another hypothesis test\n",
    "                    m +=1\n",
    "    \n",
    "    ## However, PLM, LSI, LSItfidf, LDA and the Learning to rank model will not be tested against\n",
    "    # 'BM25','TF-IDF', 'jelinek', 'dirichlet','absolute' and W2V. Thus these tests must be subtracted from m.\n",
    "    models1= ['BM25','TF-IDF', 'jelinek', 'dirichlet','W2V']\n",
    "    models2 = ['PLM','LSI','LSItfidf','LDA','LETOR']\n",
    "    \n",
    "    will_not_be_tested = len(list(itertools.product(models1, models2)))\n",
    "    m_adjusted = m-will_not_be_tested\n",
    "\n",
    "    return m_adjusted\n",
    "\n",
    "def specific_query_testing():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.807965993881226\n"
     ]
    }
   ],
   "source": [
    "'''Create mapping between external document id and indexes of objects in this task'''\n",
    "\n",
    "start_time = time.time()\n",
    "docid_2_docnr = {}\n",
    "for document_idx in range(index.document_base(), index.maximum_document()):\n",
    "    docid_2_docnr[index.document(document_idx)[0]] = document_idx\n",
    "    \n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is the connector that ports Pyndri objects in a Gensim-friendly way.\n",
    "Source: http://www.pythonexample.com/code/gensim-pyro/\n",
    "Only updated return values from _doc2bow func; swapped key, value'''\n",
    "\n",
    "class LDALSISentences(gensim.interfaces.CorpusABC):\n",
    "    # Dictionary through task 2 = vocabulary\n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    " \n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    " \n",
    "        self.max_documents = max_documents\n",
    " \n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    " \n",
    "    def _doc2bow(self, doc):\n",
    " \n",
    "        di = collections.defaultdict(int)\n",
    " \n",
    "        for token_id in doc:\n",
    "            di[token_id] += 1\n",
    "    \n",
    "        return [(key, value) for key, value in di.items()]\n",
    " \n",
    "    def __iter__(self):\n",
    "\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, tokens = self.index.document(int_doc_id)\n",
    " \n",
    "            tokens = tuple(\n",
    "                token_id\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary)\n",
    "\n",
    "            yield self._doc2bow(tokens)\n",
    " \n",
    "    def __len__(self):\n",
    "        return self._maximum_document() - self.index.document_base()\n",
    "    \n",
    "# Use this class if you want to create a gensim BOW representation, without passing a Pyndri index\n",
    "class MyConnecterForTop1000(gensim.interfaces.CorpusABC):\n",
    "    \n",
    "    \"\"\"\n",
    "    :index = a list of lists; where each list is a document\n",
    "    :dictionary = a dictionary of token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        \n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    " \n",
    "        self.max_documents = max_documents\n",
    "    \n",
    "    def _doc2bow(self, doc):\n",
    " \n",
    "        di = collections.defaultdict(int)\n",
    " \n",
    "        for token_id in doc:\n",
    "            di[token_id] += 1\n",
    "    \n",
    "        return [(key, value) for key, value in di.items()]\n",
    "    \n",
    "    def __iter__(self):\n",
    "\n",
    "        for ext_doc_id, tokens in self.index:\n",
    " \n",
    "            tokens = tuple(\n",
    "                token_id\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary)\n",
    "\n",
    "            yield self._doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSI(corpus, dictionary, num_topics, store=False):\n",
    "    \n",
    "    lsi = gensim.models.lsimodel.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "    \n",
    "    if store == True:\n",
    "        file_name = \"\"\n",
    "        full_file_name = file_name + \".p\"\n",
    "        \n",
    "        if full_file_name.exists():\n",
    "            print(\"This file already exists, pick another...\")\n",
    "            return\n",
    "        \n",
    "        pickle.dump(lsi, open(full_file_name, \"wb\"))\n",
    "        \n",
    "    return lsi\n",
    "\n",
    "\n",
    "def LDA(corpus, dictionary, num_topics, update_every=1, chunksize=500, passes=5, store=False):\n",
    "    \n",
    "    #print(\"Start training LDA model...\")\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                          id2word=dictionary, \n",
    "                                          num_topics=num_topics, \n",
    "                                          update_every=update_every, \n",
    "                                          chunksize=chunksize, \n",
    "                                          passes=passes)\n",
    "    \n",
    "    #print(\"Done training\")\n",
    "    \n",
    "    if store == True:\n",
    "        file_name = \"lda_model_tfidf100\"\n",
    "        full_file_name = file_name + \".p\"\n",
    "        \n",
    "        if full_file_name.exists():\n",
    "            print(\"This file already exists, pick another...\")\n",
    "            return\n",
    "        \n",
    "        try:    \n",
    "            pickle.dump(lda, open(full_file_name, \"wb\" ))\n",
    "        except:\n",
    "            print(\"Storing LDA model failed...\")\n",
    "        \n",
    "    return lda\n",
    "    \n",
    "def print_model_topics(model, nr_of_topics, words_per_topic):\n",
    "    print(model.print_topics(nr_of_topics, num_words=words_per_topic))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 0: Convert corpus into gensim dictionary object\n",
    "def init_corpus(store=False):\n",
    "    \n",
    "    #index = pyndri.Index('index/')\n",
    "    print(\"Building dictionary...\")\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    \n",
    "    print(\"Building corpus...\")\n",
    "    corpus = LDALSISentences(index, dictionary)\n",
    "    \n",
    "    # For our own TF IDF, skip those doc ids that are not in given list.\n",
    "    print(\"Building TF-Corpus...\")\n",
    "    corpus = [doc for doc in corpus]\n",
    "    \n",
    "    print(\"Storing dictionary and corpus...\")\n",
    "    \n",
    "    if store == True:\n",
    "        pickle.dump(dictionary, open( \"dictionary.p\", \"wb\" ))\n",
    "        pickle.dump(corpus, open( \"corpus_tf.p\", \"wb\" ))\n",
    "    \n",
    "    return corpus, dictionary\n",
    "\n",
    "# Create our own TFIDF corpus based on task 1\n",
    "def transform_corpus(corpus, store=False):\n",
    "    print(\"Initializing TFIDF model...\")\n",
    "    tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    print(\"Training TFIDF model...\")\n",
    "    corpus_tfidf = tfidf_model[corpus]\n",
    "    \n",
    "    if store == True:\n",
    "        pickle.dump(corpus_tfidf, open( \"corpus_tfidf.p\", \"wb\" ))\n",
    "    \n",
    "    return corpus_tfidf\n",
    "\n",
    "def load_from_pickle(corp_bow=\"\", q_bow=\"\", sem_model=\"\", sim_model=\"\"):\n",
    "    \n",
    "    corpus_bow, query_bows, semantic_model, similarity_model = None, None, None, None\n",
    "    \n",
    "    if corp_bow != \"\":\n",
    "        try:\n",
    "            print(\"Loading corpus BOW: %s...\".format(corp_bow))\n",
    "            with open(corp_bow, 'rb') as f:\n",
    "                corpus_bow = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccesful\")\n",
    "            \n",
    "    if q_bow != \"\":\n",
    "        try:\n",
    "            print(\"Loading query BOW: %s...\".format(q_bow))\n",
    "            with open(q_bow, 'rb') as f:\n",
    "                query_bows = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccessful\")\n",
    "    \n",
    "    if sem_model != \"\":\n",
    "        try:\n",
    "            print(\"Loading semantic model: %s...\".format(sem_model))\n",
    "            with open(sem_model, 'rb') as f:\n",
    "                semantic_model = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccessful\")\n",
    "        \n",
    "    if sim_model != \"\":\n",
    "        try:\n",
    "            print(\"Loading similarity model: %s...\".format(sim_model))\n",
    "            with open(sem_model, 'rb') as f:\n",
    "                similarity_model = pickle.load(f)\n",
    "            print(\"Load successful\")\n",
    "        except:\n",
    "            print(\"Load unsuccessful\")\n",
    "        \n",
    "    return corpus_bow, query_bows, semantic_model, similarity_model\n",
    "\n",
    "\n",
    "# Step 1: Build a representation q of the query\n",
    "def query_representation(dictionary):\n",
    "    \n",
    "    with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "        queries = parse_topics([f_topics])\n",
    "        \n",
    "    tokenized_queries = {\n",
    "        query_id: [dictionary.translate_token(token)\n",
    "                   for token in index.tokenize(query_string)\n",
    "                   if dictionary.has_token(token)]\n",
    "        for query_id, query_string in queries.items()}\n",
    "\n",
    "    unique_query_ids = set(\n",
    "        query_term_id\n",
    "        for query_term_ids in tokenized_queries.values()\n",
    "        for query_term_id in query_term_ids)\n",
    "    \n",
    "    #print(tokenized_queries)\n",
    "    \n",
    "    query_bows = {}\n",
    "    \n",
    "    for query_nr in tokenized_queries.keys():\n",
    "        query_bows[query_nr] = dictionary.doc2bow(tokenized_queries[query_nr])\n",
    "    \n",
    "    return tokenized_queries, unique_query_ids, query_bows\n",
    "\n",
    "def init_similarity_model(model, corpus, num_features=300, store=False):\n",
    "    \n",
    "    sim_index = gensim.similarities.MatrixSimilarity(model[corpus], num_features=num_features)\n",
    "    \n",
    "    if store == True:\n",
    "        pickle.dump(sim_index, open(\"sim_index_lsi_tf.p\", \"wb\"))\n",
    "    \n",
    "    return sim_model\n",
    "    \n",
    "# Generates a weird error:\n",
    "def gensim_similarity(sim_model, model, query_bow, top_num=0):\n",
    "    \n",
    "    query_representation = model[query_bow]\n",
    "    \n",
    "    similarities = sim_model[query_representation]\n",
    "    \n",
    "    sorted_similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    "    \n",
    "    if top_num != 0:\n",
    "        sorted_similarities = sorted_similarities[:top_n]\n",
    "\n",
    "    return sorted_similarities\n",
    "\n",
    "def get_tfidf_ranking_list():\n",
    "    top_1000_per_test_query = defaultdict(list)\n",
    "\n",
    "    with open('run/already_run/TF-IDF.run', 'r') as top_docs_per_query:\n",
    "        for line in top_docs_per_query:\n",
    "            query_id = line.split(' ')[0]\n",
    "            document_name = line.split(' ')[2]\n",
    "            top_1000_per_test_query[query_id].append(document_name)\n",
    "            \n",
    "    return top_1000_per_test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Work here to load all documents\n",
    "def task_2_lsi(model_name, num_topics_lsi, num_topics_lda, run=False, mode=\"validation\", retfidf=False):\n",
    "    \n",
    "    if task_2 == False:\n",
    "        return\n",
    "    \n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "    \n",
    "    num_features_lsi = num_topics_lsi\n",
    "    num_features_lda = num_topics_lda\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fetch the top 1000 ranking using our own implementation of TF-IDF (task 1)\n",
    "    tfidf1000 = get_tfidf_ranking_list()\n",
    "\n",
    "    # Narrowing queries down to appropriate dataset:\n",
    "    if mode == \"validation\":\n",
    "        validation_set = {}\n",
    "        for query_nr in tfidf1000.keys():\n",
    "            if str(query_nr) in validation_queries_ids:\n",
    "                validation_set[query_nr] = tfidf1000[query_nr]\n",
    "        print(\"Number of queries: \", len(validation_set))\n",
    "    else:\n",
    "        test_set = {}\n",
    "        for query_nr in tfidf1000.keys():\n",
    "            if str(query_nr) in test_queries_ids:\n",
    "                validation_set[query_nr] = tfidf1000[query_nr]\n",
    "        print(\"Number of queries: \", len(test_set))\n",
    "    \n",
    "    # Get the dictionary of the entire index:\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    \n",
    "    # Get the query bow representations:\n",
    "    _, _, query_bows = query_representation(dictionary)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # For each query...\n",
    "    for i, query_nr in enumerate(query_bows.keys()):\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(\"Queries processed: \", i, \" at \", time.time() - start_time, \" seconds...\")\n",
    "        \n",
    "        if mode == \"validation\":\n",
    "            if query_nr not in validation_set.keys():\n",
    "                print(\"Query nr \", query_nr, \"not in validation set, skipping...\")\n",
    "                continue \n",
    "            else:\n",
    "                original_ranking = validation_set[query_nr]\n",
    "        else:\n",
    "            if query_nr not in test_set.keys():\n",
    "                print(\"Query nr \", query_nr, \"not in test set, skipping...\")\n",
    "                continue\n",
    "            else:\n",
    "                original_ranking = test_set[query_nr]\n",
    "        \n",
    "        query_bow = query_bows[query_nr]\n",
    "        \n",
    "        # Get the list of documents that will build the corpus\n",
    "        doc_list = [index.document(docid_2_docnr[doc]) for doc in original_ranking]\n",
    "        \n",
    "        query_corpus = MyConnecterForTop1000(doc_list, dictionary)\n",
    "        \n",
    "        #TF-IDF transformation:\n",
    "        if retfidf == True:\n",
    "            query_corpus = transform_corpus(query_corpus)\n",
    "        \n",
    "        # Build LSI model for the query's corpus:\n",
    "        # The idea here is to narrow the topic-creating scope down to \n",
    "        lsi = gensim.models.lsimodel.LsiModel(query_corpus, id2word=dictionary, num_topics=num_topics_lsi)\n",
    "        \n",
    "        # Build similarity matrix for LSI:\n",
    "        sim_index = gensim.similarities.MatrixSimilarity(lsi[query_corpus], num_features=num_features_lsi)\n",
    "        \n",
    "        # Find LSI representation for query:\n",
    "        query_lsi = lsi[query_bow]\n",
    "\n",
    "        # Find similarities using the index matrix and the \n",
    "        similarities = sim_index[query_lsi]\n",
    "        \n",
    "        data[query_nr] = tuple(zip(similarities, original_ranking))\n",
    "        \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "def task_2_lda(model_name, num_topics_lsi, num_topics_lda, run=False, mode=\"validation\"):\n",
    "    \n",
    "    if task_2 == False:\n",
    "        return\n",
    "    \n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "    \n",
    "    num_features_lsi = num_topics_lsi\n",
    "    num_features_lda = num_topics_lda\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fetch the top 1000 ranking using our own implementation of TF-IDF (task 1)\n",
    "    tfidf1000 = get_tfidf_ranking_list()\n",
    "\n",
    "    # Narrowing queries down to appropriate dataset:\n",
    "    if mode == \"validation\":\n",
    "        validation_set = {}\n",
    "        for query_nr in tfidf1000.keys():\n",
    "            if str(query_nr) in validation_queries_ids:\n",
    "                validation_set[query_nr] = tfidf1000[query_nr]\n",
    "        print(\"Number of queries: \", len(validation_set))\n",
    "    else:\n",
    "        test_set = {}\n",
    "        for query_nr in tfidf1000.keys():\n",
    "            if str(query_nr) in test_queries_ids:\n",
    "                validation_set[query_nr] = tfidf1000[query_nr]\n",
    "        print(\"Number of queries: \", len(test_set))\n",
    "    \n",
    "    # Get the dictionary of the entire index:\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    \n",
    "    # Get the query bow representations:\n",
    "    _, _, query_bows = query_representation(dictionary)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # For each query...\n",
    "    for i, query_nr in enumerate(query_bows.keys()):\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(\"Queries processed: \", i, \" at \", time.time() - start_time, \" seconds...\")\n",
    "        \n",
    "        if mode == \"validation\":\n",
    "            if query_nr not in validation_set.keys():\n",
    "                #print(\"Query nr \", query_nr, \"not in validation set, skipping...\")\n",
    "                continue \n",
    "            else:\n",
    "                original_ranking = validation_set[query_nr]\n",
    "        else:\n",
    "            if query_nr not in test_set.keys():\n",
    "                #print(\"Query nr \", query_nr, \"not in test set, skipping...\")\n",
    "                continue\n",
    "            else:\n",
    "                original_ranking = test_set[query_nr]\n",
    "        \n",
    "        query_bow = query_bows[query_nr]\n",
    "        \n",
    "        # Get the list of documents that will build the corpus\n",
    "        doc_list = [index.document(docid_2_docnr[doc]) for doc in original_ranking]\n",
    "        \n",
    "        query_corpus = MyConnecterForTop1000(doc_list, dictionary)\n",
    "        \n",
    "        # Build LSI model for the query's corpus:\n",
    "        lda = LDA(query_corpus, dictionary, num_topics=num_topics_lda, update_every=1, chunksize=500, passes=5)\n",
    "        \n",
    "        # Build similarity matrix for LSI:\n",
    "        sim_index = gensim.similarities.MatrixSimilarity(lda[query_corpus], num_features=num_features_lda)\n",
    "        \n",
    "        # Find LSI representation for query:\n",
    "        query_lda = lda[query_bow]\n",
    "\n",
    "        # Find similarities using the index matrix and the \n",
    "        similarities = sim_index[query_lda]\n",
    "        \n",
    "        data[query_nr] = tuple(zip(similarities, original_ranking))\n",
    "        \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "    \n",
    "    return data\n",
    "\n",
    "sims = task_2(\"LDA_validation_topics_100\", 100, 100, run=True, mode=\"validation\")\n",
    "print(\"1 done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_w2v_model(file=\"\", train=False, epochs=5):\n",
    "    \n",
    "    if train == True:\n",
    "        word2vec = gensim.models.Word2Vec(\n",
    "        size=300,  # Embedding size\n",
    "        window=5,  # One-sided window size\n",
    "        sg=True,  # Skip-gram.\n",
    "        min_count=5,  # Minimum word frequency.\n",
    "        sample=1e-3,  # Sub-sample threshold.\n",
    "        hs=False,  # Hierarchical softmax.\n",
    "        negative=10,  # Number of negative examples.\n",
    "        iter=1,  # Number of iterations.\n",
    "        workers=8,  # Number of workers.\n",
    "    )\n",
    "\n",
    "        print(\"Fetching dictionary...\")\n",
    "        dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "        print(\"Fetching sentences...\")\n",
    "        sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "        print(\"building vectors...\")\n",
    "        %time word2vec.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "        print(\"Training...\")\n",
    "        w2v_models = [word2vec]\n",
    "\n",
    "        N_EPOCHS = epochs\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            print('Epoch ', epoch)\n",
    "\n",
    "            w2v_model = copy.deepcopy(w2v_models[-1])\n",
    "            w2v_model.train(sentences, total_examples=word2vec.corpus_count, epochs=1)\n",
    "\n",
    "            w2v_models.append(w2v_model)\n",
    "        \n",
    "    else:\n",
    "        if file != \"\":\n",
    "            with open(file, 'rb') as f:\n",
    "                w2v_model = pickle.load(f)\n",
    "        else:\n",
    "            print(\"Enter file name or train the model (i.e set train=True)\")\n",
    "            return None\n",
    "    \n",
    "    return w2v_model\n",
    "        \n",
    "        \n",
    "def build_query_vectors(word2vec, dictionary, mode=\"validation\"):\n",
    "    \n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    \n",
    "    with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "        queries = parse_topics([f_topics])\n",
    "        \n",
    "    tokenized_queries = {\n",
    "        query_id: [token\n",
    "                   for token in index.tokenize(query_string)\n",
    "                   if dictionary.has_token(token)]\n",
    "        for query_id, query_string in queries.items()}\n",
    "    \n",
    "    not_found = []\n",
    "    query2vec = {}\n",
    "    for query in tokenized_queries.keys():\n",
    "        words = np.array(tokenized_queries[query])\n",
    "        \n",
    "        embeddings = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                embeddings.append(word2vec.wv[word])\n",
    "            except:\n",
    "                not_found.append(word)\n",
    "        \n",
    "        query2vec[query] = np.average(embeddings, axis=0)\n",
    "        \n",
    "    return query2vec, not_found\n",
    "\n",
    "def build_doc_vectors(w2v, sentences, not_found=[], mode=\"validation\"):\n",
    "    \n",
    "    doc2vec = {}\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i % 50000 == 0:\n",
    "            print(i, \"sentences processed\")\n",
    "        \n",
    "        embeddings = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                embeddings.append(word2vec.wv[word])\n",
    "            except:\n",
    "                not_found.append(word)\n",
    "                \n",
    "        doc2vec[index.document(i + 1)[0]] = np.average(embeddings, axis=0)\n",
    " \n",
    "    return doc2vec, not_found\n",
    "\n",
    "def get_ranking(model_name, query2vec, doc2vec, mode=\"validation\"):\n",
    "    \n",
    "    ranking_by_query = {}\n",
    "    \n",
    "    if mode == \"validation\":\n",
    "        queries = [query for query in query2vec.keys() if query in validation_queries_ids]\n",
    "    else:\n",
    "        queries = [query for query in query2vec.keys() if query in test_queries_ids]\n",
    "        \n",
    "    issues = []\n",
    "    ranking = defaultdict(list)\n",
    "    for i, query in enumerate(queries):\n",
    "        print(\"Query nr\", i, \"/\", len(queries), )\n",
    "        q_embedding = query2vec[query]\n",
    "        \n",
    "        for document in doc2vec.keys():\n",
    "            d_embedding = doc2vec[document]\n",
    "            if d_embedding.shape == (300,):\n",
    "                score = 1 - spatial.distance.cosine(q_embedding, d_embedding)\n",
    "            else:\n",
    "                issues.append(document)\n",
    "                score = 0.0\n",
    "            \n",
    "            ranking[query].append((score, document))\n",
    "\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=ranking,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "    \n",
    "    return ranking, issues\n",
    "\n",
    "# %time w2v_model2 = build_w2v_model(file=\"self_trained_word2vec.pickle\")\n",
    "\n",
    "#%time query2vec, not_found = build_query_vectors(w2v_model, dictionary)\n",
    "\n",
    "#%time doc2vec, words_not_found = build_doc_vectors(w2v_model, sentences, not_found=not_found)\n",
    "\n",
    "#%time ranking, doc_issues = get_ranking(query2vec, doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_run(path_to_run, top1000, empty_queries):\n",
    "    '''\n",
    "    Read document scores from a .run file\n",
    "    \n",
    "    :param path_to_run: where to find the .run file\n",
    "    '''\n",
    "    \n",
    "    print(\"reading from {}\".format(path_to_run))\n",
    "    \n",
    "    scores = defaultdict(dict)\n",
    "    \n",
    "    with open(path_to_run) as run_file:\n",
    "        for i, line in enumerate(run_file.readlines()):\n",
    "            query_id, _, ext_doc_id, document_rank, score, label = line.split()\n",
    "            \n",
    "            # Skip q/d pairs that don't share words\n",
    "            if score == '0' or score == '-inf':\n",
    "                continue\n",
    "            \n",
    "            # Only look at top1000 document for the query\n",
    "            if top1000 and not ext_doc_id in top_1000_per_test_query[query_id]:\n",
    "                continue\n",
    "                \n",
    "            scores[int(query_id)][ext2int[ext_doc_id]] = float(score)\n",
    "    \n",
    "    filter_queries = list(scores.keys()) if empty_queries else None\n",
    "    \n",
    "    return matrix_from_two_level_dict(scores, filter_top_1000=top1000, filter_queries=filter_queries)\n",
    "\n",
    "def unique_docs_top_1000(top_1000):\n",
    "    \n",
    "    '''Return the set of unique documents that occur in one of the top1000 for at least one of the queries'''\n",
    "    unique_top_docs = set()\n",
    "    for top in top_1000.values():\n",
    "        for doc in top:\n",
    "            unique_top_docs.add(ext2int[doc])\n",
    "    return unique_top_docs\n",
    "    \n",
    "def indices_of_all_top_1000(top_1000):\n",
    "    '''Get the indices of the documents that occur in a top1000'''\n",
    "    \n",
    "    unique_top_docs = unique_docs_top_1000(top_1000)      \n",
    "    indices = np.array(sorted(list(unique_top_docs))) - 1\n",
    "    \n",
    "    return indices\n",
    "    \n",
    "def build_relevance_matrix(path_to_relavance_labels, top1000, empty_queries, sparse=True):\n",
    "    '''\n",
    "    Constructs a sparse matrix where the relevant query/document pairs are 1's and the rest 0's\n",
    "    \n",
    "    :param path_to_relevance_labels: where to find the relevance file\n",
    "    '''\n",
    "    \n",
    "    relevance_labels_validation = defaultdict(dict)\n",
    "\n",
    "    # Read from relevance file\n",
    "    with open(path_to_relavance_labels) as valid_file:\n",
    "        for i, line in enumerate(valid_file.readlines()):\n",
    "            query_id, _, ext_doc_id, relevance = line.split()\n",
    "            \n",
    "            if top1000 and not ext_doc_id in top_1000_per_test_query[query_id]:\n",
    "                continue\n",
    "                \n",
    "            relevance_labels_validation[int(query_id)][ext2int[ext_doc_id]] = int(relevance)\n",
    "    \n",
    "    filter_queries = list(relevance_labels_validation.keys()) if empty_queries else None\n",
    "    \n",
    "    relevance_matrix = matrix_from_two_level_dict(relevance_labels_validation, \n",
    "                                                  make_sparse=sparse,\n",
    "                                                  filter_top_1000=top1000, \n",
    "                                                  filter_queries=filter_queries)\n",
    "\n",
    "    \n",
    "    return np.ravel(relevance_matrix)\n",
    "\n",
    "def matrix_from_two_level_dict(dictionary, make_sparse=False, filter_top_1000=False, filter_queries=None):\n",
    "    \n",
    "    '''Takes a two level nested dictionary and returns an array of the data of shpae (n_keys, n_values)'''\n",
    "    \n",
    "    matrix = np.zeros((max(dictionary.keys()) + 1, len(ext2int)))\n",
    "    \n",
    "    for key, sub_dict in dictionary.items():\n",
    "        for sub_key, value in sub_dict.items():\n",
    "            matrix[key, sub_key] = value\n",
    "    \n",
    "    if filter_top_1000:\n",
    "        doc_idxs = indices_of_all_top_1000(top_1000_per_test_query)\n",
    "        matrix = matrix[:, doc_idxs]\n",
    "        \n",
    "    if np.any(filter_queries):\n",
    "        q_idxs = np.array(sorted(filter_queries))\n",
    "        matrix = matrix[q_idxs, :]\n",
    "        \n",
    "    if make_sparse:\n",
    "        matrix = sparse.csr_matrix(matrix)\n",
    "        \n",
    "    print(\"Matrix size: \", matrix.shape)\n",
    "            \n",
    "    return matrix\n",
    "\n",
    "def load_features(run_dir, top1000, empty_queries, add_document_features=False, make_sparse=True):\n",
    "    '''Build a matrix based on features for all documents and queries'''\n",
    "    \n",
    "    run_file_names = [\"{}/{}\".format(run_dir,file_name) for file_name in os.listdir(run_dir) if file_name[-4:] == '.run']\n",
    "    \n",
    "    print(\"Reading data into matrices\")\n",
    "    features_per_pair = [read_run(file_name, top1000=top1000, empty_queries=empty_queries) for file_name in run_file_names]\n",
    "    \n",
    "    # Save dimensions\n",
    "    n_features = len(features_per_pair)\n",
    "    n_queries = features_per_pair[0].shape[0]\n",
    "    n_docs = features_per_pair[0].shape[1]\n",
    "\n",
    "    print(\"Ascending to 3rd dimension\")\n",
    "    # Combine query/doc matrices for each feature into 3D array\n",
    "    features = np.concatenate(features_per_pair, axis=0)\n",
    "    features = features.reshape(n_features, n_queries, n_docs)\n",
    "    \n",
    "    if np.any(add_document_features):\n",
    "        print(\"Adding document features...\")\n",
    "        doc_features = document_features(top_1000_per_test_query)\n",
    "        \n",
    "        if top1000:\n",
    "            doc_idxs = indices_of_all_top_1000(top_1000_per_test_query)\n",
    "            doc_features = doc_features[doc_idxs]\n",
    "        \n",
    "        assert n_docs == doc_features.shape[0], \"The returned document features are the wrong size. \\\n",
    "            Got [ {} {} ]\".format(n_docs, doc_features.shape[0])\n",
    "        \n",
    "        doc_features = np.tile(doc_features, n_queries).reshape(n_queries, n_docs)\n",
    "        \n",
    "        print(features.shape, np.expand_dims(doc_features, axis=0).shape)\n",
    "        \n",
    "        features = np.concatenate((features, np.expand_dims(doc_features, axis=0)), axis=0)\n",
    "    \n",
    "    print(\"Going through a wormhole\")\n",
    "    # Transform from (features, queries, documents) to (queries, documents, features)\n",
    "    # so that features per doc/query pair are aligned in a single vector\n",
    "    features = np.transpose(features, (1, 2, 0))\n",
    "    \n",
    "    if make_sparse:\n",
    "        print(\"Back to earth...\")\n",
    "        features = sparse.csr_matrix(np.vstack(features), dtype=np.float64)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def document_features(top_1000):\n",
    "    \n",
    "    '''Build a feature vector of document lengths'''\n",
    "    \n",
    "    feature_values = np.zeros(len(ext2int))\n",
    "    \n",
    "    for doc_id in unique_docs_top_1000(top_1000):\n",
    "        # Document length\n",
    "        doc_length = document_lengths[doc_id]\n",
    "        feature_values[doc_id] = doc_length\n",
    "    \n",
    "    return feature_values\n",
    "\n",
    "def compute_ratio(majority_share, y):\n",
    "    \n",
    "    '''Build a dictionary with the desired representation ratio per class in the trainings data '''\n",
    "    \n",
    "    n_positive = np.sum(y).astype(np.int32)\n",
    "    n_negative = (majority_share * n_positive).astype(np.int32)\n",
    "    \n",
    "    return {0 : n_negative, 1: n_positive}\n",
    "\n",
    "def standardize_sparse_matrix(scaler, train):\n",
    "    '''\n",
    "    Center mean by casting back and forth to dense matrix. Mean centering\n",
    "    not possible on sparse matrix.\n",
    "    '''\n",
    "    \n",
    "    train=train.todense()\n",
    "    scaler.fit(train)\n",
    "    train=scaler.transform(train)\n",
    "        \n",
    "    return scaler, sparse.csr_matrix(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features for training data\n",
    "\n",
    "For this task, we swap the roles of the `qrel_validation` and `qrel_test` sets. Because we want a large data set to train and perform 10-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "\n",
    "feature_matrix = load_features('letor_data/baseline/{}'.format(split), \n",
    "                                            top1000=True, empty_queries=True, \n",
    "                                            make_sparse=True)\n",
    "\n",
    "relevance_labels = build_relevance_matrix('ap_88_89/qrel_{}'.format(split), \n",
    "                                          top1000=True, \n",
    "                                          empty_queries=True, \n",
    "                                          sparse=False)\n",
    "\n",
    "print()\n",
    "print(\"Amount of relevant pairs:\", np.sum(relevance_labels))\n",
    "print(\"Percentage of positive examples:\", np.sum(relevance_labels) / len(relevance_labels))\n",
    "print(\"Dataset of size:\", feature_matrix.shape)\n",
    "\n",
    "positive_ratio = np.sum(relevance_labels) / len(relevance_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have the raw features corresonding to all 11,825,760 training pairs. But, as you can see, the amount of irrelevant pairs is grossly over represented. Also, the features are all distributed differently. In the next cell, normalize the data. Dealing with the class imbalance is left to the cross-validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dense representation for centering the mean\n",
    "# scaler is re-used later to standardise features for ranking\n",
    "scaler = StandardScaler()\n",
    "scaler, feature_matrix = standardize_sparse_matrix(scaler, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_parameters = {'C': [1e-1, 1e-2, 1e-3], \n",
    "                    'undersampling_rate' : [1, 25, 50]}\n",
    "\n",
    "def resample_data(train_features, train_labels, Sampler, magnitude):\n",
    "    print(\"Generating re-sampled dataset\")\n",
    "    rus = Sampler(random_state=0, ratio=compute_ratio(magnitude, train_labels))\n",
    "    X_resampled, y_resampled = rus.fit_sample(train_features, train_labels)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "model = LogisticRegression(penalty='l2',                        \n",
    "                           solver='sag', \n",
    "                           max_iter=1000,\n",
    "                           verbose=6)\n",
    "\n",
    "Sampler = RandomUnderSampler\n",
    "\n",
    "model_evaluation = defaultdict(list)\n",
    "\n",
    "folder = KFold(n_splits=10, shuffle=True)\n",
    "count = 1\n",
    "\n",
    "for regularization in hyper_parameters['C']:\n",
    "    for undersampling_rate in reversed(hyper_parameters['undersampling_rate']):\n",
    "\n",
    "        params = (regularization, undersampling_rate)\n",
    "        model.set_params(C=regularization)\n",
    "\n",
    "        # Do cross-validation for parameter set\n",
    "        for train_idx, valid_idx in folder.split(feature_matrix):\n",
    "\n",
    "            print('Starting fold {}'.format(count))\n",
    "\n",
    "            # Resample training fold\n",
    "            train_feature_fold = feature_matrix[train_idx]\n",
    "            train_label_fold = relevance_labels[train_idx]\n",
    "            train_feature_fold, train_label_fold = resample_data(train_feature_fold, \n",
    "                                                                 train_label_fold,\n",
    "                                                                 Sampler,\n",
    "                                                                 undersampling_rate)\n",
    "\n",
    "            print(\"Fitting model...\")\n",
    "            # Fit model\n",
    "            model.fit(train_feature_fold, train_label_fold)\n",
    "\n",
    "            print(\"Doing evaluation...\")\n",
    "            # Evaluate on unbalanced validation fold\n",
    "            predictions = model.predict(feature_matrix[valid_idx])\n",
    "            precision = precision_score(relevance_labels[valid_idx], predictions)\n",
    "            recall = recall_score(relevance_labels[valid_idx], predictions)\n",
    "\n",
    "            model_evaluation[params].append((precision, recall))\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        with open('letor_true_evaluation_dict.pickle', 'wb') as p_file:\n",
    "            pickle.dump(model_evaluation, p_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('letor_evaluation_dict.pickle', 'rb') as p_file:\n",
    "#     model_evaluation = pickle.load(p_file)\n",
    "\n",
    "'''Compute averages of the metrics to compare hypter parameter performance'''\n",
    "\n",
    "for param_set, metrics in model_evaluation.items():\n",
    "    precision, recall = zip(*metrics)\n",
    "    precision, recall = np.array(precision), np.array(recall)\n",
    "    \n",
    "    f1 = (precision * recall) / (precision + recall)\n",
    "    mean_f1 = f1.mean()\n",
    "    \n",
    "    mean_prec, mean_rec = precision.mean(), recall.mean()\n",
    "    \n",
    "    print('C: {} \\nRatio: {}'.format(param_set[0], param_set[1]))\n",
    "    print('Avg Precision: {} \\nAvg Recall: {} \\nF1: {}'.format(mean_prec, mean_rec, mean_f1))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model is unresponsive to the regularization weight. And inspection the avarages of the evaluation measures, you can see that a ratio 50 negative examples to 1 positive example in the training set has the best balance between precision and recall. Therefore, in the remainder of this experiment we will set:\n",
    "\n",
    "$ C = 0.001 $\n",
    "\n",
    "$ Balance = 50 : 1$\n",
    "\n",
    "And retrain the baseline model on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = LogisticRegression(penalty='l2',\n",
    "                                    C=1e-3,\n",
    "                                    solver='sag', \n",
    "                                    max_iter=1000,\n",
    "                                    verbose=6)\n",
    "\n",
    "feature_matrix, relevance_labels = resample_data(feature_matrix, relevance_labels, Sampler, 50)\n",
    "\n",
    "baseline_model.fit(feature_matrix, relevance_labels)\n",
    "\n",
    "joblib.dump(baseline_model, 'baseline_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-rank top-1000 using the LETOR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we need complete matrices so coordinates are aligned with query/document indices. That way we can\n",
    "retrieve the original query and external document id needed for TREC.\n",
    "'''\n",
    "\n",
    "split = 'validation'\n",
    "prune_docs = False\n",
    "prune_queries = False\n",
    "\n",
    "# Read validation data\n",
    "full_feature_matrix = load_features('letor_data/baseline/{}'.format(split), \n",
    "                                            top1000=prune_docs, empty_queries=prune_queries, \n",
    "                                            make_sparse=False)\n",
    "\n",
    "\n",
    "print(\"Standardizing data...\")\n",
    "\n",
    "n_queries, n_docs, n_feats = full_feature_matrix.shape\n",
    "\n",
    "full_feature_matrix = scaler.transform(np.vstack(full_feature_matrix)).reshape(n_queries, n_docs, n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_letor_ranking(model_name, run_type, full_feature_matrix, letor_model):\n",
    "    \n",
    "    '''Modified version of run retrievel to accomodate the learning to rank framework'''\n",
    "    \n",
    "    run_out_path = 'letor_run/{}_{}.run'.format(model_name, run_type)\n",
    "    if os.path.exists(run_out_path):\n",
    "        print(\"File already exists; exiting...\")\n",
    "        return\n",
    "    \n",
    "    data = defaultdict(list)\n",
    "    \n",
    "    if run_type == 'validation':\n",
    "        subset = validation_queries_ids\n",
    "    elif run_typ == 'test':\n",
    "        subset = test_queries_ids\n",
    "    else:\n",
    "        print('Invalid run type. Should be either \"valiation\" or \"test\".')\n",
    "        return\n",
    "    \n",
    "    print(\"Calculating scores...\")\n",
    "    \n",
    "    for query_id, feature_matrix in enumerate(full_feature_matrix):\n",
    "        \n",
    "        if str(query_id) in subset:\n",
    "            sparse_feat = sparse.csr_matrix(feature_matrix)\n",
    "            predicted_probabilities = letor_model.predict_proba(sparse_feat)\n",
    "            for doc_id, score in enumerate(predicted_probabilities):\n",
    "                data[str(query_id)].append((score[1], int2ext[doc_id + 1]))\n",
    "    \n",
    "    print(\"Writing to file...\")\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extra features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_plm(top_1000, split):\n",
    "    \n",
    "    '''Run the special version of PLM that returns the index of the maximum and save data. This\n",
    "    is used as a feature in the extended model.'''\n",
    "    \n",
    "    data = defaultdict(dict)\n",
    "    \n",
    "    for i, query_id in enumerate(top_1000.keys()):\n",
    "        \n",
    "        print(\"{} of {}\".format(i, len(list(top_1000.keys()))))\n",
    "        \n",
    "        for doc_id in top_1000[query_id]:\n",
    "            index = PLM_score_old(tokenized_queries[query_id], \n",
    "                                                   ext2int[doc_id], \n",
    "                                                   gaussian_kernel, \n",
    "                                                   mu=500, \n",
    "                                                   return_argmax=True)\n",
    "            \n",
    "            inv_index = document_lengths[ext2int[doc_id]] / (index + 1)\n",
    "            data[int(query_id)][ext2int[doc_id]] = inv_index\n",
    "        \n",
    "    with open('indices_of_plm_{}.pickle'.format(split), 'wb') as p_file:\n",
    "        pickle.dump(data, p_file)\n",
    "\n",
    "# Dict to filter documents in test-set\n",
    "top_1000_per_test_query = defaultdict(list)\n",
    "with open('./letor_data/incremental/test/tfidf_test.run', 'r') as top_docs_per_query:\n",
    "    for line in top_docs_per_query:\n",
    "        query_id = line.split(' ')[0]\n",
    "        document_name = line.split(' ')[2]\n",
    "        top_1000_per_test_query[query_id].append(document_name)\n",
    "        \n",
    "# Dict to filter documents in validation set\n",
    "top_1000_per_val_query = defaultdict(list)\n",
    "with open('./letor_data/incremental/validation/tfidf_val.run', 'r') as top_docs_per_query:\n",
    "    for line in top_docs_per_query:\n",
    "        query_id = line.split(' ')[0]\n",
    "        document_name = line.split(' ')[2]\n",
    "        top_1000_per_val_query[query_id].append(document_name)\n",
    "\n",
    "# index_of_plm(top_1000_per_test_query, 'test')\n",
    "# index_of_plm(top_1000_per_val_query, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prepare training data for extended model'''\n",
    "\n",
    "plm_indices_train = None\n",
    "\n",
    "# REMINDER: We are training on the test set!\n",
    "with open('indices_of_plm_test.pickle', 'rb') as p_file:\n",
    "    data = pickle.load(p_file)\n",
    "    plm_indices_train = matrix_from_two_level_dict(data, \n",
    "                                                  make_sparse=False, \n",
    "                                                  filter_top_1000=True,\n",
    "                                                  filter_queries=list(data.keys()))\n",
    "    \n",
    "    plm_indices_train = sparse.csr_matrix(np.expand_dims(plm_indices_train.ravel(), 1))\n",
    "\n",
    "\n",
    "split = 'test'\n",
    "\n",
    "extended_features = load_features('letor_data/incremental/{}'.format(split), \n",
    "                                    top1000=True, empty_queries=True,\n",
    "                                    make_sparse=True, add_document_features=True)\n",
    "\n",
    "relevance_labels = build_relevance_matrix('ap_88_89/qrel_{}'.format(split), top1000=True, empty_queries=True, sparse=False)\n",
    "\n",
    "extended_features = sparse.hstack([extended_features, plm_indices_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train extend model on the entire training set'''\n",
    "\n",
    "incremental_model = LogisticRegression(penalty='l2',\n",
    "                                    C=1e-3,\n",
    "                                    solver='sag', \n",
    "                                    max_iter=1000,\n",
    "                                    verbose=6)\n",
    "\n",
    "\n",
    "\n",
    "extended_features, relevance_labels = resample_data(extended_features, relevance_labels, Sampler, 50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler, extended_features = standardize_sparse_matrix(scaler, extended_features)\n",
    "\n",
    "incremental_model.fit(extended_features, relevance_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(incremental_model, 'incremental_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_indices_test = None\n",
    "\n",
    "# REMINDER: We are testing on the validation queries!\n",
    "with open('indices_of_plm_validation.pickle', 'rb') as p_file:\n",
    "    data = pickle.load(p_file)\n",
    "    plm_indices_test = matrix_from_two_level_dict(data, \n",
    "                                                  make_sparse=False, \n",
    "                                                  filter_top_1000=False,\n",
    "                                                  filter_queries=False)\n",
    "    \n",
    "    # Following is slightly different than in the training phase: the feature matrix is 3D, so we have to\n",
    "    # raise the dimensionality. Also transpose to fit the orientation of the feature matrix\n",
    "    plm_indices_test = np.transpose(np.expand_dims(plm_indices_test, 0), (1, 2, 0))\n",
    "\n",
    "valid_extended_features = load_features('letor_data/incremental/validation', \n",
    "                                        top1000=False, \n",
    "                                        empty_queries=False, \n",
    "                                        add_document_features=True, \n",
    "                                        make_sparse=False)\n",
    "\n",
    "\n",
    "\n",
    "# Add PLM position features\n",
    "valid_extended_features = np.append(valid_extended_features, plm_indices_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standardizing data...\")\n",
    "\n",
    "n_queries, n_docs, n_feats = valid_extended_features.shape\n",
    "\n",
    "\n",
    "valid_extended_features = scaler.transform(np.vstack(valid_extended_features))\n",
    "valid_extended_features = valid_extended_features.reshape(n_queries, n_docs, n_feats)\n",
    "\n",
    "\n",
    "run_letor_ranking('C=0,001_ratio=50_incremental', 'validation', valid_extended_features, incremental_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics = ['ndcg_cut_10', 'recall_1000', 'map']\n",
    "\n",
    "baseline_metrics = read_trec_results('letor_run/C=0,001_ratio=50_baseline_validation.txt', filter_metrics)\n",
    "incremental_metrics = read_trec_results('letor_run/C=0,001_ratio=50_incremental_validation.txt', filter_metrics)\n",
    "\n",
    "# Bonferoni alpha because we are going to perform 3 t-tests on the same dataset.\n",
    "alpha = 0.05 / len(filter_metrics)\n",
    "\n",
    "significant_diffs = []\n",
    "\n",
    "for metric in filter_metrics:\n",
    "    _, baseline_list = zip(*baseline_metrics[metric])\n",
    "    _, incremental_list = zip(*incremental_metrics[metric])\n",
    "    \n",
    "    # Last element is the average over all queries\n",
    "    baseline_list, incremental_list = baseline_list[:-1], incremental_list[:-1]\n",
    "\n",
    "    \n",
    "    t_stat, p_value = scipy.stats.ttest_rel(baseline_list, incremental_list)\n",
    "    \n",
    "    baseline_mean = np.mean(baseline_list)\n",
    "    incremental_mean = np.mean(incremental_list)\n",
    "    \n",
    "    print(metric, (baseline_mean, incremental_mean))\n",
    "    \n",
    "    if p_value < alpha:\n",
    "        \n",
    "        \n",
    "        significant_diffs.append((baseline_mean, incremental_mean))\n",
    "        \n",
    "\n",
    "print(significant_diffs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both LETOR models perform poorly. And the incremental is not significantly better than the baseline for any metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ml1labs",
   "language": "python",
   "name": "ml1labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
